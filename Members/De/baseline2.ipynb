{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10190\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "TRAININFO = \"/data/HitSongPrediction/train_info.tsv\"\n",
    "TRAINRANK =  \"/data/HitSongPrediction/train_rank.csv\"\n",
    "TESTINFO = \"/data/HitSongPrediction/test_info.tsv\"\n",
    "SUBMISSION = \"/data/HitSongPrediction/submission.csv\"\n",
    "\n",
    "# Prepare data\n",
    "df_i = pd.read_csv(TRAININFO, delimiter='\\t',encoding='utf-8')\n",
    "df_r = pd.read_csv(TRAINRANK)\n",
    "df_i_train = df_i.merge(df_r, left_on='ID', right_on='ID')\n",
    "df_i_train[\"dataset\"] = \"train\"\n",
    "\n",
    "df_i_test = pd.read_csv(TESTINFO, delimiter='\\t',encoding='utf-8')\n",
    "df_i_test[\"label\"] = np.nan\n",
    "df_i_test[\"dataset\"] = \"test\"\n",
    "\n",
    "df = pd.concat([df_i_train, df_i_test])\n",
    "df_track_info = pd.read_csv(\"../../csv/all_track_info.csv\")\n",
    "df = df.merge(df_track_info, left_on='ID', right_on='ID')\n",
    "df_audio_features = pd.read_csv(\"../../csv/all_track_audio_features.csv\")\n",
    "df =df.merge(df_audio_features,left_on=\"ID\",right_on=\"ID\", how=\"left\")\n",
    "\n",
    "df_album_hash = pd.read_csv(\"../../csv/album_hash.csv\") \n",
    "df =df.merge(df_album_hash,left_on=\"ID\",right_on=\"ID\", how=\"left\")\n",
    "\n",
    "#Drop duplicate song\n",
    "def remove_duplicate_songs_with_low_ranks(df):\n",
    "    duplicateRowsDF = df[df.duplicated([\"title\", \"album\", \"artist_name\"], False)]\n",
    "    duplicateRowsDF = duplicateRowsDF[~duplicateRowsDF.label.isnull()]\n",
    "    all_index = duplicateRowsDF.index\n",
    "\n",
    "    duplicateRowsDF= duplicateRowsDF.sort_values(by=['label'])\n",
    "    duplicateRowsDF = duplicateRowsDF.drop_duplicates([\"title\", \"album\", \"artist_name\"],keep=\"first\")\n",
    "    keep_index = duplicateRowsDF.index\n",
    "\n",
    "    remove_index = list(set(all_index) - set(keep_index))\n",
    "    df = df.drop(remove_index)\n",
    "    return df\n",
    "df = remove_duplicate_songs_with_low_ranks(df)\n",
    "\n",
    "# Sort by ID\n",
    "df = df.sort_values(by=['ID'])\n",
    "df= df.reset_index()\n",
    "\n",
    "print(len(df))\n",
    "# df.head(5683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.length==0][[\"title\", \"album\", \"album_artist\", \"artist_name\",\"genre\", \"composers_name\", \"track\",\"release_time\",\"length\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0.24033366045142296 ratio is nan album\n",
      "There is 0.24033366045142296 ratio is nan album\n",
      "There is 0.0017664376840039254 ratio is nan genre\n",
      "There is 0.24033366045142296 ratio is nan album_artist\n",
      "There is 0.0007850834151128558 ratio is nan track\n",
      "There is 0.6721295387634936 ratio is nan lyric\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Fill nan album\n",
    "print(\"There is {} ratio is nan album\".format(len(df[df[\"album\"].isnull()])/len(df)))\n",
    "print(\"There is {} ratio is nan album\".format(len(df[df[\"album\"].isnull()])/len(df)))\n",
    "\n",
    "df[\"album_right\"] = df.release_time.astype(\"category\").cat.codes\n",
    "df[\"albumHashAndName\"] = df[\"album_hash\"].fillna(df['album'])\n",
    "df[\"albumHashAndNameAndReleaseday\"] = df[\"albumHashAndName\"].fillna(df['album_right']).astype(\"category\").cat.codes\n",
    "assert df['albumHashAndNameAndReleaseday'].isnull().sum() == 0\n",
    "\n",
    "df[\"album_tmp\"]  = df[\"album\"].copy()\n",
    "\n",
    "df[\"album\"]  = df[\"album\"].fillna(\"\")\n",
    "df[\"len_album_name\"] = df[\"album\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemixAlbum\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isOSTAlbum\"] = [ 1 if \"OST\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isSingleAlbum\"] = [ 1 if \"Single\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isBeatAlbum\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isTopHitAlbum\"] = [ 1 if \"Top Hits\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isCoverAlbum\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isEPAlbum\"] = [ 1 if \"EP\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isLienKhucAlbum\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"album\"]]\n",
    "\n",
    "def no_bracket(tit):\n",
    "    return tit.split('(')[0]\n",
    "\n",
    "df['no_bracket_title'] = df.title.apply(lambda x: no_bracket(x))\n",
    "df[\"num_same_title_no_bracket\"] = df.groupby(\"no_bracket_title\")[\"no_bracket_title\"].transform(\"count\")\n",
    "df['no_bracket_title_cat'] = df['no_bracket_title'].astype('category').cat.codes\n",
    "\n",
    "df[\"isEDM\"] = [ 1 if \"EDM\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isDJ\"] = [ 1 if \"DJ\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isMix\"] = [ 1 if \"Mix\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isLive\"] = [ 1 if \"Live\" in t else 0 for t in df[\"title\"]]\n",
    "\n",
    "df[\"num_same_title\"] = df.groupby(\"title\")[\"title\"].transform(\"count\")\n",
    "df[\"title_cat\"] = df[\"title\"].astype('category').cat.codes\n",
    "df[\"album_name_is_title_name\"]= [1 if r.title in r.album  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album\"] = df[\"album\"].astype('category')\n",
    "df[\"album\"] =  df[\"album\"].cat.codes\n",
    "\n",
    "df[\"artist_name_cat\"] = df[\"artist_name\"].astype('category')\n",
    "df[\"artist_name_cat\"] =  df[\"artist_name_cat\"].cat.codes\n",
    "df[\"composers_name_cat\"] = df[\"composers_name\"].astype('category')\n",
    "df[\"composers_name_cat\"] =  df[\"composers_name_cat\"].cat.codes\n",
    "df[\"copyright_cat\"] = df[\"copyright\"].astype('category')\n",
    "df[\"copyright_cat\"] =  df[\"copyright_cat\"].cat.codes\n",
    "\n",
    "import re\n",
    "def get_min_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.min(ps)\n",
    "\n",
    "def get_max_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.max(ps)\n",
    "\n",
    "df[\"artist_id_min\"]=  df[\"artist_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"artist_id_min_cat\"] = df[\"artist_id_min\"].astype('category')\n",
    "df[\"artist_id_min_cat\"] =  df[\"artist_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_min\"]=  df[\"composers_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"composers_id_min_cat\"] = df[\"composers_id_min\"].astype('category')\n",
    "df[\"composers_id_min_cat\"] =  df[\"composers_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"artist_id_max\"]=  df[\"artist_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"artist_id_max_cat\"] = df[\"artist_id_max\"].astype('category')\n",
    "df[\"artist_id_max_cat\"] =  df[\"artist_id_max_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_max\"]=  df[\"composers_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"composers_id_max_cat\"] = df[\"composers_id_max\"].astype('category')\n",
    "df[\"composers_id_max_cat\"] =  df[\"composers_id_max_cat\"].cat.codes\n",
    "\n",
    "#New feature\n",
    "# df[\"group_album_artist_id_min_cat\"] = df.groupby([\"album\",\"artist_id_min_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_min_cat\"] = df[\"group_album_artist_id_min_cat\"].astype(\"category\").cat.codes\n",
    "# df[\"group_album_artist_id_max_cat\"] = df.groupby([\"album\",\"artist_id_max_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_max_cat\"] = df[\"group_album_artist_id_max_cat\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Fill genre\n",
    "print(\"There is {} ratio is nan genre\".format(len(df[df[\"genre\"].isnull()])/len(df)))\n",
    "df[\"genre\"]  = df[\"genre\"].fillna(\"No genre\")\n",
    "df[\"genre\"] = df[\"genre\"].astype('category')\n",
    "df[\"genre\"] =  df[\"genre\"].cat.codes\n",
    "\n",
    "# Fill album_artist\n",
    "print(\"There is {} ratio is nan album_artist\".format(len(df[df[\"album_artist\"].isnull()])/len(df)))\n",
    "df[\"album_artist\"]  = df[\"album_artist\"].fillna(\"No album_artist\")\n",
    "df[\"album_artist_contain_artistname\"]= [1 if r.album_artist in r.artist_name  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album_artist\"] = df[\"album_artist\"].astype('category')\n",
    "df[\"album_artist\"] =  df[\"album_artist\"].cat.codes\n",
    "\n",
    "# Fill track\n",
    "print(\"There is {} ratio is nan track\".format(len(df[df[\"track\"].isnull()])/len(df)))\n",
    "df[\"track_tmp\"]  = df[\"track\"].copy()\n",
    "df[\"track\"]  = df[\"track\"].fillna(\"(1, 1)\")\n",
    "df[\"istrack11\"] = df[\"track\"] == \"(1, 1)\"\n",
    "def tracknum_to_value(track_num):\n",
    "    try:\n",
    "        \n",
    "        track_num = make_tuple(track_num)\n",
    "        if track_num[0] is not None:\n",
    "            return float(track_num[0]) / float(track_num[1])\n",
    "        else:\n",
    "            return 1.0\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "df[\"track\"] = df[\"track\"].apply(lambda t: tracknum_to_value(t))\n",
    "\n",
    "\n",
    "# Fill lyric\n",
    "print(\"There is {} ratio is nan lyric\".format(len(df[df[\"lyric\"].isnull()])/len(df)))\n",
    "df[\"lyric\"]  = df[\"lyric\"].fillna(\"\")\n",
    "df[\"islyric\"] = df[\"lyric\"].apply(lambda x:  True if len(x)  else False)\n",
    "df[\"num_line_lyric\"] = df[\"lyric\"].apply(lambda x : len(x.split(\"\\r\")))\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "from dateutil import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ast import literal_eval as make_tuple\n",
    "df['no_artist'] = df.artist_name.apply(lambda x: len(x.split(\",\")))\n",
    "df['no_composer'] = df.composers_name.apply(lambda x: len(x.split(\",\")))\n",
    "df[\"freq_artist\"] = df.groupby('artist_id')['artist_id'].transform('count').astype('float')\n",
    "df[\"freq_composer\"] = df.groupby('composers_id')['composers_id'].transform('count').astype('float')\n",
    "df[\"freq_artist_min\"] = df.groupby('artist_id_min_cat')['artist_id_min_cat'].transform('count').astype('float')\n",
    "df[\"freq_composer_min\"] = df.groupby('composers_id_min_cat')['composers_id_min_cat'].transform('count').astype('float')\n",
    "\n",
    "df[\"num_album_per_min_artist\"] = df.groupby(['artist_id_min_cat','album_right'])['album_right'].transform('count').astype('float')\n",
    "df[\"num_album_per_min_composer\"] = df.groupby(['composers_id_min','album_right'])['album_right'].transform('count').astype('float')\n",
    "\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df.release_time)\n",
    "df[\"year\"] = df[\"datetime\"].dt.year\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"day\"] = df[\"datetime\"].dt.day\n",
    "df[\"dayofyear\"] = df[\"datetime\"].dt.dayofyear\n",
    "df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "from datetime import date \n",
    "import holidays \n",
    "\n",
    "in_holidays = holidays.HolidayBase() \n",
    "for i in range(26,32):\n",
    "    in_holidays.append(str(i)+'-01-2017')\n",
    "in_holidays.append('01-02-2017')\n",
    "for i in range(14,21):\n",
    "    in_holidays.append(str(i)+'-02-2018')\n",
    "in_holidays.append('30-04-2017')\n",
    "in_holidays.append('30-04-2018')\n",
    "in_holidays.append('01-01-2017')\n",
    "in_holidays.append('01-01-2018')\n",
    "in_holidays.append('14-02-2017')\n",
    "in_holidays.append('14-02-2018')\n",
    "in_holidays.append('08-03-2017')\n",
    "in_holidays.append('08-03-2018')\n",
    "in_holidays.append('01-05-2017')\n",
    "in_holidays.append('01-05-2018')\n",
    "in_holidays.append('06-04-2017')\n",
    "in_holidays.append('25-04-2018')\n",
    "in_holidays.append('01-06-2017')\n",
    "in_holidays.append('01-06-2018')\n",
    "in_holidays.append('04-10-2017')\n",
    "in_holidays.append('24-09-2018')\n",
    "in_holidays.append('20-10-2017')\n",
    "in_holidays.append('20-10-2018')\n",
    "in_holidays.append('20-11-2017')\n",
    "in_holidays.append('20-11-2018')\n",
    "in_holidays.append('24-12-2017')\n",
    "in_holidays.append('24-12-2018')\n",
    "df['isHoliday'] = df.release_time.apply(lambda x: x in in_holidays)\n",
    "\n",
    "\n",
    "\n",
    "df[\"len_of_songname\"] = df[\"title\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemix\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isOST\"] = [ 1 if \"OST\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isBeat\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isVersion\"] = [ 1 if \"Version\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isCover\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isLienKhuc\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"title\"]]\n",
    "\n",
    "\n",
    "\n",
    "def find_num_song_release_in_final_month(df, day):\n",
    "    month5th = day + relativedelta.relativedelta(months=5)\n",
    "    month6th = day + relativedelta.relativedelta(months=6)  \n",
    "    return len(df.datetime[(df.datetime >= month5th)&(df.datetime<=month6th)])\n",
    "\n",
    "\n",
    "\n",
    "df[\"num_song_release_in_final_month\"] = df.datetime.apply(lambda d:find_num_song_release_in_final_month(df ,d))\n",
    "\n",
    "#It seems like all songs on albums release at the same time, so groupby by release_time will create album \n",
    "\n",
    "df[\"day_release\"] = df.groupby([\"year\",\"dayofyear\"]).ngroup().astype(\"category\").cat.codes\n",
    "df[\"numsongInAlbum\"] = df.groupby(\"album_right\")[\"album_right\"].transform(\"count\")\n",
    "df[\"isSingleAlbum_onesong\"]= df[\"isSingleAlbum\"] & (df[\"numsongInAlbum\"]==1)\n",
    "\n",
    "# Remove len =0\n",
    "df = df[(df.length>0) | (df.num_same_title==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100957)\n",
    "df = df.sort_values(by=['albumHashAndNameAndReleaseday'])\n",
    "print(len(df))\n",
    "df[[\"ID\",\"title\", \"album_tmp\", \"album_hash\", \"albumHashAndNameAndReleaseday\",\"artist_name\", \"track_tmp\",\"release_time\",\"length\",\"label\"]].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def split_id(s):\n",
    "    return re.split(',|\\.',s)\n",
    "\n",
    "m = df.artist_id.unique()\n",
    "idx_lst = []\n",
    "for idx in m:\n",
    "    ps = split_id(idx)\n",
    "    for i in ps:\n",
    "        idx_lst.append(i)\n",
    "        \n",
    "id_lst = list(set(idx_lst))\n",
    "\n",
    "def condition(df, artist_id):\n",
    "    r = df.artist_id.apply(lambda x: artist_id in split_id(x))\n",
    "    return r\n",
    "\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "data= [df_train[condition(df_train, artist_id)].label.agg([\"mean\",\"std\",\"count\"]) for artist_id in id_lst]\n",
    "new_df = pd.DataFrame(data=data)\n",
    "new_df[\"artist_id\"] =  id_lst\n",
    "\n",
    "new_df.dropna(inplace=True)\n",
    "new_df.set_index('artist_id', inplace=True)\n",
    "art_dict = new_df.to_dict()\n",
    "\n",
    "def best_count_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_mean = 0\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['count'][id] > temp_mean:\n",
    "                temp_mean = art_dict['count'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_mean = temp_mean\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "    \n",
    "df['artist_count_id'] = df['artist_id'].apply(best_count_id)\n",
    "\n",
    "def best_mean_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_mean = 10\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['mean'][id] < temp_mean:\n",
    "                temp_mean = art_dict['mean'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_mean = temp_mean\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "\n",
    "df['artist_mean_id'] = df['artist_id'].apply(best_mean_id)\n",
    "\n",
    "def best_std_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_std = 10\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['std'][id] < temp_std:\n",
    "                temp_std = art_dict['std'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_std = temp_std\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "\n",
    "df['artist_std_id'] = df['artist_id'].apply(best_std_id)\n",
    "\n",
    "df['artist_mean_id'] = df['artist_mean_id'].astype(\"category\")\n",
    "df['artist_std_id'] = df['artist_std_id'].astype(\"category\")\n",
    "df['artist_count_id'] = df['artist_count_id'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def create_album_score_lookup_table(df):\n",
    "    data = df.groupby('album_right').label.agg([\"mean\",\"std\",\"count\"])\n",
    "    return data\n",
    "\n",
    "def create_artist_score_lookup_table(df):\n",
    "    def split_id(s):\n",
    "        return re.split(',|\\.',s)\n",
    "    \n",
    "    def mask_row_contain_artist_id(df, artist_id):\n",
    "        r = df.artist_id.apply(lambda x: artist_id in split_id(x))\n",
    "        return r\n",
    "    \n",
    "    # Get all artist ids\n",
    "    artist_group = df.artist_id.unique()\n",
    "    artist_ids = reduce(lambda l,e: l+split_id(e), artist_group, [])\n",
    "    artist_ids = list(set(artist_ids))\n",
    "    # Get data\n",
    "    data= [df[mask_row_contain_artist_id(df, artist_id)].label.agg([\"mean\",\"std\",\"count\", \"median\"]) \n",
    "                                                        for artist_id in artist_ids]\n",
    "    new_df = pd.DataFrame(data=data)\n",
    "    new_df[\"artist_id\"] =  artist_ids\n",
    "    return new_df.set_index(\"artist_id\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_by_key(table, k, field=\"mean\"):\n",
    "    if k in table.index:\n",
    "        return table.loc[k][field]\n",
    "    return np.nan\n",
    "\n",
    "def get_value_by_key(table, k):\n",
    "    if k in table.index:\n",
    "        return table.loc[k], False\n",
    "    return np.nan, True\n",
    "\n",
    "def assign_value(album_table, artist_table, r):\n",
    "    d1,isnul1 = get_value_by_key(album_table, r.album_right)\n",
    "    d2,isnul2 = get_value_by_key(artist_table, r.artist_id_min_cat)\n",
    "#     print(type(d2),isnul2)\n",
    "    if isnul1 and isnul2:\n",
    "        return np.nan\n",
    "    elif isnul1 and d2[\"std\"] <2:\n",
    "        return d2[\"mean\"]\n",
    "    elif isnul2 and d1[\"std\"] <2:\n",
    "        return d1[\"mean\"]\n",
    "    \n",
    "    elif not isnul1 and d1[\"std\"] <2 and not isnul2 and d2[\"std\"] < 2:\n",
    "        return d1[\"mean\"] \n",
    "    \n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "def assign_value_album(album_table, r):\n",
    "    d1,isnul1 = get_value_by_key(album_table, r.album_right)\n",
    "    \n",
    "    if not isnul1 and d1[\"std\"] <1.75:\n",
    "        return d1[\"std\"]\n",
    "    \n",
    "    return np.nan\n",
    "    \n",
    "def assign_value_artist(artist_table, r):\n",
    "    d1,isnul1 = get_value_by_key(artist_table, r.artist_id_min_cat)\n",
    "    \n",
    "    if not isnul1 and d1[\"std\"] <1.75:\n",
    "        return d1[\"std\"]\n",
    "    \n",
    "    return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.artist_mean_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xam\n",
    "\n",
    "encoder = xam.feature_extraction.BayesianTargetEncoder(\n",
    "    columns=['album_right', 'artist_mean_id',\"artist_id_min_cat\",\n",
    "             \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\"],\n",
    "    prior_weight=10000,\n",
    "    suffix='_encode')\n",
    "\n",
    "# df_train = df[df.dataset==\"train\"]\n",
    "\n",
    "target_encode = encoder.fit_transform(df[['album_right', 'artist_mean_id',\"artist_id_min_cat\",\n",
    "             \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\"]], df.label )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baysianEncodeFeature(df_train, trn_idx, featurename, prior_weight, fillmissing, suffix='_baysencoded'):\n",
    "    '''Returns new df '''\n",
    "    import xam\n",
    "\n",
    "    encoder = xam.feature_extraction.BayesianTargetEncoder(\n",
    "        columns=[featurename, ],\n",
    "        prior_weight=prior_weight,\n",
    "        suffix=suffix)\n",
    "\n",
    "    train_df_fold = df_train.iloc[trn_idx]\n",
    "\n",
    "    encoder.fit(train_df_fold[[featurename]], train_df_fold.label)\n",
    "\n",
    "    _resulting_df = encoder.transform(df_train[[featurename]], df_train.label)\n",
    "    _resulting_df[featurename + suffix] = _resulting_df[featurename + suffix].astype('float64')\n",
    "    _resulting_df[featurename + suffix].fillna(fillmissing, inplace=True)\n",
    "\n",
    "    # Add the column to original df_train\n",
    "    df_train[featurename + suffix] = _resulting_df[featurename + suffix].round(0).astype('int64')\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuthede/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage null in valid: 0.22941822173435786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuthede/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53395\tvalid_1's rmse: 1.81977\n",
      "[10000]\ttraining's rmse: 1.2448\tvalid_1's rmse: 1.68537\n",
      "[15000]\ttraining's rmse: 1.08578\tvalid_1's rmse: 1.63256\n",
      "[20000]\ttraining's rmse: 0.97271\tvalid_1's rmse: 1.60373\n",
      "[25000]\ttraining's rmse: 0.885039\tvalid_1's rmse: 1.58524\n",
      "[30000]\ttraining's rmse: 0.813652\tvalid_1's rmse: 1.5733\n",
      "[35000]\ttraining's rmse: 0.752916\tvalid_1's rmse: 1.56393\n",
      "[40000]\ttraining's rmse: 0.699852\tvalid_1's rmse: 1.55654\n",
      "[45000]\ttraining's rmse: 0.653755\tvalid_1's rmse: 1.55077\n",
      "[50000]\ttraining's rmse: 0.61199\tvalid_1's rmse: 1.54604\n",
      "[55000]\ttraining's rmse: 0.574849\tvalid_1's rmse: 1.54237\n",
      "[60000]\ttraining's rmse: 0.541014\tvalid_1's rmse: 1.53942\n",
      "[65000]\ttraining's rmse: 0.510339\tvalid_1's rmse: 1.53666\n",
      "[70000]\ttraining's rmse: 0.482156\tvalid_1's rmse: 1.53471\n",
      "[75000]\ttraining's rmse: 0.45631\tvalid_1's rmse: 1.5331\n",
      "[80000]\ttraining's rmse: 0.432321\tvalid_1's rmse: 1.53151\n",
      "[85000]\ttraining's rmse: 0.410306\tvalid_1's rmse: 1.53058\n",
      "[90000]\ttraining's rmse: 0.389748\tvalid_1's rmse: 1.52956\n",
      "[95000]\ttraining's rmse: 0.370591\tvalid_1's rmse: 1.52889\n",
      "[100000]\ttraining's rmse: 0.352439\tvalid_1's rmse: 1.52835\n",
      "[105000]\ttraining's rmse: 0.336035\tvalid_1's rmse: 1.52785\n",
      "[110000]\ttraining's rmse: 0.320476\tvalid_1's rmse: 1.52755\n",
      "[115000]\ttraining's rmse: 0.305951\tvalid_1's rmse: 1.52722\n",
      "[120000]\ttraining's rmse: 0.29222\tvalid_1's rmse: 1.52693\n",
      "[125000]\ttraining's rmse: 0.279449\tvalid_1's rmse: 1.52682\n",
      "[130000]\ttraining's rmse: 0.267191\tvalid_1's rmse: 1.52688\n",
      "[135000]\ttraining's rmse: 0.255695\tvalid_1's rmse: 1.52688\n",
      "[140000]\ttraining's rmse: 0.245018\tvalid_1's rmse: 1.52692\n",
      "[145000]\ttraining's rmse: 0.234752\tvalid_1's rmse: 1.52699\n",
      "Early stopping, best iteration is:\n",
      "[126341]\ttraining's rmse: 0.276191\tvalid_1's rmse: 1.52678\n",
      "Fold 1\n",
      "Percentage null in valid: 0.22941822173435786\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53586\tvalid_1's rmse: 1.77662\n",
      "[10000]\ttraining's rmse: 1.24051\tvalid_1's rmse: 1.65005\n",
      "[15000]\ttraining's rmse: 1.07954\tvalid_1's rmse: 1.60735\n",
      "[20000]\ttraining's rmse: 0.965281\tvalid_1's rmse: 1.5874\n",
      "[25000]\ttraining's rmse: 0.877898\tvalid_1's rmse: 1.57595\n",
      "[30000]\ttraining's rmse: 0.806453\tvalid_1's rmse: 1.56841\n",
      "[35000]\ttraining's rmse: 0.745678\tvalid_1's rmse: 1.56319\n",
      "[40000]\ttraining's rmse: 0.692822\tvalid_1's rmse: 1.55893\n",
      "[45000]\ttraining's rmse: 0.646799\tvalid_1's rmse: 1.5559\n",
      "[50000]\ttraining's rmse: 0.605283\tvalid_1's rmse: 1.55338\n",
      "[55000]\ttraining's rmse: 0.568481\tvalid_1's rmse: 1.55116\n",
      "[60000]\ttraining's rmse: 0.534984\tvalid_1's rmse: 1.54973\n",
      "[65000]\ttraining's rmse: 0.50442\tvalid_1's rmse: 1.54824\n",
      "[70000]\ttraining's rmse: 0.476372\tvalid_1's rmse: 1.5471\n",
      "[75000]\ttraining's rmse: 0.450976\tvalid_1's rmse: 1.54624\n",
      "[80000]\ttraining's rmse: 0.427401\tvalid_1's rmse: 1.54559\n",
      "[85000]\ttraining's rmse: 0.405574\tvalid_1's rmse: 1.54509\n",
      "[90000]\ttraining's rmse: 0.385195\tvalid_1's rmse: 1.5446\n",
      "[95000]\ttraining's rmse: 0.366235\tvalid_1's rmse: 1.54424\n",
      "[100000]\ttraining's rmse: 0.34843\tvalid_1's rmse: 1.54413\n",
      "[105000]\ttraining's rmse: 0.33233\tvalid_1's rmse: 1.54375\n",
      "[110000]\ttraining's rmse: 0.317222\tvalid_1's rmse: 1.54356\n",
      "[115000]\ttraining's rmse: 0.303081\tvalid_1's rmse: 1.54339\n",
      "[120000]\ttraining's rmse: 0.289647\tvalid_1's rmse: 1.54316\n",
      "[125000]\ttraining's rmse: 0.277095\tvalid_1's rmse: 1.54299\n",
      "[130000]\ttraining's rmse: 0.265317\tvalid_1's rmse: 1.54289\n",
      "[135000]\ttraining's rmse: 0.254183\tvalid_1's rmse: 1.54284\n",
      "[140000]\ttraining's rmse: 0.243757\tvalid_1's rmse: 1.54274\n",
      "[145000]\ttraining's rmse: 0.233737\tvalid_1's rmse: 1.54276\n",
      "[150000]\ttraining's rmse: 0.224394\tvalid_1's rmse: 1.54273\n",
      "[155000]\ttraining's rmse: 0.215577\tvalid_1's rmse: 1.54282\n",
      "[160000]\ttraining's rmse: 0.20702\tvalid_1's rmse: 1.54283\n",
      "Early stopping, best iteration is:\n",
      "[143518]\ttraining's rmse: 0.236649\tvalid_1's rmse: 1.5427\n",
      "Fold 2\n",
      "Percentage null in valid: 0.2299229922992299\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.52724\tvalid_1's rmse: 1.84747\n",
      "[10000]\ttraining's rmse: 1.23765\tvalid_1's rmse: 1.71817\n",
      "[15000]\ttraining's rmse: 1.07983\tvalid_1's rmse: 1.66533\n",
      "[20000]\ttraining's rmse: 0.967371\tvalid_1's rmse: 1.63617\n",
      "[25000]\ttraining's rmse: 0.880361\tvalid_1's rmse: 1.61655\n",
      "[30000]\ttraining's rmse: 0.809514\tvalid_1's rmse: 1.60331\n",
      "[35000]\ttraining's rmse: 0.748993\tvalid_1's rmse: 1.59229\n",
      "[40000]\ttraining's rmse: 0.696139\tvalid_1's rmse: 1.58406\n",
      "[45000]\ttraining's rmse: 0.650126\tvalid_1's rmse: 1.57723\n",
      "[50000]\ttraining's rmse: 0.609063\tvalid_1's rmse: 1.57188\n",
      "[55000]\ttraining's rmse: 0.571954\tvalid_1's rmse: 1.56751\n",
      "[60000]\ttraining's rmse: 0.538426\tvalid_1's rmse: 1.56378\n",
      "[65000]\ttraining's rmse: 0.507824\tvalid_1's rmse: 1.56082\n",
      "[70000]\ttraining's rmse: 0.479512\tvalid_1's rmse: 1.55863\n",
      "[75000]\ttraining's rmse: 0.453945\tvalid_1's rmse: 1.55672\n",
      "[80000]\ttraining's rmse: 0.430339\tvalid_1's rmse: 1.55491\n",
      "[85000]\ttraining's rmse: 0.408187\tvalid_1's rmse: 1.55354\n",
      "[90000]\ttraining's rmse: 0.387656\tvalid_1's rmse: 1.55238\n",
      "[95000]\ttraining's rmse: 0.368582\tvalid_1's rmse: 1.55145\n",
      "[100000]\ttraining's rmse: 0.350695\tvalid_1's rmse: 1.55059\n",
      "[105000]\ttraining's rmse: 0.334231\tvalid_1's rmse: 1.55003\n",
      "[110000]\ttraining's rmse: 0.318674\tvalid_1's rmse: 1.54933\n",
      "[115000]\ttraining's rmse: 0.304129\tvalid_1's rmse: 1.54884\n",
      "[120000]\ttraining's rmse: 0.29039\tvalid_1's rmse: 1.54852\n",
      "[125000]\ttraining's rmse: 0.277644\tvalid_1's rmse: 1.54803\n",
      "[130000]\ttraining's rmse: 0.265454\tvalid_1's rmse: 1.5477\n",
      "[135000]\ttraining's rmse: 0.254062\tvalid_1's rmse: 1.54742\n",
      "[140000]\ttraining's rmse: 0.243458\tvalid_1's rmse: 1.54729\n",
      "[145000]\ttraining's rmse: 0.233286\tvalid_1's rmse: 1.54721\n",
      "[150000]\ttraining's rmse: 0.223837\tvalid_1's rmse: 1.54723\n",
      "[155000]\ttraining's rmse: 0.214877\tvalid_1's rmse: 1.54701\n",
      "[160000]\ttraining's rmse: 0.206172\tvalid_1's rmse: 1.54687\n",
      "[165000]\ttraining's rmse: 0.19813\tvalid_1's rmse: 1.54679\n",
      "[170000]\ttraining's rmse: 0.190461\tvalid_1's rmse: 1.54673\n",
      "[175000]\ttraining's rmse: 0.183131\tvalid_1's rmse: 1.54675\n",
      "[180000]\ttraining's rmse: 0.176146\tvalid_1's rmse: 1.5467\n",
      "[185000]\ttraining's rmse: 0.169639\tvalid_1's rmse: 1.54663\n",
      "[190000]\ttraining's rmse: 0.163447\tvalid_1's rmse: 1.54665\n",
      "[195000]\ttraining's rmse: 0.157487\tvalid_1's rmse: 1.54659\n",
      "[200000]\ttraining's rmse: 0.151795\tvalid_1's rmse: 1.54656\n",
      "[205000]\ttraining's rmse: 0.146445\tvalid_1's rmse: 1.54659\n",
      "[210000]\ttraining's rmse: 0.141323\tvalid_1's rmse: 1.54668\n",
      "[215000]\ttraining's rmse: 0.136438\tvalid_1's rmse: 1.54671\n",
      "[220000]\ttraining's rmse: 0.131795\tvalid_1's rmse: 1.54677\n",
      "Early stopping, best iteration is:\n",
      "[201773]\ttraining's rmse: 0.149829\tvalid_1's rmse: 1.54651\n",
      "Fold 3\n",
      "Percentage null in valid: 0.2299229922992299\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.52792\tvalid_1's rmse: 1.86018\n",
      "[10000]\ttraining's rmse: 1.23566\tvalid_1's rmse: 1.73023\n",
      "[15000]\ttraining's rmse: 1.07593\tvalid_1's rmse: 1.68156\n",
      "[20000]\ttraining's rmse: 0.96248\tvalid_1's rmse: 1.65706\n",
      "[25000]\ttraining's rmse: 0.87489\tvalid_1's rmse: 1.64127\n",
      "[30000]\ttraining's rmse: 0.803889\tvalid_1's rmse: 1.63212\n",
      "[35000]\ttraining's rmse: 0.743539\tvalid_1's rmse: 1.62507\n",
      "[40000]\ttraining's rmse: 0.690709\tvalid_1's rmse: 1.62004\n",
      "[45000]\ttraining's rmse: 0.644797\tvalid_1's rmse: 1.61665\n",
      "[50000]\ttraining's rmse: 0.603642\tvalid_1's rmse: 1.61426\n",
      "[55000]\ttraining's rmse: 0.566747\tvalid_1's rmse: 1.61252\n",
      "[60000]\ttraining's rmse: 0.53334\tvalid_1's rmse: 1.61126\n",
      "[65000]\ttraining's rmse: 0.502721\tvalid_1's rmse: 1.60991\n",
      "[70000]\ttraining's rmse: 0.474761\tvalid_1's rmse: 1.60916\n",
      "[75000]\ttraining's rmse: 0.449187\tvalid_1's rmse: 1.6088\n",
      "[80000]\ttraining's rmse: 0.425696\tvalid_1's rmse: 1.60845\n",
      "[85000]\ttraining's rmse: 0.40359\tvalid_1's rmse: 1.60825\n",
      "[90000]\ttraining's rmse: 0.383323\tvalid_1's rmse: 1.60812\n",
      "[95000]\ttraining's rmse: 0.364556\tvalid_1's rmse: 1.60799\n",
      "[100000]\ttraining's rmse: 0.346836\tvalid_1's rmse: 1.60823\n",
      "[105000]\ttraining's rmse: 0.330686\tvalid_1's rmse: 1.6084\n",
      "[110000]\ttraining's rmse: 0.315482\tvalid_1's rmse: 1.6085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[92486]\ttraining's rmse: 0.373836\tvalid_1's rmse: 1.60794\n",
      "Fold 4\n",
      "Percentage null in valid: 0.23042998897464168\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.52696\tvalid_1's rmse: 1.88255\n",
      "[10000]\ttraining's rmse: 1.23713\tvalid_1's rmse: 1.73801\n",
      "[15000]\ttraining's rmse: 1.07788\tvalid_1's rmse: 1.68041\n",
      "[20000]\ttraining's rmse: 0.965075\tvalid_1's rmse: 1.64837\n",
      "[25000]\ttraining's rmse: 0.877592\tvalid_1's rmse: 1.62779\n",
      "[30000]\ttraining's rmse: 0.806533\tvalid_1's rmse: 1.61436\n",
      "[35000]\ttraining's rmse: 0.745595\tvalid_1's rmse: 1.60396\n",
      "[40000]\ttraining's rmse: 0.691807\tvalid_1's rmse: 1.59633\n",
      "[45000]\ttraining's rmse: 0.645221\tvalid_1's rmse: 1.58994\n",
      "[50000]\ttraining's rmse: 0.603603\tvalid_1's rmse: 1.58534\n",
      "[55000]\ttraining's rmse: 0.566336\tvalid_1's rmse: 1.58151\n",
      "[60000]\ttraining's rmse: 0.532497\tvalid_1's rmse: 1.57867\n",
      "[65000]\ttraining's rmse: 0.501803\tvalid_1's rmse: 1.57603\n",
      "[70000]\ttraining's rmse: 0.473507\tvalid_1's rmse: 1.57394\n",
      "[75000]\ttraining's rmse: 0.447414\tvalid_1's rmse: 1.57227\n",
      "[80000]\ttraining's rmse: 0.423494\tvalid_1's rmse: 1.57113\n",
      "[85000]\ttraining's rmse: 0.401411\tvalid_1's rmse: 1.56994\n",
      "[90000]\ttraining's rmse: 0.380984\tvalid_1's rmse: 1.56915\n",
      "[95000]\ttraining's rmse: 0.361994\tvalid_1's rmse: 1.56852\n",
      "[100000]\ttraining's rmse: 0.34399\tvalid_1's rmse: 1.56778\n",
      "[105000]\ttraining's rmse: 0.327729\tvalid_1's rmse: 1.56732\n",
      "[110000]\ttraining's rmse: 0.312297\tvalid_1's rmse: 1.56697\n",
      "[115000]\ttraining's rmse: 0.297966\tvalid_1's rmse: 1.56669\n",
      "[120000]\ttraining's rmse: 0.28436\tvalid_1's rmse: 1.56658\n",
      "[125000]\ttraining's rmse: 0.271677\tvalid_1's rmse: 1.5666\n",
      "[130000]\ttraining's rmse: 0.259682\tvalid_1's rmse: 1.56644\n",
      "[135000]\ttraining's rmse: 0.248463\tvalid_1's rmse: 1.5664\n",
      "[140000]\ttraining's rmse: 0.237915\tvalid_1's rmse: 1.56635\n",
      "[145000]\ttraining's rmse: 0.22784\tvalid_1's rmse: 1.56623\n",
      "[150000]\ttraining's rmse: 0.218531\tvalid_1's rmse: 1.56609\n",
      "[155000]\ttraining's rmse: 0.209647\tvalid_1's rmse: 1.56619\n",
      "[160000]\ttraining's rmse: 0.201088\tvalid_1's rmse: 1.5663\n",
      "[165000]\ttraining's rmse: 0.19308\tvalid_1's rmse: 1.56635\n",
      "Early stopping, best iteration is:\n",
      "[149820]\ttraining's rmse: 0.21888\tvalid_1's rmse: 1.56607\n",
      "Fold 5\n",
      "Percentage null in valid: 0.23042998897464168\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53302\tvalid_1's rmse: 1.86122\n",
      "[10000]\ttraining's rmse: 1.24312\tvalid_1's rmse: 1.72705\n",
      "[15000]\ttraining's rmse: 1.0834\tvalid_1's rmse: 1.67548\n",
      "[20000]\ttraining's rmse: 0.969915\tvalid_1's rmse: 1.64864\n",
      "[25000]\ttraining's rmse: 0.882078\tvalid_1's rmse: 1.63167\n",
      "[30000]\ttraining's rmse: 0.810697\tvalid_1's rmse: 1.6207\n",
      "[35000]\ttraining's rmse: 0.749966\tvalid_1's rmse: 1.61207\n",
      "[40000]\ttraining's rmse: 0.696825\tvalid_1's rmse: 1.60532\n",
      "[45000]\ttraining's rmse: 0.6508\tvalid_1's rmse: 1.59995\n",
      "[50000]\ttraining's rmse: 0.609371\tvalid_1's rmse: 1.59534\n",
      "[55000]\ttraining's rmse: 0.572272\tvalid_1's rmse: 1.59155\n",
      "[60000]\ttraining's rmse: 0.5384\tvalid_1's rmse: 1.58852\n",
      "[65000]\ttraining's rmse: 0.507687\tvalid_1's rmse: 1.58578\n",
      "[70000]\ttraining's rmse: 0.479682\tvalid_1's rmse: 1.58343\n",
      "[75000]\ttraining's rmse: 0.453771\tvalid_1's rmse: 1.58151\n",
      "[80000]\ttraining's rmse: 0.430091\tvalid_1's rmse: 1.57991\n",
      "[85000]\ttraining's rmse: 0.40825\tvalid_1's rmse: 1.57838\n",
      "[90000]\ttraining's rmse: 0.387876\tvalid_1's rmse: 1.5772\n",
      "[95000]\ttraining's rmse: 0.368826\tvalid_1's rmse: 1.57621\n",
      "[100000]\ttraining's rmse: 0.350848\tvalid_1's rmse: 1.57517\n",
      "[105000]\ttraining's rmse: 0.334502\tvalid_1's rmse: 1.57431\n",
      "[110000]\ttraining's rmse: 0.319109\tvalid_1's rmse: 1.57363\n",
      "[115000]\ttraining's rmse: 0.304806\tvalid_1's rmse: 1.57285\n",
      "[120000]\ttraining's rmse: 0.291134\tvalid_1's rmse: 1.57235\n",
      "[125000]\ttraining's rmse: 0.278535\tvalid_1's rmse: 1.5718\n",
      "[130000]\ttraining's rmse: 0.266577\tvalid_1's rmse: 1.57163\n",
      "[135000]\ttraining's rmse: 0.255215\tvalid_1's rmse: 1.57141\n",
      "[140000]\ttraining's rmse: 0.24457\tvalid_1's rmse: 1.57115\n",
      "[145000]\ttraining's rmse: 0.234409\tvalid_1's rmse: 1.57095\n",
      "[150000]\ttraining's rmse: 0.225005\tvalid_1's rmse: 1.5707\n",
      "[155000]\ttraining's rmse: 0.216047\tvalid_1's rmse: 1.57054\n",
      "[160000]\ttraining's rmse: 0.207346\tvalid_1's rmse: 1.57051\n",
      "[165000]\ttraining's rmse: 0.199248\tvalid_1's rmse: 1.5705\n",
      "[170000]\ttraining's rmse: 0.191596\tvalid_1's rmse: 1.57042\n",
      "[175000]\ttraining's rmse: 0.184251\tvalid_1's rmse: 1.57034\n",
      "[180000]\ttraining's rmse: 0.177328\tvalid_1's rmse: 1.57038\n",
      "[185000]\ttraining's rmse: 0.170905\tvalid_1's rmse: 1.57037\n",
      "[190000]\ttraining's rmse: 0.164676\tvalid_1's rmse: 1.5704\n",
      "Early stopping, best iteration is:\n",
      "[174901]\ttraining's rmse: 0.184392\tvalid_1's rmse: 1.57033\n",
      "Fold 6\n",
      "Percentage null in valid: 0.23119469026548672\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53873\tvalid_1's rmse: 1.79328\n",
      "[10000]\ttraining's rmse: 1.24926\tvalid_1's rmse: 1.64517\n",
      "[15000]\ttraining's rmse: 1.08999\tvalid_1's rmse: 1.58779\n",
      "[20000]\ttraining's rmse: 0.976738\tvalid_1's rmse: 1.55637\n",
      "[25000]\ttraining's rmse: 0.888573\tvalid_1's rmse: 1.53659\n",
      "[30000]\ttraining's rmse: 0.816369\tvalid_1's rmse: 1.52256\n",
      "[35000]\ttraining's rmse: 0.75501\tvalid_1's rmse: 1.51219\n",
      "[40000]\ttraining's rmse: 0.701383\tvalid_1's rmse: 1.50432\n",
      "[45000]\ttraining's rmse: 0.654683\tvalid_1's rmse: 1.49783\n",
      "[50000]\ttraining's rmse: 0.612515\tvalid_1's rmse: 1.49216\n",
      "[55000]\ttraining's rmse: 0.574819\tvalid_1's rmse: 1.48855\n",
      "[60000]\ttraining's rmse: 0.540435\tvalid_1's rmse: 1.48511\n",
      "[65000]\ttraining's rmse: 0.509321\tvalid_1's rmse: 1.48246\n",
      "[70000]\ttraining's rmse: 0.480676\tvalid_1's rmse: 1.48021\n",
      "[75000]\ttraining's rmse: 0.454626\tvalid_1's rmse: 1.47809\n",
      "[80000]\ttraining's rmse: 0.430674\tvalid_1's rmse: 1.47678\n",
      "[85000]\ttraining's rmse: 0.408372\tvalid_1's rmse: 1.4755\n",
      "[90000]\ttraining's rmse: 0.387853\tvalid_1's rmse: 1.47437\n",
      "[95000]\ttraining's rmse: 0.36864\tvalid_1's rmse: 1.47332\n",
      "[100000]\ttraining's rmse: 0.350521\tvalid_1's rmse: 1.47265\n",
      "[105000]\ttraining's rmse: 0.334077\tvalid_1's rmse: 1.47186\n",
      "[110000]\ttraining's rmse: 0.318537\tvalid_1's rmse: 1.47128\n",
      "[115000]\ttraining's rmse: 0.304159\tvalid_1's rmse: 1.47086\n",
      "[120000]\ttraining's rmse: 0.290404\tvalid_1's rmse: 1.47055\n",
      "[125000]\ttraining's rmse: 0.277792\tvalid_1's rmse: 1.47024\n",
      "[130000]\ttraining's rmse: 0.265688\tvalid_1's rmse: 1.46995\n",
      "[135000]\ttraining's rmse: 0.254296\tvalid_1's rmse: 1.46978\n",
      "[140000]\ttraining's rmse: 0.243642\tvalid_1's rmse: 1.46959\n",
      "[145000]\ttraining's rmse: 0.233534\tvalid_1's rmse: 1.46952\n",
      "[150000]\ttraining's rmse: 0.224074\tvalid_1's rmse: 1.46944\n",
      "[155000]\ttraining's rmse: 0.21514\tvalid_1's rmse: 1.46936\n",
      "[160000]\ttraining's rmse: 0.206505\tvalid_1's rmse: 1.46927\n",
      "[165000]\ttraining's rmse: 0.198476\tvalid_1's rmse: 1.46919\n",
      "[170000]\ttraining's rmse: 0.190846\tvalid_1's rmse: 1.46927\n",
      "[175000]\ttraining's rmse: 0.183552\tvalid_1's rmse: 1.46926\n",
      "[180000]\ttraining's rmse: 0.176596\tvalid_1's rmse: 1.46925\n",
      "Early stopping, best iteration is:\n",
      "[164747]\ttraining's rmse: 0.198861\tvalid_1's rmse: 1.46917\n",
      "Fold 7\n",
      "Percentage null in valid: 0.23145071982281284\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53038\tvalid_1's rmse: 1.8774\n",
      "[10000]\ttraining's rmse: 1.24464\tvalid_1's rmse: 1.73314\n",
      "[15000]\ttraining's rmse: 1.08734\tvalid_1's rmse: 1.67374\n",
      "[20000]\ttraining's rmse: 0.974514\tvalid_1's rmse: 1.64038\n",
      "[25000]\ttraining's rmse: 0.886921\tvalid_1's rmse: 1.61913\n",
      "[30000]\ttraining's rmse: 0.815185\tvalid_1's rmse: 1.60477\n",
      "[35000]\ttraining's rmse: 0.753822\tvalid_1's rmse: 1.59421\n",
      "[40000]\ttraining's rmse: 0.700364\tvalid_1's rmse: 1.58598\n",
      "[45000]\ttraining's rmse: 0.653647\tvalid_1's rmse: 1.57926\n",
      "[50000]\ttraining's rmse: 0.611402\tvalid_1's rmse: 1.57388\n",
      "[55000]\ttraining's rmse: 0.573857\tvalid_1's rmse: 1.5701\n",
      "[60000]\ttraining's rmse: 0.539409\tvalid_1's rmse: 1.56703\n",
      "[65000]\ttraining's rmse: 0.508236\tvalid_1's rmse: 1.56447\n",
      "[70000]\ttraining's rmse: 0.479608\tvalid_1's rmse: 1.56229\n",
      "[75000]\ttraining's rmse: 0.453545\tvalid_1's rmse: 1.56036\n",
      "[80000]\ttraining's rmse: 0.429442\tvalid_1's rmse: 1.55912\n",
      "[85000]\ttraining's rmse: 0.407051\tvalid_1's rmse: 1.55807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90000]\ttraining's rmse: 0.386453\tvalid_1's rmse: 1.55718\n",
      "[95000]\ttraining's rmse: 0.367415\tvalid_1's rmse: 1.55649\n",
      "[100000]\ttraining's rmse: 0.349272\tvalid_1's rmse: 1.5559\n",
      "[105000]\ttraining's rmse: 0.333035\tvalid_1's rmse: 1.55545\n",
      "[110000]\ttraining's rmse: 0.317607\tvalid_1's rmse: 1.55518\n",
      "[115000]\ttraining's rmse: 0.303134\tvalid_1's rmse: 1.55485\n",
      "[120000]\ttraining's rmse: 0.289403\tvalid_1's rmse: 1.5546\n",
      "[125000]\ttraining's rmse: 0.276807\tvalid_1's rmse: 1.55455\n",
      "[130000]\ttraining's rmse: 0.264806\tvalid_1's rmse: 1.55443\n",
      "[135000]\ttraining's rmse: 0.253479\tvalid_1's rmse: 1.55435\n",
      "[140000]\ttraining's rmse: 0.24279\tvalid_1's rmse: 1.55435\n",
      "[145000]\ttraining's rmse: 0.232591\tvalid_1's rmse: 1.55436\n",
      "[150000]\ttraining's rmse: 0.223134\tvalid_1's rmse: 1.55435\n",
      "[155000]\ttraining's rmse: 0.214172\tvalid_1's rmse: 1.55449\n",
      "[160000]\ttraining's rmse: 0.205584\tvalid_1's rmse: 1.55465\n",
      "[165000]\ttraining's rmse: 0.197581\tvalid_1's rmse: 1.5548\n",
      "Early stopping, best iteration is:\n",
      "[148140]\ttraining's rmse: 0.226598\tvalid_1's rmse: 1.55424\n",
      "Fold 8\n",
      "Percentage null in valid: 0.23170731707317074\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53148\tvalid_1's rmse: 1.87473\n",
      "[10000]\ttraining's rmse: 1.23862\tvalid_1's rmse: 1.73616\n",
      "[15000]\ttraining's rmse: 1.07829\tvalid_1's rmse: 1.68307\n",
      "[20000]\ttraining's rmse: 0.964488\tvalid_1's rmse: 1.65496\n",
      "[25000]\ttraining's rmse: 0.876243\tvalid_1's rmse: 1.63781\n",
      "[30000]\ttraining's rmse: 0.804103\tvalid_1's rmse: 1.62582\n",
      "[35000]\ttraining's rmse: 0.743169\tvalid_1's rmse: 1.61668\n",
      "[40000]\ttraining's rmse: 0.689799\tvalid_1's rmse: 1.61015\n",
      "[45000]\ttraining's rmse: 0.643937\tvalid_1's rmse: 1.6048\n",
      "[50000]\ttraining's rmse: 0.602711\tvalid_1's rmse: 1.60023\n",
      "[55000]\ttraining's rmse: 0.565725\tvalid_1's rmse: 1.59642\n",
      "[60000]\ttraining's rmse: 0.531886\tvalid_1's rmse: 1.5936\n",
      "[65000]\ttraining's rmse: 0.501317\tvalid_1's rmse: 1.59153\n",
      "[70000]\ttraining's rmse: 0.473356\tvalid_1's rmse: 1.58951\n",
      "[75000]\ttraining's rmse: 0.447776\tvalid_1's rmse: 1.5881\n",
      "[80000]\ttraining's rmse: 0.424049\tvalid_1's rmse: 1.58685\n",
      "[85000]\ttraining's rmse: 0.402228\tvalid_1's rmse: 1.5859\n",
      "[90000]\ttraining's rmse: 0.382015\tvalid_1's rmse: 1.58518\n",
      "[95000]\ttraining's rmse: 0.363303\tvalid_1's rmse: 1.58446\n",
      "[100000]\ttraining's rmse: 0.345669\tvalid_1's rmse: 1.58389\n",
      "[105000]\ttraining's rmse: 0.329649\tvalid_1's rmse: 1.58337\n",
      "[110000]\ttraining's rmse: 0.314462\tvalid_1's rmse: 1.5831\n",
      "[115000]\ttraining's rmse: 0.300223\tvalid_1's rmse: 1.58292\n",
      "[120000]\ttraining's rmse: 0.286833\tvalid_1's rmse: 1.58286\n",
      "[125000]\ttraining's rmse: 0.274298\tvalid_1's rmse: 1.58267\n",
      "[130000]\ttraining's rmse: 0.262358\tvalid_1's rmse: 1.58241\n",
      "[135000]\ttraining's rmse: 0.251217\tvalid_1's rmse: 1.58241\n",
      "[140000]\ttraining's rmse: 0.240656\tvalid_1's rmse: 1.58249\n",
      "[145000]\ttraining's rmse: 0.23058\tvalid_1's rmse: 1.58233\n",
      "[150000]\ttraining's rmse: 0.22127\tvalid_1's rmse: 1.58232\n",
      "[155000]\ttraining's rmse: 0.212519\tvalid_1's rmse: 1.5824\n",
      "[160000]\ttraining's rmse: 0.203969\tvalid_1's rmse: 1.58249\n",
      "[165000]\ttraining's rmse: 0.196001\tvalid_1's rmse: 1.58258\n",
      "[170000]\ttraining's rmse: 0.188504\tvalid_1's rmse: 1.58273\n",
      "Early stopping, best iteration is:\n",
      "[150578]\ttraining's rmse: 0.22023\tvalid_1's rmse: 1.58229\n",
      "Fold 9\n",
      "Percentage null in valid: 0.23170731707317074\n",
      "Percentage null in test: 0.18694096601073346\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.53811\tvalid_1's rmse: 1.80826\n",
      "[10000]\ttraining's rmse: 1.24619\tvalid_1's rmse: 1.66001\n",
      "[15000]\ttraining's rmse: 1.08561\tvalid_1's rmse: 1.6079\n",
      "[20000]\ttraining's rmse: 0.971145\tvalid_1's rmse: 1.5812\n",
      "[25000]\ttraining's rmse: 0.882628\tvalid_1's rmse: 1.56413\n",
      "[30000]\ttraining's rmse: 0.810793\tvalid_1's rmse: 1.55267\n",
      "[35000]\ttraining's rmse: 0.749433\tvalid_1's rmse: 1.54368\n",
      "[40000]\ttraining's rmse: 0.696097\tvalid_1's rmse: 1.53727\n",
      "[45000]\ttraining's rmse: 0.64931\tvalid_1's rmse: 1.53182\n",
      "[50000]\ttraining's rmse: 0.607709\tvalid_1's rmse: 1.52777\n",
      "[55000]\ttraining's rmse: 0.570177\tvalid_1's rmse: 1.5245\n",
      "[60000]\ttraining's rmse: 0.536231\tvalid_1's rmse: 1.52177\n",
      "[65000]\ttraining's rmse: 0.505483\tvalid_1's rmse: 1.51942\n",
      "[70000]\ttraining's rmse: 0.477228\tvalid_1's rmse: 1.51764\n",
      "[75000]\ttraining's rmse: 0.451426\tvalid_1's rmse: 1.5162\n",
      "[80000]\ttraining's rmse: 0.427538\tvalid_1's rmse: 1.51483\n",
      "[85000]\ttraining's rmse: 0.405565\tvalid_1's rmse: 1.51366\n",
      "[90000]\ttraining's rmse: 0.38523\tvalid_1's rmse: 1.51279\n",
      "[95000]\ttraining's rmse: 0.366178\tvalid_1's rmse: 1.51212\n",
      "[100000]\ttraining's rmse: 0.348264\tvalid_1's rmse: 1.51141\n",
      "[105000]\ttraining's rmse: 0.331976\tvalid_1's rmse: 1.51082\n",
      "[110000]\ttraining's rmse: 0.316723\tvalid_1's rmse: 1.51033\n",
      "[115000]\ttraining's rmse: 0.302338\tvalid_1's rmse: 1.50996\n",
      "[120000]\ttraining's rmse: 0.288748\tvalid_1's rmse: 1.50967\n",
      "[125000]\ttraining's rmse: 0.276112\tvalid_1's rmse: 1.50943\n",
      "[130000]\ttraining's rmse: 0.264194\tvalid_1's rmse: 1.50908\n",
      "[135000]\ttraining's rmse: 0.252937\tvalid_1's rmse: 1.50898\n",
      "[140000]\ttraining's rmse: 0.242435\tvalid_1's rmse: 1.50887\n",
      "[145000]\ttraining's rmse: 0.232353\tvalid_1's rmse: 1.50878\n",
      "[150000]\ttraining's rmse: 0.223004\tvalid_1's rmse: 1.50866\n",
      "[155000]\ttraining's rmse: 0.214105\tvalid_1's rmse: 1.50863\n",
      "[160000]\ttraining's rmse: 0.205452\tvalid_1's rmse: 1.50858\n",
      "[165000]\ttraining's rmse: 0.197372\tvalid_1's rmse: 1.50855\n",
      "[170000]\ttraining's rmse: 0.189772\tvalid_1's rmse: 1.50865\n",
      "[175000]\ttraining's rmse: 0.182529\tvalid_1's rmse: 1.50862\n",
      "[180000]\ttraining's rmse: 0.175695\tvalid_1's rmse: 1.50866\n",
      "Early stopping, best iteration is:\n",
      "[160394]\ttraining's rmse: 0.204827\tvalid_1's rmse: 1.50854\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "chosen_features = [ \"album_right\",\"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "                   \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "                  \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "                  \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "                  \"isBeatAlbum\", \"isCoverAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "                  \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "                   \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "                  \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "                   \"numsongInAlbum\",\"isSingleAlbum_onesong\",\"artist_mean_id\",\n",
    "                   \"artist_std_id\" ,\"artist_count_id\",\"title_cat\",\"num_same_title\"]\n",
    "\n",
    "# encode_features = ['album_right', 'artist_mean_id',\"artist_id_min_cat\",\n",
    "#                      \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\"]\n",
    "\n",
    "# encode_features=[ 'artist_mean_id',\"artist_id_min_cat\",\n",
    "#                      \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\"]\n",
    "# encode_features = [f+\"_encode\" for f in encode_features]\n",
    "\n",
    "# df[encode_features] = target_encode[encode_features]\n",
    "\n",
    "# chosen_features = [ \"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "#                    \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "#                   \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "#                   \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "#                   \"isBeatAlbum\", \"isCoverAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "#                    \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "#                   \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "#                    \"numsongInAlbum\",\"isSingleAlbum_onesong\",\"artist_mean_id\",\n",
    "#                    \"artist_std_id\" ,\"artist_count_id\",\"title_cat\",\"num_same_title\"]\n",
    "\n",
    "# chosen_features =[]\n",
    "# chosen_features += encode_features\n",
    "# chosen_features = [\"album_right\",\"freq_artist\",  \"day\", \n",
    "#                      \"isBeat\",  \"num_song_release_in_final_month\",\n",
    "#                   \"length\",\"album_artist\",\n",
    "#                   \"artist_name_cat\",\"composers_name_cat\",\n",
    "#                   \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "#                    \"freq_artist_min\",\"dayofyear\", \n",
    "#                    \"numsongInAlbum\",\"artist_mean_id\",\"artist_std_id\" ,\"artist_count_id\" ]\n",
    "\n",
    "\n",
    "# chosen_features = [  \"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "#                    \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "#                   \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "#                   \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "#                   \"isBeatAlbum\", \"isCoverAlbum\",\"copyright_cat\" ,\n",
    "#                   \"composers_id_min_cat\",\n",
    "#                    \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\n",
    "#                   \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "#                    \"numsongInAlbum\",\"isSingleAlbum_onesong\",\"artist_mean_id\", \"artist_std_id\" ,\"artist_count_id\"]\n",
    "\n",
    "chosen_features  += [\"predicted_label\"]\n",
    "# chosen_features += [\"mean_album_score\", \"mean_artist_min_score\"]\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "df_test = df[df.dataset==\"test\"]\n",
    "\n",
    "param = {\n",
    "    'bagging_freq': 20,          \n",
    "    'bagging_fraction': 0.95,   'boost_from_average':'false',   \n",
    "    'boost': 'gbdt',             'feature_fraction': 0.1,     'learning_rate': 0.001,\n",
    "    'max_depth': -1,             'metric':'root_mean_squared_error', 'min_data_in_leaf': 5,   \n",
    "       'num_leaves': 50,            \n",
    "    'num_threads': 8,              'tree_learner': 'serial',   'objective': 'regression',\n",
    "    'reg_alpha': 0.1002650970728192, 'reg_lambda': 0.1003427518866501,'verbosity': 1,\n",
    "    \"seed\": 99999,\n",
    "    \"use_missing\":True\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "labels= df_train.label\n",
    "# fig, axes = plt.subplots(5, 1, figsize=(10, 10*5))\n",
    "# axes = axes.flat\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "    # Create lookup table\n",
    "    album_lookup_table = create_album_score_lookup_table(df_train.iloc[trn_idx])\n",
    "    artist_lookup_table = create_artist_score_lookup_table(df_train.iloc[trn_idx])\n",
    "    \n",
    "    #df_train[\"mean_album_score\"]= [assign_value_album(album_lookup_table, r) for i, r in df_train.iterrows()]\n",
    "    #df_train[\"mean_artist_min_score\"]= [assign_value_artist(artist_lookup_table, r) for i, r in df_train.iterrows()]\n",
    "#     df_train = baysianEncodeFeature(df_train, trn_idx, 'albumHashAndNameAndReleaseday', prior_weight=4, fillmissing=-1, suffix='_enc')\n",
    "#     df_train = baysianEncodeFeature(df_train, trn_idx, 'artist_id_min', prior_weight=4, fillmissing=-1, suffix='_enc')\n",
    "#     df_train = baysianEncodeFeature(df_train, trn_idx, 'title_truncated', prior_weight=4, fillmissing=-1, suffix='_enc')\n",
    "    \n",
    "    df_train[\"predicted_label\"] = [assign_value(album_lookup_table,artist_lookup_table, r) for i, r in df_train.iterrows()]\n",
    "    print(\"Percentage null in valid:\", len(np.sum(df_train.iloc[val_idx].isnull())) / len(df_train.iloc[val_idx]))\n",
    "    df_test[\"predicted_label\"] = [assign_value(album_lookup_table,artist_lookup_table, r) for i, r in df_test.iterrows()]\n",
    "    print(\"Percentage null in test:\", len(np.sum(df_test.isnull())) / len(df_test))\n",
    "    \n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][chosen_features], label=labels.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][chosen_features], label=labels.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 20000)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][chosen_features], num_iteration=clf.best_iteration)\n",
    "    predictions += clf.predict(df_test[chosen_features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.54793 \n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"RMSE: {:<8.5f}\".format(sqrt(mean_squared_error(df_train.label, oof))))\n",
    "sub = pd.DataFrame({\"ID\": df_test.ID.values})\n",
    "sub[\"label\"] = predictions\n",
    "sub.to_csv(\"submission_lightgbm.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sub.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "lgb.plot_importance(clf, max_num_features=20,importance_type='gain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_features = [\"album_right\",\"freq_artist\",  \"day\", \n",
    "#                      \"isBeat\",  \"num_song_release_in_final_month\",\n",
    "#                   \"length\",\"album_artist\",\n",
    "#                   \"artist_name_cat\",\"composers_name_cat\",\n",
    "#                   \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "#                    \"freq_artist_min\",\"dayofyear\", \n",
    "#                    \"numsongInAlbum\",\"artist_mean_id\",\"artist_std_id\" ,\"artist_count_id\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
