{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>composers_name</th>\n",
       "      <th>composers_id</th>\n",
       "      <th>release_time</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.tuning_frequency</th>\n",
       "      <th>tonal.tuning_nontempered_energy_ratio</th>\n",
       "      <th>tonal.chords_key</th>\n",
       "      <th>tonal.chords_scale</th>\n",
       "      <th>tonal.key_edma.key</th>\n",
       "      <th>tonal.key_edma.scale</th>\n",
       "      <th>tonal.key_krumhansl.key</th>\n",
       "      <th>tonal.key_krumhansl.scale</th>\n",
       "      <th>tonal.key_temperley.key</th>\n",
       "      <th>tonal.key_temperley.scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1073748245</td>\n",
       "      <td>Đêm Chôn Dầu Vượt Biển</td>\n",
       "      <td>Như Quỳnh</td>\n",
       "      <td>551</td>\n",
       "      <td>Châu Đình An</td>\n",
       "      <td>5765</td>\n",
       "      <td>2017-10-01 22:07:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.601478</td>\n",
       "      <td>D</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1073751978</td>\n",
       "      <td>Mùa Thu Trong Mưa</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Trường Sa</td>\n",
       "      <td>100105</td>\n",
       "      <td>2017-10-01 20:58:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.944516</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1073835561</td>\n",
       "      <td>Rồi Ánh Trăng Tan</td>\n",
       "      <td>Lưu Bích</td>\n",
       "      <td>450</td>\n",
       "      <td>Quốc Bảo</td>\n",
       "      <td>4355</td>\n",
       "      <td>2017-11-01 18:16:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.957651</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          ID                   title artist_name artist_id  \\\n",
       "0      0  1073748245  Đêm Chôn Dầu Vượt Biển   Như Quỳnh       551   \n",
       "1      1  1073751978       Mùa Thu Trong Mưa  Minh Tuyết       455   \n",
       "2      2  1073835561       Rồi Ánh Trăng Tan    Lưu Bích       450   \n",
       "\n",
       "  composers_name composers_id         release_time  label dataset  \\\n",
       "0   Châu Đình An         5765  2017-10-01 22:07:00    7.0   train   \n",
       "1      Trường Sa       100105  2017-10-01 20:58:00    3.0   train   \n",
       "2       Quốc Bảo         4355  2017-11-01 18:16:00    6.0   train   \n",
       "\n",
       "             ...              tonal.tuning_frequency  \\\n",
       "0            ...                          440.000000   \n",
       "1            ...                          434.193115   \n",
       "2            ...                          434.193115   \n",
       "\n",
       "  tonal.tuning_nontempered_energy_ratio tonal.chords_key tonal.chords_scale  \\\n",
       "0                              0.601478                D              major   \n",
       "1                              0.944516                C              minor   \n",
       "2                              0.957651               Bb              major   \n",
       "\n",
       "  tonal.key_edma.key tonal.key_edma.scale  tonal.key_krumhansl.key  \\\n",
       "0                  G                major                        G   \n",
       "1                  C                minor                        C   \n",
       "2                  D                minor                        D   \n",
       "\n",
       "  tonal.key_krumhansl.scale  tonal.key_temperley.key  \\\n",
       "0                     major                        G   \n",
       "1                     minor                        C   \n",
       "2                     minor                        D   \n",
       "\n",
       "   tonal.key_temperley.scale  \n",
       "0                      major  \n",
       "1                      minor  \n",
       "2                      minor  \n",
       "\n",
       "[3 rows x 137 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "DATA_DIR = \"C:\\\\Users\\\\Ben\\\\OneDrive\\\\ZaloSongComp\\\\zalo-hit-song-prediction\\\\csv\"\n",
    "TRAININFO = os.path.join(DATA_DIR, \"train_info.tsv\")\n",
    "TRAINRANK =  os.path.join(DATA_DIR, \"train_rank.csv\")\n",
    "TESTINFO = os.path.join(DATA_DIR, \"test_info.tsv\")\n",
    "SUBMISSION = os.path.join(DATA_DIR, \"submission.csv\")\n",
    "\n",
    "# Prepare data\n",
    "df_i = pd.read_csv(TRAININFO, delimiter='\\t',encoding='utf-8')\n",
    "df_r = pd.read_csv(TRAINRANK)\n",
    "df_i_train = df_i.merge(df_r, left_on='ID', right_on='ID')\n",
    "df_i_train[\"dataset\"] = \"train\"\n",
    "\n",
    "df_i_test = pd.read_csv(TESTINFO, delimiter='\\t',encoding='utf-8')\n",
    "df_i_test[\"label\"] = np.nan\n",
    "df_i_test[\"dataset\"] = \"test\"\n",
    "\n",
    "df = pd.concat([df_i_train, df_i_test])\n",
    "df_track_info = pd.read_csv(os.path.join(DATA_DIR, \"all_track_info.csv\"))\n",
    "df = df.merge(df_track_info, left_on='ID', right_on='ID')\n",
    "df_audio_features = pd.read_csv(os.path.join(DATA_DIR, \"all_track_audio_features.csv\"))\n",
    "df =df.merge(df_audio_features,left_on=\"ID\",right_on=\"ID\", how=\"left\")\n",
    "\n",
    "# Sort by ID\n",
    "df = df.sort_values(by=['ID'])\n",
    "df= df.reset_index()\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0.24038838760298156 ratio is nan album\n",
      "There is 0.0017653981953707335 ratio is nan genre\n",
      "There is 0.24038838760298156 ratio is nan album_artist\n",
      "There is 0.0007846214201647705 ratio is nan track\n",
      "There is 0.6722244017261672 ratio is nan lyric\n"
     ]
    }
   ],
   "source": [
    "from format_features import format_features\n",
    "df = format_features(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'ID',\n",
       " 'title',\n",
       " 'artist_name',\n",
       " 'artist_id',\n",
       " 'composers_name',\n",
       " 'composers_id',\n",
       " 'release_time',\n",
       " 'label',\n",
       " 'dataset',\n",
       " 'Unnamed: 0_x',\n",
       " 'album',\n",
       " 'genre',\n",
       " 'album_artist',\n",
       " 'track',\n",
       " 'lyric',\n",
       " 'length',\n",
       " 'copyright',\n",
       " 'Unnamed: 0_y',\n",
       " 'lowlevel.average_loudness',\n",
       " 'lowlevel.barkbands_crest.mean',\n",
       " 'lowlevel.barkbands_crest.stdev',\n",
       " 'lowlevel.barkbands_flatness_db.mean',\n",
       " 'lowlevel.barkbands_flatness_db.stdev',\n",
       " 'lowlevel.barkbands_kurtosis.mean',\n",
       " 'lowlevel.barkbands_kurtosis.stdev',\n",
       " 'lowlevel.barkbands_skewness.mean',\n",
       " 'lowlevel.barkbands_skewness.stdev',\n",
       " 'lowlevel.barkbands_spread.mean',\n",
       " 'lowlevel.barkbands_spread.stdev',\n",
       " 'lowlevel.dissonance.mean',\n",
       " 'lowlevel.dissonance.stdev',\n",
       " 'lowlevel.dynamic_complexity',\n",
       " 'lowlevel.erbbands_crest.mean',\n",
       " 'lowlevel.erbbands_crest.stdev',\n",
       " 'lowlevel.erbbands_flatness_db.mean',\n",
       " 'lowlevel.erbbands_flatness_db.stdev',\n",
       " 'lowlevel.erbbands_kurtosis.mean',\n",
       " 'lowlevel.erbbands_kurtosis.stdev',\n",
       " 'lowlevel.erbbands_skewness.mean',\n",
       " 'lowlevel.erbbands_skewness.stdev',\n",
       " 'lowlevel.erbbands_spread.mean',\n",
       " 'lowlevel.erbbands_spread.stdev',\n",
       " 'lowlevel.hfc.mean',\n",
       " 'lowlevel.hfc.stdev',\n",
       " 'lowlevel.loudness_ebu128.integrated',\n",
       " 'lowlevel.loudness_ebu128.loudness_range',\n",
       " 'lowlevel.loudness_ebu128.momentary.mean',\n",
       " 'lowlevel.loudness_ebu128.momentary.stdev',\n",
       " 'lowlevel.loudness_ebu128.short_term.mean',\n",
       " 'lowlevel.loudness_ebu128.short_term.stdev',\n",
       " 'lowlevel.melbands_crest.mean',\n",
       " 'lowlevel.melbands_crest.stdev',\n",
       " 'lowlevel.melbands_flatness_db.mean',\n",
       " 'lowlevel.melbands_flatness_db.stdev',\n",
       " 'lowlevel.melbands_kurtosis.mean',\n",
       " 'lowlevel.melbands_kurtosis.stdev',\n",
       " 'lowlevel.melbands_skewness.mean',\n",
       " 'lowlevel.melbands_skewness.stdev',\n",
       " 'lowlevel.melbands_spread.mean',\n",
       " 'lowlevel.melbands_spread.stdev',\n",
       " 'lowlevel.pitch_salience.mean',\n",
       " 'lowlevel.pitch_salience.stdev',\n",
       " 'lowlevel.silence_rate_20dB.mean',\n",
       " 'lowlevel.silence_rate_20dB.stdev',\n",
       " 'lowlevel.silence_rate_30dB.mean',\n",
       " 'lowlevel.silence_rate_30dB.stdev',\n",
       " 'lowlevel.silence_rate_60dB.mean',\n",
       " 'lowlevel.silence_rate_60dB.stdev',\n",
       " 'lowlevel.spectral_centroid.mean',\n",
       " 'lowlevel.spectral_centroid.stdev',\n",
       " 'lowlevel.spectral_complexity.mean',\n",
       " 'lowlevel.spectral_complexity.stdev',\n",
       " 'lowlevel.spectral_decrease.mean',\n",
       " 'lowlevel.spectral_decrease.stdev',\n",
       " 'lowlevel.spectral_energy.mean',\n",
       " 'lowlevel.spectral_energy.stdev',\n",
       " 'lowlevel.spectral_energyband_high.mean',\n",
       " 'lowlevel.spectral_energyband_high.stdev',\n",
       " 'lowlevel.spectral_energyband_low.mean',\n",
       " 'lowlevel.spectral_energyband_low.stdev',\n",
       " 'lowlevel.spectral_energyband_middle_high.mean',\n",
       " 'lowlevel.spectral_energyband_middle_high.stdev',\n",
       " 'lowlevel.spectral_energyband_middle_low.mean',\n",
       " 'lowlevel.spectral_energyband_middle_low.stdev',\n",
       " 'lowlevel.spectral_entropy.mean',\n",
       " 'lowlevel.spectral_entropy.stdev',\n",
       " 'lowlevel.spectral_flux.mean',\n",
       " 'lowlevel.spectral_flux.stdev',\n",
       " 'lowlevel.spectral_kurtosis.mean',\n",
       " 'lowlevel.spectral_kurtosis.stdev',\n",
       " 'lowlevel.spectral_rms.mean',\n",
       " 'lowlevel.spectral_rms.stdev',\n",
       " 'lowlevel.spectral_rolloff.mean',\n",
       " 'lowlevel.spectral_rolloff.stdev',\n",
       " 'lowlevel.spectral_skewness.mean',\n",
       " 'lowlevel.spectral_skewness.stdev',\n",
       " 'lowlevel.spectral_spread.mean',\n",
       " 'lowlevel.spectral_spread.stdev',\n",
       " 'lowlevel.spectral_strongpeak.mean',\n",
       " 'lowlevel.spectral_strongpeak.stdev',\n",
       " 'lowlevel.zerocrossingrate.mean',\n",
       " 'lowlevel.zerocrossingrate.stdev',\n",
       " 'rhythm.beats_count',\n",
       " 'rhythm.beats_loudness.mean',\n",
       " 'rhythm.beats_loudness.stdev',\n",
       " 'rhythm.bpm',\n",
       " 'rhythm.bpm_histogram_first_peak_bpm',\n",
       " 'rhythm.bpm_histogram_first_peak_weight',\n",
       " 'rhythm.bpm_histogram_second_peak_bpm',\n",
       " 'rhythm.bpm_histogram_second_peak_spread',\n",
       " 'rhythm.bpm_histogram_second_peak_weight',\n",
       " 'rhythm.danceability',\n",
       " 'rhythm.onset_rate',\n",
       " 'tonal.chords_changes_rate',\n",
       " 'tonal.chords_number_rate',\n",
       " 'tonal.chords_strength.mean',\n",
       " 'tonal.chords_strength.stdev',\n",
       " 'tonal.hpcp_crest.mean',\n",
       " 'tonal.hpcp_crest.stdev',\n",
       " 'tonal.hpcp_entropy.mean',\n",
       " 'tonal.hpcp_entropy.stdev',\n",
       " 'tonal.key_edma.strength',\n",
       " 'tonal.key_krumhansl.strength',\n",
       " 'tonal.key_temperley.strength',\n",
       " 'tonal.tuning_diatonic_strength',\n",
       " 'tonal.tuning_equal_tempered_deviation',\n",
       " 'tonal.tuning_frequency',\n",
       " 'tonal.tuning_nontempered_energy_ratio',\n",
       " 'tonal.chords_key',\n",
       " 'tonal.chords_scale',\n",
       " 'tonal.key_edma.key',\n",
       " 'tonal.key_edma.scale',\n",
       " 'tonal.key_krumhansl.key',\n",
       " 'tonal.key_krumhansl.scale',\n",
       " 'tonal.key_temperley.key',\n",
       " 'tonal.key_temperley.scale',\n",
       " 'len_album_name',\n",
       " 'isRemixAlbum',\n",
       " 'isOSTAlbum',\n",
       " 'isSingleAlbum',\n",
       " 'isBeatAlbum',\n",
       " 'isTopHitAlbum',\n",
       " 'isCoverAlbum',\n",
       " 'isEPAlbum',\n",
       " 'isLienKhucAlbum',\n",
       " 'album_name_is_title_name',\n",
       " 'artist_id_min',\n",
       " 'artist_id_max',\n",
       " 'composers_id_min',\n",
       " 'composers_id_max',\n",
       " 'album_artist_contain_artistname',\n",
       " 'istrack11',\n",
       " 'islyric',\n",
       " 'num_line_lyric',\n",
       " 'no_artist',\n",
       " 'no_composer',\n",
       " 'datetime',\n",
       " 'year',\n",
       " 'month',\n",
       " 'hour',\n",
       " 'day',\n",
       " 'dayofyear',\n",
       " 'weekday',\n",
       " 'isHoliday',\n",
       " 'len_of_songname',\n",
       " 'isRemix',\n",
       " 'isOST',\n",
       " 'isBeat',\n",
       " 'isVersion',\n",
       " 'isCover',\n",
       " 'isLienKhuc',\n",
       " 'day_release',\n",
       " 'datetimeYear',\n",
       " 'datetimeMonth',\n",
       " 'datetimeWeek',\n",
       " 'datetimeDay',\n",
       " 'datetimeDayofweek',\n",
       " 'datetimeDayofyear',\n",
       " 'datetimeIs_month_end',\n",
       " 'datetimeIs_month_start',\n",
       " 'datetimeIs_quarter_end',\n",
       " 'datetimeIs_quarter_start',\n",
       " 'datetimeIs_year_end',\n",
       " 'datetimeIs_year_start',\n",
       " 'datetimeElapsed',\n",
       " 'album_right',\n",
       " 'numsongInAlbum',\n",
       " 'isSingleAlbum_onesong',\n",
       " 'num_song_released_that_week',\n",
       " 'num_song_release_in_final_month',\n",
       " 'freq_artist',\n",
       " 'freq_composer',\n",
       " '_artist_id_min_cat',\n",
       " '_composers_id_min_cat',\n",
       " 'freq_artist_min',\n",
       " 'freq_composer_min',\n",
       " 'num_album_per_min_artist',\n",
       " 'num_album_per_min_composer']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = [\"album_right\", \"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "                   \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "                  \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "                  \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "                  \"isBeatAlbum\", \"isCoverAlbum\", \"artist_name\",\"composers_name\",\"copyright\" ,\n",
    "                  \"artist_id_min\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "                   \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "                  \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "                   \"numsongInAlbum\",\"isSingleAlbum_onesong\" ]\n",
    "\n",
    "all_features_in_order = {\"album\":\"category\", # album name from mp3 metadata textual\n",
    "                          \"len_album_name\":\"int64\",\n",
    "                          \"isRemixAlbum\":\"category\",\n",
    "                          \"isOSTAlbum\":\"category\",\n",
    "                          \"isSingleAlbum\":\"category\",\n",
    "                          \"isBeatAlbum\":\"category\",\n",
    "                          \"isTopHitAlbum\":\"category\",\n",
    "                          \"isCoverAlbum\":\"category\",\n",
    "                          \"isEPAlbum\":\"category\",\n",
    "                          \"isLienKhucAlbum\":\"category\",\n",
    "                          \"album_name_is_title_name\":\"category\",\n",
    "                          \"artist_name\":\"category\",\n",
    "                          \"composers_name\":\"category\",\n",
    "                          \"copyright\":\"category\" ,\n",
    "                          \"artist_id_min\":\"category\",\n",
    "                          \"artist_id_max\":\"category\", \n",
    "                          \"composers_id_min\":\"category\", \n",
    "                          \"composers_id_max\":\"category\",\n",
    "                          \"genre\":\"category\", \n",
    "                          \"album_artist\":\"category\", # album artist name\n",
    "                          \"album_artist_contain_artistname\":\"category\",\n",
    "                          \"track\":\"float64\", # float between 0 and 1 representing track_num/total_tracks\n",
    "                          \"istrack11\":\"category\", # 1 if first track\n",
    "                          # \"lyric\":\"string\" # Not a trainable feature\n",
    "                          \"islyric\":\"category\",\n",
    "                          \"num_line_lyric\":\"int64\",\n",
    "                          \"no_artist\":\"int64\",\n",
    "                          \"no_composer\":\"int64\",\n",
    "                          #\"datetime\":\"datetime64\", # Not a trainable feature\n",
    "                          \"day\":\"category\",\n",
    "                          \"month\":\"category\",\n",
    "                          \"year\":\"category\",\n",
    "                          \"hour\":\"category\",\n",
    "                          \"dayofyear\":\"category\",\n",
    "                          \"weekday\":\"category\",\n",
    "                          \"isHoliday\":\"category\",\n",
    "                          \"len_of_songname\":\"int64\",\n",
    "                          \"isRemix\":\"category\",\n",
    "                          \"isOST\":\"category\",\n",
    "                          \"isBeat\":\"category\",\n",
    "                          \"isVersion\":\"category\",\n",
    "                          \"isCover\":\"category\",\n",
    "                          \"isLienKhuc\":\"category\",\n",
    "                          \"day_release\":\"int64\", # the specific day of the day across all days (> 365)  \n",
    "                          \"datetimeIs_month_end\":\"category\",\n",
    "                          \"datetimeIs_month_start\":\"category\",\n",
    "                          \"datetimeIs_quarter_end\":\"category\",\n",
    "                          \"datetimeIs_quarter_start\":\"category\",\n",
    "                          \"datetimeIs_year_end\":\"category\",\n",
    "                          \"datetimeIs_year_start\":\"category\",\n",
    "                          \"datetimeDayofweek\":\"category\",\n",
    "                         \n",
    "                          ####\n",
    "                          #Features those that require \"global\" knowledge beyond that example\n",
    "                          ####\n",
    "                          \"album_right\":\"category\", # a different representaion of \"album\" based on release time - can be combined with it, and using \n",
    "                          \"numsongInAlbum\":\"category\",\n",
    "                          \"isSingleAlbum_onesong\":\"category\",\n",
    "                          \"num_song_released_that_week\":'int64',\n",
    "                          \"num_song_release_in_final_month\":\"int64\",  \n",
    "                          \"freq_artist\":\"int64\",  # number of times the unique artist string is present in dataset\n",
    "                          \"freq_artist_min\":\"int64\", # number of times the first listed artist is present in dataset\n",
    "                          \"num_album_per_min_artist\":\"int64\",\n",
    "                          \"num_album_per_min_composer\":\"int64\",\n",
    "}\n",
    "all_features_in_order_list = list(all_features_in_order.keys())\n",
    "for feat_name, feat_type in all_features_in_order.items():\n",
    "    df[feat_name] = df[feat_name].astype(feat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0.24038838760298156 ratio is nan album\n",
      "There is 0.0017653981953707335 ratio is nan genre\n",
      "There is 0.24038838760298156 ratio is nan album_artist\n",
      "There is 0.0007846214201647705 ratio is nan track\n",
      "There is 0.6722244017261672 ratio is nan lyric\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74708\tvalid_1's rmse: 1.95719\n",
      "[10000]\ttraining's rmse: 1.43107\tvalid_1's rmse: 1.79029\n",
      "[15000]\ttraining's rmse: 1.24494\tvalid_1's rmse: 1.71663\n",
      "[20000]\ttraining's rmse: 1.11329\tvalid_1's rmse: 1.67337\n",
      "[25000]\ttraining's rmse: 1.01312\tvalid_1's rmse: 1.64622\n",
      "[30000]\ttraining's rmse: 0.932046\tvalid_1's rmse: 1.62791\n",
      "[35000]\ttraining's rmse: 0.865129\tvalid_1's rmse: 1.6149\n",
      "[40000]\ttraining's rmse: 0.808398\tvalid_1's rmse: 1.60554\n",
      "[45000]\ttraining's rmse: 0.759385\tvalid_1's rmse: 1.5982\n",
      "[50000]\ttraining's rmse: 0.715781\tvalid_1's rmse: 1.5933\n",
      "[55000]\ttraining's rmse: 0.677387\tvalid_1's rmse: 1.58914\n",
      "[60000]\ttraining's rmse: 0.642672\tvalid_1's rmse: 1.58585\n",
      "[65000]\ttraining's rmse: 0.611394\tvalid_1's rmse: 1.58355\n",
      "[70000]\ttraining's rmse: 0.582694\tvalid_1's rmse: 1.58225\n",
      "[75000]\ttraining's rmse: 0.556833\tvalid_1's rmse: 1.58089\n",
      "[80000]\ttraining's rmse: 0.532766\tvalid_1's rmse: 1.57979\n",
      "[85000]\ttraining's rmse: 0.510256\tvalid_1's rmse: 1.57914\n",
      "[90000]\ttraining's rmse: 0.489572\tvalid_1's rmse: 1.57896\n",
      "[95000]\ttraining's rmse: 0.470835\tvalid_1's rmse: 1.57905\n",
      "[100000]\ttraining's rmse: 0.453188\tvalid_1's rmse: 1.57894\n",
      "[105000]\ttraining's rmse: 0.436829\tvalid_1's rmse: 1.57886\n",
      "[110000]\ttraining's rmse: 0.421462\tvalid_1's rmse: 1.57883\n",
      "[115000]\ttraining's rmse: 0.407198\tvalid_1's rmse: 1.57897\n",
      "[120000]\ttraining's rmse: 0.393841\tvalid_1's rmse: 1.57901\n",
      "[125000]\ttraining's rmse: 0.38121\tvalid_1's rmse: 1.57918\n",
      "Early stopping, best iteration is:\n",
      "[107585]\ttraining's rmse: 0.428706\tvalid_1's rmse: 1.57874\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.746\tvalid_1's rmse: 1.94154\n",
      "[10000]\ttraining's rmse: 1.42627\tvalid_1's rmse: 1.79139\n",
      "[15000]\ttraining's rmse: 1.23834\tvalid_1's rmse: 1.73056\n",
      "[20000]\ttraining's rmse: 1.10674\tvalid_1's rmse: 1.6975\n",
      "[25000]\ttraining's rmse: 1.00706\tvalid_1's rmse: 1.67666\n",
      "[30000]\ttraining's rmse: 0.927356\tvalid_1's rmse: 1.66255\n",
      "[35000]\ttraining's rmse: 0.860702\tvalid_1's rmse: 1.65271\n",
      "[40000]\ttraining's rmse: 0.804641\tvalid_1's rmse: 1.64505\n",
      "[45000]\ttraining's rmse: 0.756293\tvalid_1's rmse: 1.6392\n",
      "[50000]\ttraining's rmse: 0.712995\tvalid_1's rmse: 1.63496\n",
      "[55000]\ttraining's rmse: 0.674841\tvalid_1's rmse: 1.63114\n",
      "[60000]\ttraining's rmse: 0.640535\tvalid_1's rmse: 1.62789\n",
      "[65000]\ttraining's rmse: 0.609424\tvalid_1's rmse: 1.62566\n",
      "[70000]\ttraining's rmse: 0.580901\tvalid_1's rmse: 1.6238\n",
      "[75000]\ttraining's rmse: 0.554946\tvalid_1's rmse: 1.62197\n",
      "[80000]\ttraining's rmse: 0.53106\tvalid_1's rmse: 1.62057\n",
      "[85000]\ttraining's rmse: 0.508577\tvalid_1's rmse: 1.61957\n",
      "[90000]\ttraining's rmse: 0.487908\tvalid_1's rmse: 1.61859\n",
      "[95000]\ttraining's rmse: 0.469088\tvalid_1's rmse: 1.61783\n",
      "[100000]\ttraining's rmse: 0.451432\tvalid_1's rmse: 1.61729\n",
      "[105000]\ttraining's rmse: 0.435037\tvalid_1's rmse: 1.61687\n",
      "[110000]\ttraining's rmse: 0.419663\tvalid_1's rmse: 1.61637\n",
      "[115000]\ttraining's rmse: 0.405273\tvalid_1's rmse: 1.61624\n",
      "[120000]\ttraining's rmse: 0.391805\tvalid_1's rmse: 1.61594\n",
      "[125000]\ttraining's rmse: 0.379134\tvalid_1's rmse: 1.61588\n",
      "[130000]\ttraining's rmse: 0.367281\tvalid_1's rmse: 1.61601\n",
      "[135000]\ttraining's rmse: 0.356163\tvalid_1's rmse: 1.61593\n",
      "[140000]\ttraining's rmse: 0.345784\tvalid_1's rmse: 1.61603\n",
      "Early stopping, best iteration is:\n",
      "[123070]\ttraining's rmse: 0.384048\tvalid_1's rmse: 1.61582\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.75254\tvalid_1's rmse: 1.9361\n",
      "[10000]\ttraining's rmse: 1.43393\tvalid_1's rmse: 1.75531\n",
      "[15000]\ttraining's rmse: 1.24619\tvalid_1's rmse: 1.67356\n",
      "[20000]\ttraining's rmse: 1.11358\tvalid_1's rmse: 1.62621\n",
      "[25000]\ttraining's rmse: 1.01282\tvalid_1's rmse: 1.59679\n",
      "[30000]\ttraining's rmse: 0.931536\tvalid_1's rmse: 1.57675\n",
      "[35000]\ttraining's rmse: 0.863977\tvalid_1's rmse: 1.56234\n",
      "[40000]\ttraining's rmse: 0.806818\tvalid_1's rmse: 1.55145\n",
      "[45000]\ttraining's rmse: 0.757958\tvalid_1's rmse: 1.54379\n",
      "[50000]\ttraining's rmse: 0.714452\tvalid_1's rmse: 1.53791\n",
      "[55000]\ttraining's rmse: 0.676121\tvalid_1's rmse: 1.53289\n",
      "[60000]\ttraining's rmse: 0.641648\tvalid_1's rmse: 1.52931\n",
      "[65000]\ttraining's rmse: 0.610417\tvalid_1's rmse: 1.52626\n",
      "[70000]\ttraining's rmse: 0.58183\tvalid_1's rmse: 1.52436\n",
      "[75000]\ttraining's rmse: 0.556061\tvalid_1's rmse: 1.52252\n",
      "[80000]\ttraining's rmse: 0.5324\tvalid_1's rmse: 1.5212\n",
      "[85000]\ttraining's rmse: 0.509998\tvalid_1's rmse: 1.51981\n",
      "[90000]\ttraining's rmse: 0.489478\tvalid_1's rmse: 1.51909\n",
      "[95000]\ttraining's rmse: 0.470693\tvalid_1's rmse: 1.51855\n",
      "[100000]\ttraining's rmse: 0.453264\tvalid_1's rmse: 1.51844\n",
      "[105000]\ttraining's rmse: 0.436794\tvalid_1's rmse: 1.51806\n",
      "[110000]\ttraining's rmse: 0.421581\tvalid_1's rmse: 1.51807\n",
      "[115000]\ttraining's rmse: 0.407225\tvalid_1's rmse: 1.51784\n",
      "[120000]\ttraining's rmse: 0.393984\tvalid_1's rmse: 1.51807\n",
      "[125000]\ttraining's rmse: 0.381351\tvalid_1's rmse: 1.51805\n",
      "[130000]\ttraining's rmse: 0.369322\tvalid_1's rmse: 1.51825\n",
      "[135000]\ttraining's rmse: 0.35818\tvalid_1's rmse: 1.5184\n",
      "Early stopping, best iteration is:\n",
      "[115396]\ttraining's rmse: 0.406198\tvalid_1's rmse: 1.51775\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74667\tvalid_1's rmse: 1.97432\n",
      "[10000]\ttraining's rmse: 1.4307\tvalid_1's rmse: 1.79587\n",
      "[15000]\ttraining's rmse: 1.24341\tvalid_1's rmse: 1.71732\n",
      "[20000]\ttraining's rmse: 1.11105\tvalid_1's rmse: 1.67208\n",
      "[25000]\ttraining's rmse: 1.01084\tvalid_1's rmse: 1.64552\n",
      "[30000]\ttraining's rmse: 0.929966\tvalid_1's rmse: 1.62743\n",
      "[35000]\ttraining's rmse: 0.862918\tvalid_1's rmse: 1.61591\n",
      "[40000]\ttraining's rmse: 0.80612\tvalid_1's rmse: 1.60744\n",
      "[45000]\ttraining's rmse: 0.757444\tvalid_1's rmse: 1.60137\n",
      "[50000]\ttraining's rmse: 0.714019\tvalid_1's rmse: 1.59728\n",
      "[55000]\ttraining's rmse: 0.675631\tvalid_1's rmse: 1.59428\n",
      "[60000]\ttraining's rmse: 0.640992\tvalid_1's rmse: 1.59231\n",
      "[65000]\ttraining's rmse: 0.60961\tvalid_1's rmse: 1.5912\n",
      "[70000]\ttraining's rmse: 0.580808\tvalid_1's rmse: 1.59052\n",
      "[75000]\ttraining's rmse: 0.555008\tvalid_1's rmse: 1.58999\n",
      "[80000]\ttraining's rmse: 0.530705\tvalid_1's rmse: 1.5897\n",
      "[85000]\ttraining's rmse: 0.508255\tvalid_1's rmse: 1.59003\n",
      "[90000]\ttraining's rmse: 0.487521\tvalid_1's rmse: 1.59037\n",
      "[95000]\ttraining's rmse: 0.468555\tvalid_1's rmse: 1.59068\n",
      "Early stopping, best iteration is:\n",
      "[79537]\ttraining's rmse: 0.532889\tvalid_1's rmse: 1.58967\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74398\tvalid_1's rmse: 1.98113\n",
      "[10000]\ttraining's rmse: 1.42674\tvalid_1's rmse: 1.80969\n",
      "[15000]\ttraining's rmse: 1.23986\tvalid_1's rmse: 1.73644\n",
      "[20000]\ttraining's rmse: 1.1083\tvalid_1's rmse: 1.69407\n",
      "[25000]\ttraining's rmse: 1.00799\tvalid_1's rmse: 1.66689\n",
      "[30000]\ttraining's rmse: 0.927002\tvalid_1's rmse: 1.64867\n",
      "[35000]\ttraining's rmse: 0.859755\tvalid_1's rmse: 1.63526\n",
      "[40000]\ttraining's rmse: 0.802858\tvalid_1's rmse: 1.6265\n",
      "[45000]\ttraining's rmse: 0.754274\tvalid_1's rmse: 1.61979\n",
      "[50000]\ttraining's rmse: 0.710636\tvalid_1's rmse: 1.61426\n",
      "[55000]\ttraining's rmse: 0.672121\tvalid_1's rmse: 1.60993\n",
      "[60000]\ttraining's rmse: 0.637509\tvalid_1's rmse: 1.60621\n",
      "[65000]\ttraining's rmse: 0.606027\tvalid_1's rmse: 1.60346\n",
      "[70000]\ttraining's rmse: 0.577428\tvalid_1's rmse: 1.60139\n",
      "[75000]\ttraining's rmse: 0.551565\tvalid_1's rmse: 1.59963\n",
      "[80000]\ttraining's rmse: 0.527644\tvalid_1's rmse: 1.59817\n",
      "[85000]\ttraining's rmse: 0.505284\tvalid_1's rmse: 1.59723\n",
      "[90000]\ttraining's rmse: 0.484769\tvalid_1's rmse: 1.5963\n",
      "[95000]\ttraining's rmse: 0.465837\tvalid_1's rmse: 1.59559\n",
      "[100000]\ttraining's rmse: 0.448207\tvalid_1's rmse: 1.59519\n",
      "[105000]\ttraining's rmse: 0.431763\tvalid_1's rmse: 1.59491\n",
      "[110000]\ttraining's rmse: 0.41625\tvalid_1's rmse: 1.59468\n",
      "[115000]\ttraining's rmse: 0.4018\tvalid_1's rmse: 1.59445\n",
      "[120000]\ttraining's rmse: 0.388549\tvalid_1's rmse: 1.59435\n",
      "[125000]\ttraining's rmse: 0.375926\tvalid_1's rmse: 1.59437\n",
      "[130000]\ttraining's rmse: 0.363913\tvalid_1's rmse: 1.59442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135000]\ttraining's rmse: 0.352768\tvalid_1's rmse: 1.59449\n",
      "Early stopping, best iteration is:\n",
      "[117539]\ttraining's rmse: 0.395041\tvalid_1's rmse: 1.59427\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74416\tvalid_1's rmse: 1.97861\n",
      "[10000]\ttraining's rmse: 1.42737\tvalid_1's rmse: 1.808\n",
      "[15000]\ttraining's rmse: 1.24048\tvalid_1's rmse: 1.73443\n",
      "[20000]\ttraining's rmse: 1.10854\tvalid_1's rmse: 1.69264\n",
      "[25000]\ttraining's rmse: 1.00847\tvalid_1's rmse: 1.66628\n",
      "[30000]\ttraining's rmse: 0.927733\tvalid_1's rmse: 1.64903\n",
      "[35000]\ttraining's rmse: 0.860953\tvalid_1's rmse: 1.63626\n",
      "[40000]\ttraining's rmse: 0.804731\tvalid_1's rmse: 1.62695\n",
      "[45000]\ttraining's rmse: 0.755754\tvalid_1's rmse: 1.61981\n",
      "[50000]\ttraining's rmse: 0.711956\tvalid_1's rmse: 1.615\n",
      "[55000]\ttraining's rmse: 0.673466\tvalid_1's rmse: 1.61074\n",
      "[60000]\ttraining's rmse: 0.638754\tvalid_1's rmse: 1.60766\n",
      "[65000]\ttraining's rmse: 0.607396\tvalid_1's rmse: 1.60541\n",
      "[70000]\ttraining's rmse: 0.579035\tvalid_1's rmse: 1.60381\n",
      "[75000]\ttraining's rmse: 0.553059\tvalid_1's rmse: 1.60233\n",
      "[80000]\ttraining's rmse: 0.529267\tvalid_1's rmse: 1.6012\n",
      "[85000]\ttraining's rmse: 0.506904\tvalid_1's rmse: 1.60056\n",
      "[90000]\ttraining's rmse: 0.486345\tvalid_1's rmse: 1.60008\n",
      "[95000]\ttraining's rmse: 0.467547\tvalid_1's rmse: 1.59975\n",
      "[100000]\ttraining's rmse: 0.450067\tvalid_1's rmse: 1.59941\n",
      "[105000]\ttraining's rmse: 0.433632\tvalid_1's rmse: 1.59938\n",
      "[110000]\ttraining's rmse: 0.418093\tvalid_1's rmse: 1.59919\n",
      "[115000]\ttraining's rmse: 0.403669\tvalid_1's rmse: 1.59902\n",
      "[120000]\ttraining's rmse: 0.39009\tvalid_1's rmse: 1.59888\n",
      "[125000]\ttraining's rmse: 0.377273\tvalid_1's rmse: 1.59903\n",
      "[130000]\ttraining's rmse: 0.365164\tvalid_1's rmse: 1.59906\n",
      "[135000]\ttraining's rmse: 0.353896\tvalid_1's rmse: 1.59919\n",
      "[140000]\ttraining's rmse: 0.343394\tvalid_1's rmse: 1.59929\n",
      "Early stopping, best iteration is:\n",
      "[120663]\ttraining's rmse: 0.388357\tvalid_1's rmse: 1.5988\n",
      "Fold 6\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74848\tvalid_1's rmse: 1.96701\n",
      "[10000]\ttraining's rmse: 1.43311\tvalid_1's rmse: 1.79383\n",
      "[15000]\ttraining's rmse: 1.24543\tvalid_1's rmse: 1.71764\n",
      "[20000]\ttraining's rmse: 1.11245\tvalid_1's rmse: 1.67367\n",
      "[25000]\ttraining's rmse: 1.01086\tvalid_1's rmse: 1.64594\n",
      "[30000]\ttraining's rmse: 0.929412\tvalid_1's rmse: 1.62738\n",
      "[35000]\ttraining's rmse: 0.861847\tvalid_1's rmse: 1.61406\n",
      "[40000]\ttraining's rmse: 0.804376\tvalid_1's rmse: 1.60444\n",
      "[45000]\ttraining's rmse: 0.755267\tvalid_1's rmse: 1.59788\n",
      "[50000]\ttraining's rmse: 0.711019\tvalid_1's rmse: 1.59288\n",
      "[55000]\ttraining's rmse: 0.672191\tvalid_1's rmse: 1.58864\n",
      "[60000]\ttraining's rmse: 0.637224\tvalid_1's rmse: 1.58566\n",
      "[65000]\ttraining's rmse: 0.605426\tvalid_1's rmse: 1.58328\n",
      "[70000]\ttraining's rmse: 0.576446\tvalid_1's rmse: 1.58125\n",
      "[75000]\ttraining's rmse: 0.55034\tvalid_1's rmse: 1.57988\n",
      "[80000]\ttraining's rmse: 0.526145\tvalid_1's rmse: 1.5788\n",
      "[85000]\ttraining's rmse: 0.503463\tvalid_1's rmse: 1.5782\n",
      "[90000]\ttraining's rmse: 0.482691\tvalid_1's rmse: 1.57783\n",
      "[95000]\ttraining's rmse: 0.463292\tvalid_1's rmse: 1.57778\n",
      "[100000]\ttraining's rmse: 0.445512\tvalid_1's rmse: 1.57753\n",
      "[105000]\ttraining's rmse: 0.428989\tvalid_1's rmse: 1.5777\n",
      "[110000]\ttraining's rmse: 0.413407\tvalid_1's rmse: 1.57777\n",
      "[115000]\ttraining's rmse: 0.398954\tvalid_1's rmse: 1.57826\n",
      "[120000]\ttraining's rmse: 0.385364\tvalid_1's rmse: 1.57832\n",
      "Early stopping, best iteration is:\n",
      "[100840]\ttraining's rmse: 0.442754\tvalid_1's rmse: 1.57743\n",
      "Fold 7\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74348\tvalid_1's rmse: 1.97609\n",
      "[10000]\ttraining's rmse: 1.42672\tvalid_1's rmse: 1.8178\n",
      "[15000]\ttraining's rmse: 1.24042\tvalid_1's rmse: 1.74657\n",
      "[20000]\ttraining's rmse: 1.10863\tvalid_1's rmse: 1.70483\n",
      "[25000]\ttraining's rmse: 1.00833\tvalid_1's rmse: 1.67795\n",
      "[30000]\ttraining's rmse: 0.927311\tvalid_1's rmse: 1.65876\n",
      "[35000]\ttraining's rmse: 0.85984\tvalid_1's rmse: 1.64468\n",
      "[40000]\ttraining's rmse: 0.802764\tvalid_1's rmse: 1.63402\n",
      "[45000]\ttraining's rmse: 0.753445\tvalid_1's rmse: 1.6258\n",
      "[50000]\ttraining's rmse: 0.709101\tvalid_1's rmse: 1.61937\n",
      "[55000]\ttraining's rmse: 0.669808\tvalid_1's rmse: 1.61422\n",
      "[60000]\ttraining's rmse: 0.6346\tvalid_1's rmse: 1.61061\n",
      "[65000]\ttraining's rmse: 0.60278\tvalid_1's rmse: 1.60787\n",
      "[70000]\ttraining's rmse: 0.573514\tvalid_1's rmse: 1.60567\n",
      "[75000]\ttraining's rmse: 0.547272\tvalid_1's rmse: 1.60401\n",
      "[80000]\ttraining's rmse: 0.522739\tvalid_1's rmse: 1.60277\n",
      "[85000]\ttraining's rmse: 0.499906\tvalid_1's rmse: 1.60221\n",
      "[90000]\ttraining's rmse: 0.478878\tvalid_1's rmse: 1.60145\n",
      "[95000]\ttraining's rmse: 0.459598\tvalid_1's rmse: 1.6011\n",
      "[100000]\ttraining's rmse: 0.441445\tvalid_1's rmse: 1.60103\n",
      "[105000]\ttraining's rmse: 0.424756\tvalid_1's rmse: 1.60103\n",
      "[110000]\ttraining's rmse: 0.409088\tvalid_1's rmse: 1.60129\n",
      "[115000]\ttraining's rmse: 0.394429\tvalid_1's rmse: 1.60153\n",
      "[120000]\ttraining's rmse: 0.380733\tvalid_1's rmse: 1.60203\n",
      "Early stopping, best iteration is:\n",
      "[103759]\ttraining's rmse: 0.428878\tvalid_1's rmse: 1.60091\n",
      "Fold 8\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74588\tvalid_1's rmse: 1.97469\n",
      "[10000]\ttraining's rmse: 1.42705\tvalid_1's rmse: 1.81331\n",
      "[15000]\ttraining's rmse: 1.2393\tvalid_1's rmse: 1.74292\n",
      "[20000]\ttraining's rmse: 1.10712\tvalid_1's rmse: 1.70338\n",
      "[25000]\ttraining's rmse: 1.0065\tvalid_1's rmse: 1.67828\n",
      "[30000]\ttraining's rmse: 0.925475\tvalid_1's rmse: 1.66263\n",
      "[35000]\ttraining's rmse: 0.857835\tvalid_1's rmse: 1.65121\n",
      "[40000]\ttraining's rmse: 0.800671\tvalid_1's rmse: 1.64336\n",
      "[45000]\ttraining's rmse: 0.751533\tvalid_1's rmse: 1.63713\n",
      "[50000]\ttraining's rmse: 0.707479\tvalid_1's rmse: 1.63303\n",
      "[55000]\ttraining's rmse: 0.668862\tvalid_1's rmse: 1.6294\n",
      "[60000]\ttraining's rmse: 0.634196\tvalid_1's rmse: 1.62709\n",
      "[65000]\ttraining's rmse: 0.603141\tvalid_1's rmse: 1.62524\n",
      "[70000]\ttraining's rmse: 0.574714\tvalid_1's rmse: 1.62373\n",
      "[75000]\ttraining's rmse: 0.548992\tvalid_1's rmse: 1.62277\n",
      "[80000]\ttraining's rmse: 0.525029\tvalid_1's rmse: 1.62204\n",
      "[85000]\ttraining's rmse: 0.502649\tvalid_1's rmse: 1.62191\n",
      "[90000]\ttraining's rmse: 0.482321\tvalid_1's rmse: 1.62138\n",
      "[95000]\ttraining's rmse: 0.463507\tvalid_1's rmse: 1.62127\n",
      "[100000]\ttraining's rmse: 0.445876\tvalid_1's rmse: 1.62146\n",
      "[105000]\ttraining's rmse: 0.429508\tvalid_1's rmse: 1.62176\n",
      "[110000]\ttraining's rmse: 0.414053\tvalid_1's rmse: 1.62208\n",
      "[115000]\ttraining's rmse: 0.399848\tvalid_1's rmse: 1.62262\n",
      "Early stopping, best iteration is:\n",
      "[95140]\ttraining's rmse: 0.462957\tvalid_1's rmse: 1.62126\n",
      "Fold 9\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.75039\tvalid_1's rmse: 1.95673\n",
      "[10000]\ttraining's rmse: 1.43232\tvalid_1's rmse: 1.77465\n",
      "[15000]\ttraining's rmse: 1.24377\tvalid_1's rmse: 1.69541\n",
      "[20000]\ttraining's rmse: 1.11102\tvalid_1's rmse: 1.64932\n",
      "[25000]\ttraining's rmse: 1.0102\tvalid_1's rmse: 1.6203\n",
      "[30000]\ttraining's rmse: 0.92975\tvalid_1's rmse: 1.60076\n",
      "[35000]\ttraining's rmse: 0.862967\tvalid_1's rmse: 1.58634\n",
      "[40000]\ttraining's rmse: 0.80603\tvalid_1's rmse: 1.57646\n",
      "[45000]\ttraining's rmse: 0.757345\tvalid_1's rmse: 1.56853\n",
      "[50000]\ttraining's rmse: 0.713825\tvalid_1's rmse: 1.56301\n",
      "[55000]\ttraining's rmse: 0.675099\tvalid_1's rmse: 1.55814\n",
      "[60000]\ttraining's rmse: 0.640291\tvalid_1's rmse: 1.55478\n",
      "[65000]\ttraining's rmse: 0.609146\tvalid_1's rmse: 1.55218\n",
      "[70000]\ttraining's rmse: 0.580274\tvalid_1's rmse: 1.55023\n",
      "[75000]\ttraining's rmse: 0.554705\tvalid_1's rmse: 1.54842\n",
      "[80000]\ttraining's rmse: 0.530589\tvalid_1's rmse: 1.54737\n",
      "[85000]\ttraining's rmse: 0.507883\tvalid_1's rmse: 1.54658\n",
      "[90000]\ttraining's rmse: 0.48721\tvalid_1's rmse: 1.54621\n",
      "[95000]\ttraining's rmse: 0.467986\tvalid_1's rmse: 1.54595\n",
      "[100000]\ttraining's rmse: 0.450109\tvalid_1's rmse: 1.5457\n",
      "[105000]\ttraining's rmse: 0.433483\tvalid_1's rmse: 1.54585\n",
      "[110000]\ttraining's rmse: 0.417822\tvalid_1's rmse: 1.54598\n",
      "[115000]\ttraining's rmse: 0.403256\tvalid_1's rmse: 1.54627\n",
      "[120000]\ttraining's rmse: 0.389796\tvalid_1's rmse: 1.54655\n",
      "Early stopping, best iteration is:\n",
      "[101909]\ttraining's rmse: 0.443652\tvalid_1's rmse: 1.54561\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Fill nan album\n",
    "print(\"There is {} ratio is nan album\".format(len(df[df[\"album\"].isnull()])/len(df)))\n",
    "df[\"album\"]  = df[\"album\"].fillna(\"\")\n",
    "df[\"len_album_name\"] = df[\"album\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemixAlbum\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isOSTAlbum\"] = [ 1 if \"OST\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isSingleAlbum\"] = [ 1 if \"Single\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isBeatAlbum\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isTopHitAlbum\"] = [ 1 if \"Top Hits\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isCoverAlbum\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isEPAlbum\"] = [ 1 if \"EP\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isLienKhucAlbum\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"album\"]]\n",
    "\n",
    "df[\"album_name_is_title_name\"]= [1 if r.title in r.album  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album\"] = df[\"album\"].astype('category')\n",
    "df[\"album\"] =  df[\"album\"].cat.codes\n",
    "\n",
    "df[\"artist_name_cat\"] = df[\"artist_name\"].astype('category')\n",
    "df[\"artist_name_cat\"] =  df[\"artist_name_cat\"].cat.codes\n",
    "df[\"composers_name_cat\"] = df[\"composers_name\"].astype('category')\n",
    "df[\"composers_name_cat\"] =  df[\"composers_name_cat\"].cat.codes\n",
    "df[\"copyright_cat\"] = df[\"copyright\"].astype('category')\n",
    "df[\"copyright_cat\"] =  df[\"copyright_cat\"].cat.codes\n",
    "\n",
    "import re\n",
    "def get_min_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.min(ps)\n",
    "\n",
    "def get_max_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.max(ps)\n",
    "\n",
    "df[\"artist_id_min\"]=  df[\"artist_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"artist_id_min_cat\"] = df[\"artist_id_min\"].astype('category')\n",
    "df[\"artist_id_min_cat\"] =  df[\"artist_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_min\"]=  df[\"composers_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"composers_id_min_cat\"] = df[\"composers_id_min\"].astype('category')\n",
    "df[\"composers_id_min_cat\"] =  df[\"composers_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"artist_id_max\"]=  df[\"artist_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"artist_id_max_cat\"] = df[\"artist_id_max\"].astype('category')\n",
    "df[\"artist_id_max_cat\"] =  df[\"artist_id_max_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_max\"]=  df[\"composers_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"composers_id_max_cat\"] = df[\"composers_id_max\"].astype('category')\n",
    "df[\"composers_id_max_cat\"] =  df[\"composers_id_max_cat\"].cat.codes\n",
    "\n",
    "#New feature\n",
    "# df[\"group_album_artist_id_min_cat\"] = df.groupby([\"album\",\"artist_id_min_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_min_cat\"] = df[\"group_album_artist_id_min_cat\"].astype(\"category\").cat.codes\n",
    "# df[\"group_album_artist_id_max_cat\"] = df.groupby([\"album\",\"artist_id_max_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_max_cat\"] = df[\"group_album_artist_id_max_cat\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Fill genre\n",
    "print(\"There is {} ratio is nan genre\".format(len(df[df[\"genre\"].isnull()])/len(df)))\n",
    "df[\"genre\"]  = df[\"genre\"].fillna(\"No genre\")\n",
    "df[\"genre\"] = df[\"genre\"].astype('category')\n",
    "df[\"genre\"] =  df[\"genre\"].cat.codes\n",
    "\n",
    "# Fill album_artist\n",
    "print(\"There is {} ratio is nan album_artist\".format(len(df[df[\"album_artist\"].isnull()])/len(df)))\n",
    "df[\"album_artist\"]  = df[\"album_artist\"].fillna(\"No album_artist\")\n",
    "df[\"album_artist_contain_artistname\"]= [1 if r.album_artist in r.artist_name  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album_artist\"] = df[\"album_artist\"].astype('category')\n",
    "df[\"album_artist\"] =  df[\"album_artist\"].cat.codes\n",
    "\n",
    "# Fill track\n",
    "print(\"There is {} ratio is nan track\".format(len(df[df[\"track\"].isnull()])/len(df)))\n",
    "df[\"track\"]  = df[\"track\"].fillna(\"(1, 1)\")\n",
    "df[\"istrack11\"] = df[\"track\"] == \"(1, 1)\"\n",
    "def tracknum_to_value(track_num):\n",
    "    try:\n",
    "        \n",
    "        track_num = make_tuple(track_num)\n",
    "        if track_num[0] is not None:\n",
    "            return float(track_num[0]) / float(track_num[1])\n",
    "        else:\n",
    "            return 1.0\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "df[\"track\"] = df[\"track\"].apply(lambda t: tracknum_to_value(t))\n",
    "\n",
    "\n",
    "# Fill lyric\n",
    "print(\"There is {} ratio is nan lyric\".format(len(df[df[\"lyric\"].isnull()])/len(df)))\n",
    "df[\"lyric\"]  = df[\"lyric\"].fillna(\"\")\n",
    "df[\"islyric\"] = df[\"lyric\"].apply(lambda x:  True if len(x)  else False)\n",
    "df[\"num_line_lyric\"] = df[\"lyric\"].apply(lambda x : len(x.split(\"\\r\")))\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "from dateutil import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ast import literal_eval as make_tuple\n",
    "df['no_artist'] = df.artist_name.apply(lambda x: len(x.split(\",\")))\n",
    "df['no_composer'] = df.composers_name.apply(lambda x: len(x.split(\",\")))\n",
    "df[\"freq_artist\"] = df.groupby('artist_id')['artist_id'].transform('count').astype('float')\n",
    "df[\"freq_composer\"] = df.groupby('composers_id')['composers_id'].transform('count').astype('float')\n",
    "df[\"freq_artist_min\"] = df.groupby('artist_id_min_cat')['artist_id_min_cat'].transform('count').astype('float')\n",
    "df[\"freq_composer_min\"] = df.groupby('composers_id_min_cat')['composers_id_min_cat'].transform('count').astype('float')\n",
    "\n",
    "df[\"num_album_per_min_artist\"] = df.groupby(['artist_id_min_cat','album'])['album'].transform('count').astype('float')\n",
    "df[\"num_album_per_min_composer\"] = df.groupby(['composers_id_min','album'])['album'].transform('count').astype('float')\n",
    "\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df.release_time)\n",
    "df[\"year\"] = df[\"datetime\"].dt.year\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"day\"] = df[\"datetime\"].dt.day\n",
    "df[\"dayofyear\"] = df[\"datetime\"].dt.dayofyear\n",
    "df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "from datetime import date \n",
    "import holidays \n",
    "\n",
    "in_holidays = holidays.HolidayBase() \n",
    "for i in range(26,32):\n",
    "    in_holidays.append(str(i)+'-01-2017')\n",
    "in_holidays.append('01-02-2017')\n",
    "for i in range(14,21):\n",
    "    in_holidays.append(str(i)+'-02-2018')\n",
    "in_holidays.append('30-04-2017')\n",
    "in_holidays.append('30-04-2018')\n",
    "in_holidays.append('01-01-2017')\n",
    "in_holidays.append('01-01-2018')\n",
    "in_holidays.append('14-02-2017')\n",
    "in_holidays.append('14-02-2018')\n",
    "in_holidays.append('08-03-2017')\n",
    "in_holidays.append('08-03-2018')\n",
    "in_holidays.append('01-05-2017')\n",
    "in_holidays.append('01-05-2018')\n",
    "in_holidays.append('06-04-2017')\n",
    "in_holidays.append('25-04-2018')\n",
    "in_holidays.append('01-06-2017')\n",
    "in_holidays.append('01-06-2018')\n",
    "in_holidays.append('04-10-2017')\n",
    "in_holidays.append('24-09-2018')\n",
    "in_holidays.append('20-10-2017')\n",
    "in_holidays.append('20-10-2018')\n",
    "in_holidays.append('20-11-2017')\n",
    "in_holidays.append('20-11-2018')\n",
    "in_holidays.append('24-12-2017')\n",
    "in_holidays.append('24-12-2018')\n",
    "df['isHoliday'] = df.release_time.apply(lambda x: x in in_holidays)\n",
    "\n",
    "\n",
    "\n",
    "df[\"len_of_songname\"] = df[\"title\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemix\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isOST\"] = [ 1 if \"OST\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isBeat\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isVersion\"] = [ 1 if \"Version\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isCover\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isLienKhuc\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"title\"]]\n",
    "\n",
    "\n",
    "\n",
    "def find_num_song_release_in_final_month(df, day):\n",
    "    month5th = day + relativedelta.relativedelta(months=5)\n",
    "    month6th = day + relativedelta.relativedelta(months=6)  \n",
    "    return len(df.datetime[(df.datetime >= month5th)&(df.datetime<=month6th)])\n",
    "\n",
    "\n",
    "\n",
    "df[\"num_song_release_in_final_month\"] = df.datetime.apply(lambda d:find_num_song_release_in_final_month(df ,d))\n",
    "\n",
    "#It seems like all songs on albums release at the same time, so groupby by release_time will create album \n",
    "df[\"album_right\"] = df.groupby(df.release_time).ngroup().astype(\"category\").cat.codes\n",
    "df[\"day_release\"] = df.groupby([\"year\",\"dayofyear\"]).ngroup().astype(\"category\").cat.codes\n",
    "df[\"numsongInAlbum\"] = df.groupby(\"album_right\")[\"album_right\"].transform(\"count\")\n",
    "df[\"isSingleAlbum_onesong\"]= df[\"isSingleAlbum\"] & (df[\"numsongInAlbum\"]==1)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "chosen_features = [\"album_right\", \"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "                   \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "                  \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "                  \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "                  \"isBeatAlbum\", \"isCoverAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "                  \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "                   \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "                  \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "                   \"numsongInAlbum\",\"isSingleAlbum_onesong\" ]\n",
    "\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "df_test = df[df.dataset==\"test\"]\n",
    "\n",
    "param = {\n",
    "    'bagging_freq': 20,          \n",
    "    'bagging_fraction': 0.95,   'boost_from_average':'false',   \n",
    "    'boost': 'gbdt',             'feature_fraction': 0.1,     'learning_rate': 0.001,\n",
    "    'max_depth': -1,             'metric':'root_mean_squared_error', 'min_data_in_leaf': 5,   \n",
    "       'num_leaves': 50,            \n",
    "    'num_threads': 4,              'tree_learner': 'serial',   'objective': 'regression',\n",
    "    'reg_alpha': 0.1002650970728192, 'reg_lambda': 0.1003427518866501,'verbosity': 1,\n",
    "    \"seed\": 99999\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "labels= df_train.label\n",
    "# fig, axes = plt.subplots(5, 1, figsize=(10, 10*5))\n",
    "# axes = axes.flat\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][chosen_features], label=labels.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][chosen_features], label=labels.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 20000)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][chosen_features], num_iteration=clf.best_iteration)\n",
    "    predictions += clf.predict(df_test[chosen_features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gymai]",
   "language": "python",
   "name": "conda-env-gymai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
