{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from pyvi import ViUtils\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "TRAININFO = \"/media/DATA/zalo-hit-song-prediction/csv/train_info.tsv\"\n",
    "TRAINRANK =  \"/media/DATA/zalo-hit-song-prediction/csv/train_rank.csv\"\n",
    "TESTINFO = \"/media/DATA/zalo-hit-song-prediction/csv/test_info.tsv\"\n",
    "Track_info = \"/media/DATA/zalo-hit-song-prediction/csv/all_track_info.csv\"\n",
    "Audio_info = \"/media/DATA/zalo-hit-song-prediction/csv/all_track_audio_features.csv\"\n",
    "df_i = pd.read_csv(TRAININFO, delimiter='\\t',encoding='utf-8')\n",
    "df_r = pd.read_csv(TRAINRANK)\n",
    "df_i_train = df_i.merge(df_r, left_on='ID', right_on='ID')\n",
    "df_i_train[\"dataset\"] = \"train\"\n",
    "\n",
    "df_i_test = pd.read_csv(TESTINFO, delimiter='\\t',encoding='utf-8')\n",
    "df_i_test[\"label\"] = np.nan\n",
    "df_i_test[\"dataset\"] = \"test\"\n",
    "\n",
    "df = pd.concat([df_i_train, df_i_test])\n",
    "df_track_info = pd.read_csv(Track_info)\n",
    "df = df.merge(df_track_info, left_on='ID', right_on='ID')\n",
    "df_audio_features = pd.read_csv(Audio_info)\n",
    "df =df.merge(df_audio_features,left_on=\"ID\",right_on=\"ID\", how=\"left\")\n",
    "df = df[['ID','title','artist_name','lyric','label','dataset']]\n",
    "df.head()\n",
    "\n",
    "with open(\"/media/DATA/zalo-hit-song-prediction/csv/stopwords/vietnamese-stopwords.txt\", 'r') as f:\n",
    "    filecontent=f.readlines()\n",
    "stopwords = list(set([f.strip() for f in filecontent]))\n",
    "\n",
    "\n",
    "stopwords_tokenized = list(set([ViTokenizer.tokenize(stopword.lower()) for stopword in stopwords]))\n",
    "set([ViTokenizer.tokenize(word) for word in stopwords_tokenized]) - set(stopwords_tokenized)\n",
    "stopwords_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ben/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def remove_stopwords(string):\n",
    "    new_string = [word for word in string if word not in stopwords]\n",
    "    return new_string\n",
    "df.dropna(subset=['lyric'], inplace=True) # drop songs with no lyrics\n",
    "df_train = df[df.dataset=='train']\n",
    "df_test = df[df.dataset=='test']\n",
    "df_train = df_train.drop_duplicates(subset='lyric', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_union\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "'''\n",
    "def encode_column(df, column_name, to_n_columns=12):\n",
    "    \n",
    "    hasher=category_encoders.hashing.HashingEncoder()\n",
    "    #encoded=hasher.fit_transform(np.expand_dims(df[\"university_name\"].values, axis=-1),verbose=True, cols=None, drop_invariant=True, return_df=True, hash_method='sha1')\n",
    "    tmp=pd.DataFrame(np.expand_dims(df[\"university_name\"].values, axis=-1))\n",
    "    encoded_df=hasher.hashing_trick(tmp, hashing_method='md5', N=to_n_columns, make_copy=False)\n",
    "    \n",
    "    encoded_col_names=list(encoded_df.columns.values)\n",
    "    tmp=df.copy()\n",
    "    tmp[\"index\"]=tmp.index\n",
    "    tmp=tmp.reset_index()\n",
    "    df=pd.merge(tmp, encoded_df, how='left', left_index=True, right_index=True, sort=False,\n",
    "             suffixes=('_left', '_right'),validate=\"one_to_one\")\n",
    "    \n",
    "    print(type(encoded_col_names))\n",
    "    df.set_index(\"index\",inplace=True)\n",
    "    return df, encoded_col_names\n",
    "'''\n",
    "class TextColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "\n",
    "# generate features - in this case just one.\n",
    "feature = Pipeline([\n",
    "                ('selector_1', TextColumnSelector(key='lyric')),\n",
    "                ('vect', HashingVectorizer(n_features = 2**4, \n",
    "                                           tokenizer=ViTokenizer.tokenize, \n",
    "                                           lowercase=True, \n",
    "                                           stop_words=stopwords_tokenized,\n",
    "                                           norm='l2')),\n",
    "                ]\n",
    "                #('vect', CountVectorizer(analyzer='word', tokenizer=str.split, stop_words=stopwords)),\n",
    "                #('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True))\n",
    "                )\n",
    "# Another option:\n",
    "##hasher=category_encoders.hashing.HashingEncoder()\n",
    "##encoded_df=hasher.hashing_trick(tmp, hashing_method='md5', N=to_n_columns, make_copy=False)\n",
    "#vectorizer = HashingVectorizer(n_features = 2**4, tokenizer=word_tokenize, lowercase=True, stop_words=stopwords,  norm='l2')\n",
    "#vectors = vectorizer.fit_transform(df_train.lyric)\n",
    "\n",
    "feats_list=[('text', feature)]\n",
    "feats = FeatureUnion(feats_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'từ giờ'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   44.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 100 | elapsed:  4.2min remaining:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.4min finished\n",
      "/media/DATA/anaconda3/envs/thriai/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', '_', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'â', 'ã', 'è', 'é', 'ê', 'ì', 'í', 'ò', 'ó', 'ô', 'õ', 'ù', 'ú', 'ă', 'đ', 'ĩ', 'ũ', 'ả', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ặ', 'ẹ', 'ẻ', 'ẽ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'ố', 'ổ', 'ỗ', 'ộ', 'ỡ', 'ợ', 'ụ', 'ủ', 'ứ', 'ữ', 'ự', 'ỳ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.920596703520487 +/- 0.0930542621402546\n",
      "-2.920596703520487 {'features__text__vect__ngram_range': (1, 1), 'features__text__vect__n_features': 100000, 'classifier__gamma': 21.54434690031882, 'classifier__C': 0.2682695795279725}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_union\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "def rmse(targets, predictions):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "#model=LogisticRegression(class_weight=\"balanced\",penalty='l1', random_state=1)\n",
    "#model_lc=\"classifier__C\"\n",
    "#values=[719.686, ] # np.logspace(-4, 4, 8) # 0.001 to 1e4\n",
    "#http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html \n",
    "model=SVR(kernel=\"rbf\", C=np.logspace(-4, 4, 8))\n",
    "model_lc=\"classifier__gamma\"\n",
    "values = np.logspace(-4, 4, 4)\n",
    "model_lc_2 = \"classifier__C\"\n",
    "values2 =  np.logspace(-4, 4, 8)[0:4]\n",
    "\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', model),\n",
    "])\n",
    "\n",
    "#param_range = np.logspace(1, 10, 5)\n",
    "\n",
    "parameters = {'features__text__vect__n_features': np.array([1e3,1e4,1e5]).astype('int64'),\n",
    "              'features__text__vect__ngram_range': [(1,1)],# [(1,1), (1, 2),], # try bigrams or unigrams\n",
    "              model_lc: values,\n",
    "              model_lc_2:values2,\n",
    "             }\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=99999)\n",
    "gs_clf = RandomizedSearchCV(model_pipeline, \n",
    "                      parameters, \n",
    "                      scoring=rmse_scorer, \n",
    "                      cv=skf,\n",
    "                      n_jobs=-1,\n",
    "                      return_train_score=False,\n",
    "                      error_score='raise',\n",
    "                      n_iter=10,\n",
    "                      verbose=10\n",
    "                    )\n",
    "\n",
    "#gs_clf.get_params().keys()\n",
    "#[i for i in gs_clf.get_params().keys()]\n",
    "gs_clf.fit(df_train, df_train.label)\n",
    "\n",
    "print(gs_clf.cv_results_['mean_test_score'][gs_clf.best_index_], \"+/-\" ,gs_clf.cv_results_['std_test_score'][gs_clf.best_index_])\n",
    "print(gs_clf.best_score_, gs_clf.best_params_)\n",
    "pd.DataFrame(gs_clf.cv_results_).to_csv('svr_lyric_unigram2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 1.38949549e-03, 1.93069773e-02, 2.68269580e-01])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_clf.cv_results_).to_csv('svr_lyric_unigram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__gamma</th>\n",
       "      <th>param_features__text__vect__n_features</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.26827</td>\n",
       "      <td>0.26827</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.011608</td>\n",
       "      <td>0.008735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.26827</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.025377</td>\n",
       "      <td>0.004742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>51.7947</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.027342</td>\n",
       "      <td>0.001695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.26827</td>\n",
       "      <td>0.0013895</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.027458</td>\n",
       "      <td>0.001704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.72759</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.056874</td>\n",
       "      <td>0.037078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.062024</td>\n",
       "      <td>0.040747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.7947</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.062024</td>\n",
       "      <td>0.040749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.7947</td>\n",
       "      <td>719.686</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.085559</td>\n",
       "      <td>0.046583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>719.686</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.090328</td>\n",
       "      <td>0.047625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>719.686</td>\n",
       "      <td>51.7947</td>\n",
       "      <td>10000</td>\n",
       "      <td>3.546580</td>\n",
       "      <td>0.146699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__C param_classifier__gamma  \\\n",
       "1             0.26827                 0.26827   \n",
       "2             0.26827                   10000   \n",
       "7              0.0001                 51.7947   \n",
       "3             0.26827               0.0013895   \n",
       "8             3.72759                   10000   \n",
       "9               10000                   10000   \n",
       "5             51.7947                   10000   \n",
       "4             51.7947                 719.686   \n",
       "0               10000                 719.686   \n",
       "6             719.686                 51.7947   \n",
       "\n",
       "  param_features__text__vect__n_features  mean_test_score  std_test_score  \n",
       "1                                  10000         3.011608        0.008735  \n",
       "2                                  10000         3.025377        0.004742  \n",
       "7                                  10000         3.027342        0.001695  \n",
       "3                                  10000         3.027458        0.001704  \n",
       "8                                  10000         3.056874        0.037078  \n",
       "9                                  10000         3.062024        0.040747  \n",
       "5                                  10000         3.062024        0.040749  \n",
       "4                                  10000         3.085559        0.046583  \n",
       "0                                  10000         3.090328        0.047625  \n",
       "6                                  10000         3.546580        0.146699  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_clf.cv_results_)[['param_classifier__C','param_classifier__gamma', 'param_features__text__vect__n_features','mean_test_score', 'std_test_score']].sort_values('mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.277847802266441, '+/-', 0.08664532999630303)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "def rmse(targets, predictions):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "dummy_clf = sklearn.dummy.DummyClassifier(strategy='stratified', random_state=99999)\n",
    "skf = StratifiedKFold(n_splits=10, random_state=99999)\n",
    "from sklearn.model_selection import cross_validate\n",
    "cv_pred = cross_validate(dummy_clf, df_train['lyric'], df_train.label, scoring=rmse_scorer, cv=skf)\n",
    "cv_pred['test_score'].mean(),  '+/-', cv_pred['test_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e+09, 1.e+10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
