{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>composers_name</th>\n",
       "      <th>composers_id</th>\n",
       "      <th>release_time</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.tuning_frequency</th>\n",
       "      <th>tonal.tuning_nontempered_energy_ratio</th>\n",
       "      <th>tonal.chords_key</th>\n",
       "      <th>tonal.chords_scale</th>\n",
       "      <th>tonal.key_edma.key</th>\n",
       "      <th>tonal.key_edma.scale</th>\n",
       "      <th>tonal.key_krumhansl.key</th>\n",
       "      <th>tonal.key_krumhansl.scale</th>\n",
       "      <th>tonal.key_temperley.key</th>\n",
       "      <th>tonal.key_temperley.scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073748245</td>\n",
       "      <td>Đêm Chôn Dầu Vượt Biển</td>\n",
       "      <td>Như Quỳnh</td>\n",
       "      <td>551</td>\n",
       "      <td>Châu Đình An</td>\n",
       "      <td>5765</td>\n",
       "      <td>2017-10-01 22:07:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.601478</td>\n",
       "      <td>D</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1073751978</td>\n",
       "      <td>Mùa Thu Trong Mưa</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Trường Sa</td>\n",
       "      <td>100105</td>\n",
       "      <td>2017-10-01 20:58:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.944516</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1073835561</td>\n",
       "      <td>Rồi Ánh Trăng Tan</td>\n",
       "      <td>Lưu Bích</td>\n",
       "      <td>450</td>\n",
       "      <td>Quốc Bảo</td>\n",
       "      <td>4355</td>\n",
       "      <td>2017-11-01 18:16:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.957651</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1073856553</td>\n",
       "      <td>Còn Thương Rau Đắng Mọc Sau Hè</td>\n",
       "      <td>Như Quỳnh</td>\n",
       "      <td>551</td>\n",
       "      <td>Bắc Sơn</td>\n",
       "      <td>7686</td>\n",
       "      <td>2017-11-01 17:36:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>441.272583</td>\n",
       "      <td>0.796499</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1073929630</td>\n",
       "      <td>Người Điên Biết Yêu</td>\n",
       "      <td>Như Loan</td>\n",
       "      <td>513</td>\n",
       "      <td>Lê Minh Kha</td>\n",
       "      <td>100466</td>\n",
       "      <td>2017-11-01 17:49:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.946167</td>\n",
       "      <td>0.860068</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1073929880</td>\n",
       "      <td>Đàn Bà</td>\n",
       "      <td>Don Hồ</td>\n",
       "      <td>6515</td>\n",
       "      <td>Song Ngọc</td>\n",
       "      <td>100288</td>\n",
       "      <td>2017-10-01 21:33:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>436.960693</td>\n",
       "      <td>0.768609</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1073949310</td>\n",
       "      <td>Yêu Một Người Sống Bên Một Người</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Hoài An</td>\n",
       "      <td>100133</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>442.548920</td>\n",
       "      <td>0.701749</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9078</td>\n",
       "      <td>1073994292</td>\n",
       "      <td>Giấc Mơ Mình Em</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Minh Vy</td>\n",
       "      <td>100019</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>...</td>\n",
       "      <td>441.782684</td>\n",
       "      <td>0.659616</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1073994297</td>\n",
       "      <td>Mất Nhau Trong Đời</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Huỳnh Nhật Tân</td>\n",
       "      <td>100306</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.913990</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1073994298</td>\n",
       "      <td>Những Ân Tình Xưa</td>\n",
       "      <td>Minh Tuyết, Bằng Kiều</td>\n",
       "      <td>455.306</td>\n",
       "      <td>Hoài An</td>\n",
       "      <td>100133</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>437.971466</td>\n",
       "      <td>0.726335</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1073994300</td>\n",
       "      <td>Ở Nơi Đó Em Cười</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Hoài An</td>\n",
       "      <td>100133</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>437.718536</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1074011942</td>\n",
       "      <td>Tình Chấp Nhận</td>\n",
       "      <td>Quỳnh Vi</td>\n",
       "      <td>7928</td>\n",
       "      <td>Trần Đức</td>\n",
       "      <td>7898</td>\n",
       "      <td>2017-11-01 18:49:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.763123</td>\n",
       "      <td>0.655899</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1074239565</td>\n",
       "      <td>Rồi Mai Tôi Đưa Em</td>\n",
       "      <td>Trần Thái Hòa</td>\n",
       "      <td>901</td>\n",
       "      <td>Trường Sa</td>\n",
       "      <td>100105</td>\n",
       "      <td>2017-10-01 20:58:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.508636</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1074245455</td>\n",
       "      <td>Thiên Đàng Đánh Mất</td>\n",
       "      <td>Dương Triệu Vũ</td>\n",
       "      <td>5072</td>\n",
       "      <td>Nguyễn Hồng Thuận</td>\n",
       "      <td>9140</td>\n",
       "      <td>2017-11-01 18:42:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>443.316437</td>\n",
       "      <td>0.793990</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>major</td>\n",
       "      <td>A</td>\n",
       "      <td>major</td>\n",
       "      <td>F</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1074245511</td>\n",
       "      <td>Vai Phụ</td>\n",
       "      <td>Loan Châu</td>\n",
       "      <td>827</td>\n",
       "      <td>Phạm Khải Tuấn</td>\n",
       "      <td>100116</td>\n",
       "      <td>2017-10-01 21:22:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.983561</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>1074247432</td>\n",
       "      <td>Tình Nhỏ Mau Quên</td>\n",
       "      <td>Quang Lê, Hương Thủy</td>\n",
       "      <td>828.87</td>\n",
       "      <td>Hàn Châu</td>\n",
       "      <td>100406</td>\n",
       "      <td>2017-10-01 22:32:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>441.272583</td>\n",
       "      <td>0.738260</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>9079</td>\n",
       "      <td>1074250503</td>\n",
       "      <td>Rồi 30 Năm Qua</td>\n",
       "      <td>Tâm Đoan</td>\n",
       "      <td>518</td>\n",
       "      <td>Nhật Ngân</td>\n",
       "      <td>100218</td>\n",
       "      <td>2017-10-01 22:07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>...</td>\n",
       "      <td>440.508636</td>\n",
       "      <td>0.702865</td>\n",
       "      <td>F#</td>\n",
       "      <td>minor</td>\n",
       "      <td>B</td>\n",
       "      <td>major</td>\n",
       "      <td>B</td>\n",
       "      <td>major</td>\n",
       "      <td>B</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>1074254689</td>\n",
       "      <td>Chơi Vơi Dòng Đời</td>\n",
       "      <td>Duy Linh</td>\n",
       "      <td>918</td>\n",
       "      <td>Huy Liêu</td>\n",
       "      <td>430572</td>\n",
       "      <td>2017-11-07 23:40:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.961273</td>\n",
       "      <td>D</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>1074254690</td>\n",
       "      <td>Dòng Sông Quê Hương</td>\n",
       "      <td>Lan Phương</td>\n",
       "      <td>6517</td>\n",
       "      <td>Huy Liêu</td>\n",
       "      <td>430572</td>\n",
       "      <td>2017-11-07 23:43:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.676015</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>9080</td>\n",
       "      <td>1074254709</td>\n",
       "      <td>Hòa Bình Hoan Ca</td>\n",
       "      <td>Hùng Phú, Duy Linh</td>\n",
       "      <td>18241.918</td>\n",
       "      <td>Huy Liêu</td>\n",
       "      <td>430572</td>\n",
       "      <td>2017-11-07 23:47:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>...</td>\n",
       "      <td>436.960693</td>\n",
       "      <td>0.790986</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          ID                             title  \\\n",
       "0       0  1073748245            Đêm Chôn Dầu Vượt Biển   \n",
       "1       1  1073751978                 Mùa Thu Trong Mưa   \n",
       "2       2  1073835561                 Rồi Ánh Trăng Tan   \n",
       "3       3  1073856553    Còn Thương Rau Đắng Mọc Sau Hè   \n",
       "4       4  1073929630               Người Điên Biết Yêu   \n",
       "5       5  1073929880                            Đàn Bà   \n",
       "6       6  1073949310  Yêu Một Người Sống Bên Một Người   \n",
       "7    9078  1073994292                   Giấc Mơ Mình Em   \n",
       "8       7  1073994297                Mất Nhau Trong Đời   \n",
       "9       8  1073994298                 Những Ân Tình Xưa   \n",
       "10      9  1073994300                  Ở Nơi Đó Em Cười   \n",
       "11     10  1074011942                    Tình Chấp Nhận   \n",
       "12     11  1074239565                Rồi Mai Tôi Đưa Em   \n",
       "13     12  1074245455               Thiên Đàng Đánh Mất   \n",
       "14     13  1074245511                           Vai Phụ   \n",
       "15     14  1074247432                 Tình Nhỏ Mau Quên   \n",
       "16   9079  1074250503                    Rồi 30 Năm Qua   \n",
       "17     15  1074254689                 Chơi Vơi Dòng Đời   \n",
       "18     16  1074254690               Dòng Sông Quê Hương   \n",
       "19   9080  1074254709                  Hòa Bình Hoan Ca   \n",
       "\n",
       "              artist_name  artist_id     composers_name composers_id  \\\n",
       "0               Như Quỳnh        551       Châu Đình An         5765   \n",
       "1              Minh Tuyết        455          Trường Sa       100105   \n",
       "2                Lưu Bích        450           Quốc Bảo         4355   \n",
       "3               Như Quỳnh        551            Bắc Sơn         7686   \n",
       "4                Như Loan        513        Lê Minh Kha       100466   \n",
       "5                  Don Hồ       6515          Song Ngọc       100288   \n",
       "6              Minh Tuyết        455            Hoài An       100133   \n",
       "7              Minh Tuyết        455            Minh Vy       100019   \n",
       "8              Minh Tuyết        455     Huỳnh Nhật Tân       100306   \n",
       "9   Minh Tuyết, Bằng Kiều    455.306            Hoài An       100133   \n",
       "10             Minh Tuyết        455            Hoài An       100133   \n",
       "11               Quỳnh Vi       7928           Trần Đức         7898   \n",
       "12          Trần Thái Hòa        901          Trường Sa       100105   \n",
       "13         Dương Triệu Vũ       5072  Nguyễn Hồng Thuận         9140   \n",
       "14              Loan Châu        827     Phạm Khải Tuấn       100116   \n",
       "15   Quang Lê, Hương Thủy     828.87           Hàn Châu       100406   \n",
       "16               Tâm Đoan        518          Nhật Ngân       100218   \n",
       "17               Duy Linh        918           Huy Liêu       430572   \n",
       "18             Lan Phương       6517           Huy Liêu       430572   \n",
       "19     Hùng Phú, Duy Linh  18241.918           Huy Liêu       430572   \n",
       "\n",
       "           release_time  label dataset  ...  tonal.tuning_frequency  \\\n",
       "0   2017-10-01 22:07:00    7.0   train  ...              440.000000   \n",
       "1   2017-10-01 20:58:00    3.0   train  ...              434.193115   \n",
       "2   2017-11-01 18:16:00    6.0   train  ...              434.193115   \n",
       "3   2017-11-01 17:36:00    2.0   train  ...              441.272583   \n",
       "4   2017-11-01 17:49:00    7.0   train  ...              434.946167   \n",
       "5   2017-10-01 21:33:00    3.0   train  ...              436.960693   \n",
       "6   2017-11-01 18:27:00    4.0   train  ...              442.548920   \n",
       "7   2017-11-01 18:27:00    NaN    test  ...              441.782684   \n",
       "8   2017-11-01 18:27:00    8.0   train  ...              434.193115   \n",
       "9   2017-11-01 18:27:00    4.0   train  ...              437.971466   \n",
       "10  2017-11-01 18:27:00    8.0   train  ...              437.718536   \n",
       "11  2017-11-01 18:49:00    5.0   train  ...              440.763123   \n",
       "12  2017-10-01 20:58:00    3.0   train  ...              440.508636   \n",
       "13  2017-11-01 18:42:00    9.0   train  ...              443.316437   \n",
       "14  2017-10-01 21:22:00    6.0   train  ...              434.193115   \n",
       "15  2017-10-01 22:32:00    1.0   train  ...              441.272583   \n",
       "16  2017-10-01 22:07:00    NaN    test  ...              440.508636   \n",
       "17  2017-11-07 23:40:00    9.0   train  ...              434.193115   \n",
       "18  2017-11-07 23:43:00    8.0   train  ...              440.000000   \n",
       "19  2017-11-07 23:47:00    NaN    test  ...              436.960693   \n",
       "\n",
       "   tonal.tuning_nontempered_energy_ratio tonal.chords_key tonal.chords_scale  \\\n",
       "0                               0.601478                D              major   \n",
       "1                               0.944516                C              minor   \n",
       "2                               0.957651               Bb              major   \n",
       "3                               0.796499                G              minor   \n",
       "4                               0.860068                A              minor   \n",
       "5                               0.768609                A              minor   \n",
       "6                               0.701749                D              minor   \n",
       "7                               0.659616               Bb              major   \n",
       "8                               0.913990                C              minor   \n",
       "9                               0.726335                A              minor   \n",
       "10                              0.732484                A              minor   \n",
       "11                              0.655899                A              minor   \n",
       "12                              0.673602                D              minor   \n",
       "13                              0.793990                G              minor   \n",
       "14                              0.983561                A              minor   \n",
       "15                              0.738260               Eb              minor   \n",
       "16                              0.702865               F#              minor   \n",
       "17                              0.961273                D              major   \n",
       "18                              0.676015               C#              major   \n",
       "19                              0.790986                A              minor   \n",
       "\n",
       "   tonal.key_edma.key tonal.key_edma.scale  tonal.key_krumhansl.key  \\\n",
       "0                   G                major                        G   \n",
       "1                   C                minor                        C   \n",
       "2                   D                minor                        D   \n",
       "3                   G                minor                        G   \n",
       "4                   D                minor                        D   \n",
       "5                   D                minor                        A   \n",
       "6                   A                minor                        A   \n",
       "7                  Bb                major                       Bb   \n",
       "8                   C                minor                        C   \n",
       "9                   A                minor                        A   \n",
       "10                  D                minor                        D   \n",
       "11                  A                minor                        A   \n",
       "12                  D                minor                        D   \n",
       "13                  A                major                        A   \n",
       "14                  A                minor                        A   \n",
       "15                 Eb                minor                       Eb   \n",
       "16                  B                major                        B   \n",
       "17                  G                major                        G   \n",
       "18                 C#                major                       C#   \n",
       "19                  G                major                        G   \n",
       "\n",
       "   tonal.key_krumhansl.scale  tonal.key_temperley.key  \\\n",
       "0                      major                        G   \n",
       "1                      minor                        C   \n",
       "2                      minor                        D   \n",
       "3                      minor                        G   \n",
       "4                      minor                        D   \n",
       "5                      minor                        A   \n",
       "6                      minor                        A   \n",
       "7                      major                       Bb   \n",
       "8                      minor                        C   \n",
       "9                      minor                        A   \n",
       "10                     minor                        A   \n",
       "11                     minor                        A   \n",
       "12                     minor                        D   \n",
       "13                     major                        F   \n",
       "14                     minor                        A   \n",
       "15                     minor                       Eb   \n",
       "16                     major                        B   \n",
       "17                     major                        G   \n",
       "18                     major                       C#   \n",
       "19                     major                        G   \n",
       "\n",
       "    tonal.key_temperley.scale  \n",
       "0                       major  \n",
       "1                       minor  \n",
       "2                       minor  \n",
       "3                       minor  \n",
       "4                       minor  \n",
       "5                       minor  \n",
       "6                       minor  \n",
       "7                       major  \n",
       "8                       minor  \n",
       "9                       minor  \n",
       "10                      minor  \n",
       "11                      minor  \n",
       "12                      minor  \n",
       "13                      major  \n",
       "14                      minor  \n",
       "15                      minor  \n",
       "16                      major  \n",
       "17                      major  \n",
       "18                      major  \n",
       "19                      major  \n",
       "\n",
       "[20 rows x 137 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "TRAININFO = \"/data/HitSongPrediction/train_info.tsv\"\n",
    "TRAINRANK =  \"/data/HitSongPrediction/train_rank.csv\"\n",
    "TESTINFO = \"/data/HitSongPrediction/test_info.tsv\"\n",
    "SUBMISSION = \"/data/HitSongPrediction/submission.csv\"\n",
    "\n",
    "# Prepare data\n",
    "df_i = pd.read_csv(TRAININFO, delimiter='\\t',encoding='utf-8')\n",
    "df_r = pd.read_csv(TRAINRANK)\n",
    "df_i_train = df_i.merge(df_r, left_on='ID', right_on='ID')\n",
    "df_i_train[\"dataset\"] = \"train\"\n",
    "\n",
    "df_i_test = pd.read_csv(TESTINFO, delimiter='\\t',encoding='utf-8')\n",
    "df_i_test[\"label\"] = np.nan\n",
    "df_i_test[\"dataset\"] = \"test\"\n",
    "\n",
    "df = pd.concat([df_i_train, df_i_test])\n",
    "df_track_info = pd.read_csv(\"../../csv/all_track_info.csv\")\n",
    "df = df.merge(df_track_info, left_on='ID', right_on='ID')\n",
    "df_audio_features = pd.read_csv(\"../../csv/all_track_audio_features.csv\")\n",
    "df =df.merge(df_audio_features,left_on=\"ID\",right_on=\"ID\", how=\"left\")\n",
    "\n",
    "# Sort by ID\n",
    "df = df.sort_values(by=['ID'])\n",
    "df= df.reset_index()\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0.24038838760298156 ratio is nan album\n",
      "There is 0.0017653981953707335 ratio is nan genre\n",
      "There is 0.24038838760298156 ratio is nan album_artist\n",
      "There is 0.0007846214201647705 ratio is nan track\n",
      "There is 0.6722244017261672 ratio is nan lyric\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Fill nan album\n",
    "print(\"There is {} ratio is nan album\".format(len(df[df[\"album\"].isnull()])/len(df)))\n",
    "df[\"album\"]  = df[\"album\"].fillna(\"\")\n",
    "df[\"len_album_name\"] = df[\"album\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemixAlbum\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isOSTAlbum\"] = [ 1 if \"OST\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isSingleAlbum\"] = [ 1 if \"Single\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isBeatAlbum\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isTopHitAlbum\"] = [ 1 if \"Top Hits\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isCoverAlbum\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isEPAlbum\"] = [ 1 if \"EP\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isLienKhucAlbum\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"album\"]]\n",
    "\n",
    "df[\"album_name_is_title_name\"]= [1 if r.title in r.album  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album\"] = df[\"album\"].astype('category')\n",
    "df[\"album\"] =  df[\"album\"].cat.codes\n",
    "#It seems like all songs on albums release at the same time, so groupby by release_time will create album \n",
    "df[\"album_right\"] = df.groupby(df.release_time).ngroup().astype(\"category\").cat.codes\n",
    "\n",
    "df[\"artist_name_cat\"] = df[\"artist_name\"].astype('category')\n",
    "df[\"artist_name_cat\"] =  df[\"artist_name_cat\"].cat.codes\n",
    "df[\"composers_name_cat\"] = df[\"composers_name\"].astype('category')\n",
    "df[\"composers_name_cat\"] =  df[\"composers_name_cat\"].cat.codes\n",
    "df[\"copyright_cat\"] = df[\"copyright\"].astype('category')\n",
    "df[\"copyright_cat\"] =  df[\"copyright_cat\"].cat.codes\n",
    "\n",
    "import re\n",
    "def get_min_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.min(ps)\n",
    "\n",
    "def get_max_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.max(ps)\n",
    "\n",
    "df[\"artist_id_min\"]=  df[\"artist_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"artist_id_min_cat\"] = df[\"artist_id_min\"].astype('category')\n",
    "df[\"artist_id_min_cat\"] =  df[\"artist_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_min\"]=  df[\"composers_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"composers_id_min_cat\"] = df[\"composers_id_min\"].astype('category')\n",
    "df[\"composers_id_min_cat\"] =  df[\"composers_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"artist_id_max\"]=  df[\"artist_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"artist_id_max_cat\"] = df[\"artist_id_max\"].astype('category')\n",
    "df[\"artist_id_max_cat\"] =  df[\"artist_id_max_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_max\"]=  df[\"composers_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"composers_id_max_cat\"] = df[\"composers_id_max\"].astype('category')\n",
    "df[\"composers_id_max_cat\"] =  df[\"composers_id_max_cat\"].cat.codes\n",
    "\n",
    "#New feature\n",
    "# df[\"group_album_artist_id_min_cat\"] = df.groupby([\"album\",\"artist_id_min_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_min_cat\"] = df[\"group_album_artist_id_min_cat\"].astype(\"category\").cat.codes\n",
    "# df[\"group_album_artist_id_max_cat\"] = df.groupby([\"album\",\"artist_id_max_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_max_cat\"] = df[\"group_album_artist_id_max_cat\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Fill genre\n",
    "print(\"There is {} ratio is nan genre\".format(len(df[df[\"genre\"].isnull()])/len(df)))\n",
    "df[\"genre\"]  = df[\"genre\"].fillna(\"No genre\")\n",
    "df[\"genre\"] = df[\"genre\"].astype('category')\n",
    "df[\"genre\"] =  df[\"genre\"].cat.codes\n",
    "\n",
    "# Fill album_artist\n",
    "print(\"There is {} ratio is nan album_artist\".format(len(df[df[\"album_artist\"].isnull()])/len(df)))\n",
    "df[\"album_artist\"]  = df[\"album_artist\"].fillna(\"No album_artist\")\n",
    "df[\"album_artist_contain_artistname\"]= [1 if r.album_artist in r.artist_name  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album_artist\"] = df[\"album_artist\"].astype('category')\n",
    "df[\"album_artist\"] =  df[\"album_artist\"].cat.codes\n",
    "\n",
    "# Fill track\n",
    "print(\"There is {} ratio is nan track\".format(len(df[df[\"track\"].isnull()])/len(df)))\n",
    "df[\"track\"]  = df[\"track\"].fillna(\"(1, 1)\")\n",
    "df[\"istrack11\"] = df[\"track\"] == \"(1, 1)\"\n",
    "def tracknum_to_value(track_num):\n",
    "    try:\n",
    "        \n",
    "        track_num = make_tuple(track_num)\n",
    "        if track_num[0] is not None:\n",
    "            return float(track_num[0]) / float(track_num[1])\n",
    "        else:\n",
    "            return 1.0\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "df[\"track\"] = df[\"track\"].apply(lambda t: tracknum_to_value(t))\n",
    "\n",
    "\n",
    "# Fill lyric\n",
    "print(\"There is {} ratio is nan lyric\".format(len(df[df[\"lyric\"].isnull()])/len(df)))\n",
    "df[\"lyric\"]  = df[\"lyric\"].fillna(\"\")\n",
    "df[\"islyric\"] = df[\"lyric\"].apply(lambda x:  True if len(x)  else False)\n",
    "df[\"num_line_lyric\"] = df[\"lyric\"].apply(lambda x : len(x.split(\"\\r\")))\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "from dateutil import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ast import literal_eval as make_tuple\n",
    "df['no_artist'] = df.artist_name.apply(lambda x: len(x.split(\",\")))\n",
    "df['no_composer'] = df.composers_name.apply(lambda x: len(x.split(\",\")))\n",
    "df[\"freq_artist\"] = df.groupby('artist_id')['artist_id'].transform('count').astype('float')\n",
    "df[\"freq_composer\"] = df.groupby('composers_id')['composers_id'].transform('count').astype('float')\n",
    "df[\"freq_artist_min\"] = df.groupby('artist_id_min_cat')['artist_id_min_cat'].transform('count').astype('float')\n",
    "df[\"freq_composer_min\"] = df.groupby('composers_id_min_cat')['composers_id_min_cat'].transform('count').astype('float')\n",
    "\n",
    "df[\"num_album_per_min_artist\"] = df.groupby(['artist_id_min_cat','album_right'])['album_right'].transform('count').astype('float')\n",
    "df[\"num_album_per_min_composer\"] = df.groupby(['composers_id_min','album_right'])['album_right'].transform('count').astype('float')\n",
    "\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df.release_time)\n",
    "df[\"year\"] = df[\"datetime\"].dt.year\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"day\"] = df[\"datetime\"].dt.day\n",
    "df[\"dayofyear\"] = df[\"datetime\"].dt.dayofyear\n",
    "df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "from datetime import date \n",
    "import holidays \n",
    "\n",
    "in_holidays = holidays.HolidayBase() \n",
    "for i in range(26,32):\n",
    "    in_holidays.append(str(i)+'-01-2017')\n",
    "in_holidays.append('01-02-2017')\n",
    "for i in range(14,21):\n",
    "    in_holidays.append(str(i)+'-02-2018')\n",
    "in_holidays.append('30-04-2017')\n",
    "in_holidays.append('30-04-2018')\n",
    "in_holidays.append('01-01-2017')\n",
    "in_holidays.append('01-01-2018')\n",
    "in_holidays.append('14-02-2017')\n",
    "in_holidays.append('14-02-2018')\n",
    "in_holidays.append('08-03-2017')\n",
    "in_holidays.append('08-03-2018')\n",
    "in_holidays.append('01-05-2017')\n",
    "in_holidays.append('01-05-2018')\n",
    "in_holidays.append('06-04-2017')\n",
    "in_holidays.append('25-04-2018')\n",
    "in_holidays.append('01-06-2017')\n",
    "in_holidays.append('01-06-2018')\n",
    "in_holidays.append('04-10-2017')\n",
    "in_holidays.append('24-09-2018')\n",
    "in_holidays.append('20-10-2017')\n",
    "in_holidays.append('20-10-2018')\n",
    "in_holidays.append('20-11-2017')\n",
    "in_holidays.append('20-11-2018')\n",
    "in_holidays.append('24-12-2017')\n",
    "in_holidays.append('24-12-2018')\n",
    "df['isHoliday'] = df.release_time.apply(lambda x: x in in_holidays)\n",
    "\n",
    "\n",
    "\n",
    "df[\"len_of_songname\"] = df[\"title\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemix\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isOST\"] = [ 1 if \"OST\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isBeat\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isVersion\"] = [ 1 if \"Version\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isCover\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isLienKhuc\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"title\"]]\n",
    "\n",
    "\n",
    "\n",
    "def find_num_song_release_in_final_month(df, day):\n",
    "    month5th = day + relativedelta.relativedelta(months=5)\n",
    "    month6th = day + relativedelta.relativedelta(months=6)  \n",
    "    return len(df.datetime[(df.datetime >= month5th)&(df.datetime<=month6th)])\n",
    "\n",
    "\n",
    "\n",
    "df[\"num_song_release_in_final_month\"] = df.datetime.apply(lambda d:find_num_song_release_in_final_month(df ,d))\n",
    "\n",
    "\n",
    "df[\"day_release\"] = df.groupby([\"year\",\"dayofyear\"]).ngroup().astype(\"category\").cat.codes\n",
    "df[\"numsongInAlbum\"] = df.groupby(\"album_right\")[\"album_right\"].transform(\"count\")\n",
    "df[\"isSingleAlbum_onesong\"]= df[\"isSingleAlbum\"] & (df[\"numsongInAlbum\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def split_id(s):\n",
    "    return re.split(',|\\.',s)\n",
    "\n",
    "m = df.artist_id.unique()\n",
    "idx_lst = []\n",
    "for idx in m:\n",
    "    ps = split_id(idx)\n",
    "    for i in ps:\n",
    "        idx_lst.append(i)\n",
    "        \n",
    "id_lst = list(set(idx_lst))\n",
    "\n",
    "def condition(df, artist_id):\n",
    "    r = df.artist_id.apply(lambda x: artist_id in split_id(x))\n",
    "    return r\n",
    "\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "data= [df_train[condition(df_train, artist_id)].label.agg([\"mean\",\"std\",\"count\"]) for artist_id in id_lst]\n",
    "new_df = pd.DataFrame(data=data)\n",
    "new_df[\"artist_id\"] =  id_lst\n",
    "\n",
    "new_df.dropna(inplace=True)\n",
    "new_df.set_index('artist_id', inplace=True)\n",
    "art_dict = new_df.to_dict()\n",
    "\n",
    "def best_mean_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_mean = 10\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['mean'][id] < temp_mean:\n",
    "                temp_mean = art_dict['mean'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_mean = temp_mean\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "\n",
    "df['artist_mean_id'] = df['artist_id'].apply(best_mean_id)\n",
    "\n",
    "def best_std_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_std = 10\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['std'][id] < temp_std:\n",
    "                temp_std = art_dict['std'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_std = temp_std\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "\n",
    "df['artist_std_id'] = df['artist_id'].apply(best_std_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artist_mean_id'] = df['artist_mean_id'].astype('category')\n",
    "df['artist_std_id'] = df['artist_std_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def split_id(s):\n",
    "    return re.split(',|\\.',s)\n",
    "\n",
    "m = df.composers_id.unique()\n",
    "idx_lst = []\n",
    "for idx in m:\n",
    "    ps = split_id(idx)\n",
    "    for i in ps:\n",
    "        idx_lst.append(i)\n",
    "        \n",
    "id_lst = list(set(idx_lst))\n",
    "\n",
    "def condition(df, composers_id):\n",
    "    r = df.composers_id.apply(lambda x: composers_id in split_id(x))\n",
    "    return r\n",
    "\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "data= [df_train[condition(df_train, composers_id)].label.agg([\"mean\",\"std\",\"count\"]) for composers_id in id_lst]\n",
    "new_df = pd.DataFrame(data=data)\n",
    "new_df[\"composers_id\"] =  id_lst\n",
    "\n",
    "new_df.dropna(inplace=True)\n",
    "new_df.set_index('composers_id', inplace=True)\n",
    "art_dict = new_df.to_dict()\n",
    "\n",
    "def best_mean_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_mean = 10\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['mean'][id] < temp_mean:\n",
    "                temp_mean = art_dict['mean'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_mean = temp_mean\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "\n",
    "df['composers_mean_id'] = df['composers_id'].apply(best_mean_id)\n",
    "\n",
    "def best_std_id(values):\n",
    "    ids = split_id(values)\n",
    "    temp_std = 10\n",
    "    temp_id = str(min([int(a) for a in ids]))\n",
    "    for id in ids:\n",
    "        try:\n",
    "            if art_dict['std'][id] < temp_std:\n",
    "                temp_std = art_dict['std'][id]\n",
    "                temp_id = id\n",
    "        except:\n",
    "            temp_std = temp_std\n",
    "            temp_id = temp_id\n",
    "    return temp_id\n",
    "\n",
    "df['composers_std_id'] = df['composers_id'].apply(best_std_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['composers_mean_id'] = df['composers_mean_id'].astype('category')\n",
    "df['composers_std_id'] = df['composers_std_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68195\tvalid_1's rmse: 1.90082\n",
      "[10000]\ttraining's rmse: 1.38307\tvalid_1's rmse: 1.746\n",
      "[15000]\ttraining's rmse: 1.21271\tvalid_1's rmse: 1.68178\n",
      "[20000]\ttraining's rmse: 1.09023\tvalid_1's rmse: 1.646\n",
      "[25000]\ttraining's rmse: 0.995794\tvalid_1's rmse: 1.62273\n",
      "[30000]\ttraining's rmse: 0.918967\tvalid_1's rmse: 1.6065\n",
      "[35000]\ttraining's rmse: 0.854658\tvalid_1's rmse: 1.59497\n",
      "[40000]\ttraining's rmse: 0.799866\tvalid_1's rmse: 1.5864\n",
      "[45000]\ttraining's rmse: 0.752221\tvalid_1's rmse: 1.5807\n",
      "[50000]\ttraining's rmse: 0.710267\tvalid_1's rmse: 1.57652\n",
      "[55000]\ttraining's rmse: 0.672134\tvalid_1's rmse: 1.57346\n",
      "[60000]\ttraining's rmse: 0.638147\tvalid_1's rmse: 1.57086\n",
      "[65000]\ttraining's rmse: 0.607576\tvalid_1's rmse: 1.56931\n",
      "[70000]\ttraining's rmse: 0.579324\tvalid_1's rmse: 1.56805\n",
      "[75000]\ttraining's rmse: 0.55365\tvalid_1's rmse: 1.56718\n",
      "[80000]\ttraining's rmse: 0.530083\tvalid_1's rmse: 1.56665\n",
      "[85000]\ttraining's rmse: 0.508069\tvalid_1's rmse: 1.56619\n",
      "[90000]\ttraining's rmse: 0.487776\tvalid_1's rmse: 1.56579\n",
      "[95000]\ttraining's rmse: 0.469118\tvalid_1's rmse: 1.56581\n",
      "[100000]\ttraining's rmse: 0.451755\tvalid_1's rmse: 1.5656\n",
      "[105000]\ttraining's rmse: 0.435497\tvalid_1's rmse: 1.56593\n",
      "[110000]\ttraining's rmse: 0.420264\tvalid_1's rmse: 1.56597\n",
      "[115000]\ttraining's rmse: 0.405813\tvalid_1's rmse: 1.56608\n",
      "Early stopping, best iteration is:\n",
      "[99683]\ttraining's rmse: 0.452844\tvalid_1's rmse: 1.56557\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68033\tvalid_1's rmse: 1.8785\n",
      "[10000]\ttraining's rmse: 1.37708\tvalid_1's rmse: 1.73856\n",
      "[15000]\ttraining's rmse: 1.20467\tvalid_1's rmse: 1.68672\n",
      "[20000]\ttraining's rmse: 1.0813\tvalid_1's rmse: 1.65867\n",
      "[25000]\ttraining's rmse: 0.987099\tvalid_1's rmse: 1.64136\n",
      "[30000]\ttraining's rmse: 0.910633\tvalid_1's rmse: 1.62966\n",
      "[35000]\ttraining's rmse: 0.846548\tvalid_1's rmse: 1.62128\n",
      "[40000]\ttraining's rmse: 0.792468\tvalid_1's rmse: 1.61551\n",
      "[45000]\ttraining's rmse: 0.74544\tvalid_1's rmse: 1.61053\n",
      "[50000]\ttraining's rmse: 0.703553\tvalid_1's rmse: 1.60702\n",
      "[55000]\ttraining's rmse: 0.665862\tvalid_1's rmse: 1.60455\n",
      "[60000]\ttraining's rmse: 0.63193\tvalid_1's rmse: 1.60257\n",
      "[65000]\ttraining's rmse: 0.601609\tvalid_1's rmse: 1.60089\n",
      "[70000]\ttraining's rmse: 0.57401\tvalid_1's rmse: 1.59963\n",
      "[75000]\ttraining's rmse: 0.548658\tvalid_1's rmse: 1.59834\n",
      "[80000]\ttraining's rmse: 0.525397\tvalid_1's rmse: 1.59735\n",
      "[85000]\ttraining's rmse: 0.503933\tvalid_1's rmse: 1.59652\n",
      "[90000]\ttraining's rmse: 0.483841\tvalid_1's rmse: 1.59585\n",
      "[95000]\ttraining's rmse: 0.465366\tvalid_1's rmse: 1.59546\n",
      "[100000]\ttraining's rmse: 0.448248\tvalid_1's rmse: 1.59487\n",
      "[105000]\ttraining's rmse: 0.432373\tvalid_1's rmse: 1.5946\n",
      "[110000]\ttraining's rmse: 0.417456\tvalid_1's rmse: 1.59427\n",
      "[115000]\ttraining's rmse: 0.403337\tvalid_1's rmse: 1.59399\n",
      "[120000]\ttraining's rmse: 0.390127\tvalid_1's rmse: 1.59373\n",
      "[125000]\ttraining's rmse: 0.377896\tvalid_1's rmse: 1.59362\n",
      "[130000]\ttraining's rmse: 0.36639\tvalid_1's rmse: 1.59352\n",
      "[135000]\ttraining's rmse: 0.355319\tvalid_1's rmse: 1.59359\n",
      "[140000]\ttraining's rmse: 0.344923\tvalid_1's rmse: 1.59372\n",
      "[145000]\ttraining's rmse: 0.335152\tvalid_1's rmse: 1.59379\n",
      "[150000]\ttraining's rmse: 0.325834\tvalid_1's rmse: 1.59387\n",
      "Early stopping, best iteration is:\n",
      "[132428]\ttraining's rmse: 0.361039\tvalid_1's rmse: 1.59346\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68819\tvalid_1's rmse: 1.87443\n",
      "[10000]\ttraining's rmse: 1.38677\tvalid_1's rmse: 1.70674\n",
      "[15000]\ttraining's rmse: 1.2148\tvalid_1's rmse: 1.63746\n",
      "[20000]\ttraining's rmse: 1.09144\tvalid_1's rmse: 1.59866\n",
      "[25000]\ttraining's rmse: 0.996245\tvalid_1's rmse: 1.57258\n",
      "[30000]\ttraining's rmse: 0.91891\tvalid_1's rmse: 1.55354\n",
      "[35000]\ttraining's rmse: 0.854145\tvalid_1's rmse: 1.5405\n",
      "[40000]\ttraining's rmse: 0.798591\tvalid_1's rmse: 1.53044\n",
      "[45000]\ttraining's rmse: 0.750793\tvalid_1's rmse: 1.52279\n",
      "[50000]\ttraining's rmse: 0.708686\tvalid_1's rmse: 1.51737\n",
      "[55000]\ttraining's rmse: 0.670776\tvalid_1's rmse: 1.51318\n",
      "[60000]\ttraining's rmse: 0.63672\tvalid_1's rmse: 1.51058\n",
      "[65000]\ttraining's rmse: 0.606244\tvalid_1's rmse: 1.50866\n",
      "[70000]\ttraining's rmse: 0.578223\tvalid_1's rmse: 1.50721\n",
      "[75000]\ttraining's rmse: 0.552554\tvalid_1's rmse: 1.50578\n",
      "[80000]\ttraining's rmse: 0.529303\tvalid_1's rmse: 1.50474\n",
      "[85000]\ttraining's rmse: 0.507442\tvalid_1's rmse: 1.50446\n",
      "[90000]\ttraining's rmse: 0.487287\tvalid_1's rmse: 1.50409\n",
      "[95000]\ttraining's rmse: 0.468533\tvalid_1's rmse: 1.50399\n",
      "[100000]\ttraining's rmse: 0.451284\tvalid_1's rmse: 1.50397\n",
      "[105000]\ttraining's rmse: 0.435145\tvalid_1's rmse: 1.50417\n",
      "[110000]\ttraining's rmse: 0.42001\tvalid_1's rmse: 1.50445\n",
      "[115000]\ttraining's rmse: 0.40573\tvalid_1's rmse: 1.5046\n",
      "Early stopping, best iteration is:\n",
      "[97537]\ttraining's rmse: 0.459747\tvalid_1's rmse: 1.50373\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68174\tvalid_1's rmse: 1.932\n",
      "[10000]\ttraining's rmse: 1.38182\tvalid_1's rmse: 1.76395\n",
      "[15000]\ttraining's rmse: 1.21097\tvalid_1's rmse: 1.69606\n",
      "[20000]\ttraining's rmse: 1.08781\tvalid_1's rmse: 1.65848\n",
      "[25000]\ttraining's rmse: 0.992824\tvalid_1's rmse: 1.63429\n",
      "[30000]\ttraining's rmse: 0.915904\tvalid_1's rmse: 1.61823\n",
      "[35000]\ttraining's rmse: 0.851115\tvalid_1's rmse: 1.60696\n",
      "[40000]\ttraining's rmse: 0.796361\tvalid_1's rmse: 1.59962\n",
      "[45000]\ttraining's rmse: 0.748117\tvalid_1's rmse: 1.5946\n",
      "[50000]\ttraining's rmse: 0.706329\tvalid_1's rmse: 1.5913\n",
      "[55000]\ttraining's rmse: 0.668288\tvalid_1's rmse: 1.58904\n",
      "[60000]\ttraining's rmse: 0.634348\tvalid_1's rmse: 1.58793\n",
      "[65000]\ttraining's rmse: 0.603638\tvalid_1's rmse: 1.58716\n",
      "[70000]\ttraining's rmse: 0.57544\tvalid_1's rmse: 1.58679\n",
      "[75000]\ttraining's rmse: 0.54956\tvalid_1's rmse: 1.58654\n",
      "[80000]\ttraining's rmse: 0.526103\tvalid_1's rmse: 1.58651\n",
      "[85000]\ttraining's rmse: 0.504272\tvalid_1's rmse: 1.58686\n",
      "[90000]\ttraining's rmse: 0.484142\tvalid_1's rmse: 1.58719\n",
      "[95000]\ttraining's rmse: 0.465411\tvalid_1's rmse: 1.58761\n",
      "Early stopping, best iteration is:\n",
      "[77877]\ttraining's rmse: 0.535945\tvalid_1's rmse: 1.58641\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.67927\tvalid_1's rmse: 1.93292\n",
      "[10000]\ttraining's rmse: 1.37834\tvalid_1's rmse: 1.77205\n",
      "[15000]\ttraining's rmse: 1.20818\tvalid_1's rmse: 1.70855\n",
      "[20000]\ttraining's rmse: 1.08627\tvalid_1's rmse: 1.67239\n",
      "[25000]\ttraining's rmse: 0.992011\tvalid_1's rmse: 1.64872\n",
      "[30000]\ttraining's rmse: 0.915321\tvalid_1's rmse: 1.63206\n",
      "[35000]\ttraining's rmse: 0.850671\tvalid_1's rmse: 1.61981\n",
      "[40000]\ttraining's rmse: 0.795484\tvalid_1's rmse: 1.61088\n",
      "[45000]\ttraining's rmse: 0.747536\tvalid_1's rmse: 1.60389\n",
      "[50000]\ttraining's rmse: 0.705507\tvalid_1's rmse: 1.59861\n",
      "[55000]\ttraining's rmse: 0.667519\tvalid_1's rmse: 1.5946\n",
      "[60000]\ttraining's rmse: 0.633319\tvalid_1's rmse: 1.59148\n",
      "[65000]\ttraining's rmse: 0.603029\tvalid_1's rmse: 1.5892\n",
      "[70000]\ttraining's rmse: 0.574872\tvalid_1's rmse: 1.58707\n",
      "[75000]\ttraining's rmse: 0.549087\tvalid_1's rmse: 1.58561\n",
      "[80000]\ttraining's rmse: 0.525523\tvalid_1's rmse: 1.58422\n",
      "[85000]\ttraining's rmse: 0.503571\tvalid_1's rmse: 1.58301\n",
      "[90000]\ttraining's rmse: 0.483464\tvalid_1's rmse: 1.58225\n",
      "[95000]\ttraining's rmse: 0.464622\tvalid_1's rmse: 1.58188\n",
      "[100000]\ttraining's rmse: 0.44723\tvalid_1's rmse: 1.58148\n",
      "[105000]\ttraining's rmse: 0.431149\tvalid_1's rmse: 1.5812\n",
      "[110000]\ttraining's rmse: 0.416034\tvalid_1's rmse: 1.58092\n",
      "[115000]\ttraining's rmse: 0.401691\tvalid_1's rmse: 1.58084\n",
      "[120000]\ttraining's rmse: 0.388204\tvalid_1's rmse: 1.58052\n",
      "[125000]\ttraining's rmse: 0.375904\tvalid_1's rmse: 1.5806\n",
      "[130000]\ttraining's rmse: 0.364382\tvalid_1's rmse: 1.58072\n",
      "[135000]\ttraining's rmse: 0.35326\tvalid_1's rmse: 1.58081\n",
      "[140000]\ttraining's rmse: 0.342742\tvalid_1's rmse: 1.58096\n",
      "Early stopping, best iteration is:\n",
      "[121544]\ttraining's rmse: 0.384331\tvalid_1's rmse: 1.58046\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68016\tvalid_1's rmse: 1.93971\n",
      "[10000]\ttraining's rmse: 1.37925\tvalid_1's rmse: 1.77902\n",
      "[15000]\ttraining's rmse: 1.20767\tvalid_1's rmse: 1.71269\n",
      "[20000]\ttraining's rmse: 1.08446\tvalid_1's rmse: 1.67516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25000]\ttraining's rmse: 0.989825\tvalid_1's rmse: 1.65173\n",
      "[30000]\ttraining's rmse: 0.913282\tvalid_1's rmse: 1.63474\n",
      "[35000]\ttraining's rmse: 0.848946\tvalid_1's rmse: 1.62251\n",
      "[40000]\ttraining's rmse: 0.794152\tvalid_1's rmse: 1.61473\n",
      "[45000]\ttraining's rmse: 0.746196\tvalid_1's rmse: 1.60825\n",
      "[50000]\ttraining's rmse: 0.704843\tvalid_1's rmse: 1.6038\n",
      "[55000]\ttraining's rmse: 0.666893\tvalid_1's rmse: 1.6003\n",
      "[60000]\ttraining's rmse: 0.632944\tvalid_1's rmse: 1.59754\n",
      "[65000]\ttraining's rmse: 0.602801\tvalid_1's rmse: 1.59524\n",
      "[70000]\ttraining's rmse: 0.574691\tvalid_1's rmse: 1.59369\n",
      "[75000]\ttraining's rmse: 0.549277\tvalid_1's rmse: 1.59226\n",
      "[80000]\ttraining's rmse: 0.525795\tvalid_1's rmse: 1.59127\n",
      "[85000]\ttraining's rmse: 0.504113\tvalid_1's rmse: 1.59067\n",
      "[90000]\ttraining's rmse: 0.484007\tvalid_1's rmse: 1.58994\n",
      "[95000]\ttraining's rmse: 0.465341\tvalid_1's rmse: 1.58949\n",
      "[100000]\ttraining's rmse: 0.448195\tvalid_1's rmse: 1.58923\n",
      "[105000]\ttraining's rmse: 0.432036\tvalid_1's rmse: 1.58877\n",
      "[110000]\ttraining's rmse: 0.416854\tvalid_1's rmse: 1.58851\n",
      "[115000]\ttraining's rmse: 0.402373\tvalid_1's rmse: 1.58846\n",
      "[120000]\ttraining's rmse: 0.388918\tvalid_1's rmse: 1.58837\n",
      "[125000]\ttraining's rmse: 0.37643\tvalid_1's rmse: 1.58835\n",
      "[130000]\ttraining's rmse: 0.364674\tvalid_1's rmse: 1.58831\n",
      "[135000]\ttraining's rmse: 0.35338\tvalid_1's rmse: 1.58858\n",
      "[140000]\ttraining's rmse: 0.342792\tvalid_1's rmse: 1.58873\n",
      "[145000]\ttraining's rmse: 0.33281\tvalid_1's rmse: 1.58878\n",
      "Early stopping, best iteration is:\n",
      "[128350]\ttraining's rmse: 0.368584\tvalid_1's rmse: 1.58822\n",
      "Fold 6\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68788\tvalid_1's rmse: 1.9142\n",
      "[10000]\ttraining's rmse: 1.38861\tvalid_1's rmse: 1.75329\n",
      "[15000]\ttraining's rmse: 1.21757\tvalid_1's rmse: 1.68499\n",
      "[20000]\ttraining's rmse: 1.09349\tvalid_1's rmse: 1.6451\n",
      "[25000]\ttraining's rmse: 0.998\tvalid_1's rmse: 1.61904\n",
      "[30000]\ttraining's rmse: 0.92044\tvalid_1's rmse: 1.60195\n",
      "[35000]\ttraining's rmse: 0.854723\tvalid_1's rmse: 1.58958\n",
      "[40000]\ttraining's rmse: 0.799247\tvalid_1's rmse: 1.58058\n",
      "[45000]\ttraining's rmse: 0.750943\tvalid_1's rmse: 1.57338\n",
      "[50000]\ttraining's rmse: 0.708454\tvalid_1's rmse: 1.56812\n",
      "[55000]\ttraining's rmse: 0.669949\tvalid_1's rmse: 1.56408\n",
      "[60000]\ttraining's rmse: 0.635372\tvalid_1's rmse: 1.56089\n",
      "[65000]\ttraining's rmse: 0.604257\tvalid_1's rmse: 1.55868\n",
      "[70000]\ttraining's rmse: 0.575525\tvalid_1's rmse: 1.55708\n",
      "[75000]\ttraining's rmse: 0.549143\tvalid_1's rmse: 1.5559\n",
      "[80000]\ttraining's rmse: 0.525251\tvalid_1's rmse: 1.55506\n",
      "[85000]\ttraining's rmse: 0.503034\tvalid_1's rmse: 1.55431\n",
      "[90000]\ttraining's rmse: 0.482532\tvalid_1's rmse: 1.55406\n",
      "[95000]\ttraining's rmse: 0.463528\tvalid_1's rmse: 1.55374\n",
      "[100000]\ttraining's rmse: 0.445908\tvalid_1's rmse: 1.55383\n",
      "[105000]\ttraining's rmse: 0.429298\tvalid_1's rmse: 1.55383\n",
      "[110000]\ttraining's rmse: 0.413881\tvalid_1's rmse: 1.55391\n",
      "[115000]\ttraining's rmse: 0.399283\tvalid_1's rmse: 1.55408\n",
      "Early stopping, best iteration is:\n",
      "[98060]\ttraining's rmse: 0.452695\tvalid_1's rmse: 1.55357\n",
      "Fold 7\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.67898\tvalid_1's rmse: 1.92884\n",
      "[10000]\ttraining's rmse: 1.37923\tvalid_1's rmse: 1.77594\n",
      "[15000]\ttraining's rmse: 1.20907\tvalid_1's rmse: 1.71356\n",
      "[20000]\ttraining's rmse: 1.08714\tvalid_1's rmse: 1.67688\n",
      "[25000]\ttraining's rmse: 0.992834\tvalid_1's rmse: 1.65226\n",
      "[30000]\ttraining's rmse: 0.915857\tvalid_1's rmse: 1.63528\n",
      "[35000]\ttraining's rmse: 0.850302\tvalid_1's rmse: 1.62244\n",
      "[40000]\ttraining's rmse: 0.79453\tvalid_1's rmse: 1.61324\n",
      "[45000]\ttraining's rmse: 0.746186\tvalid_1's rmse: 1.60611\n",
      "[50000]\ttraining's rmse: 0.703614\tvalid_1's rmse: 1.60086\n",
      "[55000]\ttraining's rmse: 0.665133\tvalid_1's rmse: 1.59688\n",
      "[60000]\ttraining's rmse: 0.630552\tvalid_1's rmse: 1.59352\n",
      "[65000]\ttraining's rmse: 0.599435\tvalid_1's rmse: 1.59089\n",
      "[70000]\ttraining's rmse: 0.570785\tvalid_1's rmse: 1.58916\n",
      "[75000]\ttraining's rmse: 0.544462\tvalid_1's rmse: 1.5877\n",
      "[80000]\ttraining's rmse: 0.520585\tvalid_1's rmse: 1.58705\n",
      "[85000]\ttraining's rmse: 0.498281\tvalid_1's rmse: 1.58646\n",
      "[90000]\ttraining's rmse: 0.477721\tvalid_1's rmse: 1.58604\n",
      "[95000]\ttraining's rmse: 0.458515\tvalid_1's rmse: 1.58558\n",
      "[100000]\ttraining's rmse: 0.440966\tvalid_1's rmse: 1.58558\n",
      "[105000]\ttraining's rmse: 0.424472\tvalid_1's rmse: 1.58566\n",
      "[110000]\ttraining's rmse: 0.409062\tvalid_1's rmse: 1.58583\n",
      "[115000]\ttraining's rmse: 0.39444\tvalid_1's rmse: 1.58592\n",
      "Early stopping, best iteration is:\n",
      "[98109]\ttraining's rmse: 0.447663\tvalid_1's rmse: 1.58543\n",
      "Fold 8\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.6824\tvalid_1's rmse: 1.91536\n",
      "[10000]\ttraining's rmse: 1.37839\tvalid_1's rmse: 1.76858\n",
      "[15000]\ttraining's rmse: 1.20576\tvalid_1's rmse: 1.71002\n",
      "[20000]\ttraining's rmse: 1.08221\tvalid_1's rmse: 1.67774\n",
      "[25000]\ttraining's rmse: 0.98674\tvalid_1's rmse: 1.65707\n",
      "[30000]\ttraining's rmse: 0.909533\tvalid_1's rmse: 1.64235\n",
      "[35000]\ttraining's rmse: 0.844882\tvalid_1's rmse: 1.63259\n",
      "[40000]\ttraining's rmse: 0.78991\tvalid_1's rmse: 1.62509\n",
      "[45000]\ttraining's rmse: 0.742143\tvalid_1's rmse: 1.61944\n",
      "[50000]\ttraining's rmse: 0.700191\tvalid_1's rmse: 1.61516\n",
      "[55000]\ttraining's rmse: 0.662451\tvalid_1's rmse: 1.61204\n",
      "[60000]\ttraining's rmse: 0.628425\tvalid_1's rmse: 1.60969\n",
      "[65000]\ttraining's rmse: 0.598057\tvalid_1's rmse: 1.60784\n",
      "[70000]\ttraining's rmse: 0.570187\tvalid_1's rmse: 1.60678\n",
      "[75000]\ttraining's rmse: 0.544614\tvalid_1's rmse: 1.60573\n",
      "[80000]\ttraining's rmse: 0.521185\tvalid_1's rmse: 1.60477\n",
      "[85000]\ttraining's rmse: 0.499564\tvalid_1's rmse: 1.60436\n",
      "[90000]\ttraining's rmse: 0.479568\tvalid_1's rmse: 1.6041\n",
      "[95000]\ttraining's rmse: 0.460904\tvalid_1's rmse: 1.60396\n",
      "[100000]\ttraining's rmse: 0.443762\tvalid_1's rmse: 1.60413\n",
      "[105000]\ttraining's rmse: 0.427644\tvalid_1's rmse: 1.60423\n",
      "[110000]\ttraining's rmse: 0.412653\tvalid_1's rmse: 1.60432\n",
      "[115000]\ttraining's rmse: 0.398335\tvalid_1's rmse: 1.60456\n",
      "Early stopping, best iteration is:\n",
      "[95090]\ttraining's rmse: 0.460596\tvalid_1's rmse: 1.60392\n",
      "Fold 9\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.68482\tvalid_1's rmse: 1.89809\n",
      "[10000]\ttraining's rmse: 1.38309\tvalid_1's rmse: 1.72787\n",
      "[15000]\ttraining's rmse: 1.211\tvalid_1's rmse: 1.65633\n",
      "[20000]\ttraining's rmse: 1.08781\tvalid_1's rmse: 1.61791\n",
      "[25000]\ttraining's rmse: 0.992894\tvalid_1's rmse: 1.5924\n",
      "[30000]\ttraining's rmse: 0.915932\tvalid_1's rmse: 1.57573\n",
      "[35000]\ttraining's rmse: 0.851215\tvalid_1's rmse: 1.56385\n",
      "[40000]\ttraining's rmse: 0.79623\tvalid_1's rmse: 1.55524\n",
      "[45000]\ttraining's rmse: 0.748036\tvalid_1's rmse: 1.54849\n",
      "[50000]\ttraining's rmse: 0.705893\tvalid_1's rmse: 1.54373\n",
      "[55000]\ttraining's rmse: 0.668063\tvalid_1's rmse: 1.54049\n",
      "[60000]\ttraining's rmse: 0.633906\tvalid_1's rmse: 1.53784\n",
      "[65000]\ttraining's rmse: 0.603282\tvalid_1's rmse: 1.53595\n",
      "[70000]\ttraining's rmse: 0.574799\tvalid_1's rmse: 1.53467\n",
      "[75000]\ttraining's rmse: 0.548929\tvalid_1's rmse: 1.53389\n",
      "[80000]\ttraining's rmse: 0.525205\tvalid_1's rmse: 1.53357\n",
      "[85000]\ttraining's rmse: 0.50328\tvalid_1's rmse: 1.53334\n",
      "[90000]\ttraining's rmse: 0.482935\tvalid_1's rmse: 1.53327\n",
      "[95000]\ttraining's rmse: 0.464152\tvalid_1's rmse: 1.53351\n",
      "[100000]\ttraining's rmse: 0.446901\tvalid_1's rmse: 1.53367\n",
      "[105000]\ttraining's rmse: 0.430569\tvalid_1's rmse: 1.5339\n",
      "Early stopping, best iteration is:\n",
      "[88065]\ttraining's rmse: 0.490679\tvalid_1's rmse: 1.53308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "chosen_features = [ \"album_right\", \"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "                   \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "                  \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "                  \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "                  \"isBeatAlbum\", \"isCoverAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "                  \"artist_id_min_cat\",  \"artist_id_max_cat\",\"composers_id_min_cat\",\"composers_id_max_cat\",\n",
    "                   \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "                  \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "                   \"numsongInAlbum\",\"isSingleAlbum_onesong\",\"artist_mean_id\", \"artist_std_id\"  ]\n",
    "\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "df_test = df[df.dataset==\"test\"]\n",
    "\n",
    "param = {\n",
    "    'bagging_freq': 20,          \n",
    "    'bagging_fraction': 0.95,   'boost_from_average':'false',   \n",
    "    'boost': 'gbdt',             'feature_fraction': 0.1,     'learning_rate': 0.001,\n",
    "    'max_depth': -1,             'metric':'root_mean_squared_error', 'min_data_in_leaf': 5,   \n",
    "       'num_leaves': 50,            \n",
    "    'num_threads': 8,              'tree_learner': 'serial',   'objective': 'regression',\n",
    "    'reg_alpha': 0.1002650970728192, 'reg_lambda': 0.1003427518866501,'verbosity': 1,\n",
    "    \"seed\": 99999\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "labels= df_train.label\n",
    "# fig, axes = plt.subplots(5, 1, figsize=(10, 10*5))\n",
    "# axes = axes.flat\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][chosen_features], label=labels.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][chosen_features], label=labels.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 20000)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][chosen_features], num_iteration=clf.best_iteration)\n",
    "    predictions += clf.predict(df_test[chosen_features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.56966 \n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "print(\"RMSE: {:<8.5f}\".format(sqrt(mean_squared_error(df_train.label, oof))))\n",
    "\n",
    "sub = pd.DataFrame({\"ID\": df_test.ID.values})\n",
    "sub[\"label\"] = predictions\n",
    "sub.to_csv(\"submission_lightgbm.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
