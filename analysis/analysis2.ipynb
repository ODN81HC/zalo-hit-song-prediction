{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "TRAININFO = \"/data/HitSongPrediction/train_info.tsv\"\n",
    "TRAINRANK =  \"/data/HitSongPrediction/train_rank.csv\"\n",
    "TESTINFO = \"/data/HitSongPrediction/test_info.tsv\"\n",
    "SUBMISSION = \"/data/HitSongPrediction/submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "a = cv2.imread(\"/data/zalo/hit-song-prediction/train/1076417366.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>composers_name</th>\n",
       "      <th>composers_id</th>\n",
       "      <th>release_time</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.tuning_frequency</th>\n",
       "      <th>tonal.tuning_nontempered_energy_ratio</th>\n",
       "      <th>tonal.chords_key</th>\n",
       "      <th>tonal.chords_scale</th>\n",
       "      <th>tonal.key_edma.key</th>\n",
       "      <th>tonal.key_edma.scale</th>\n",
       "      <th>tonal.key_krumhansl.key</th>\n",
       "      <th>tonal.key_krumhansl.scale</th>\n",
       "      <th>tonal.key_temperley.key</th>\n",
       "      <th>tonal.key_temperley.scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073748245</td>\n",
       "      <td>Đêm Chôn Dầu Vượt Biển</td>\n",
       "      <td>Như Quỳnh</td>\n",
       "      <td>551</td>\n",
       "      <td>Châu Đình An</td>\n",
       "      <td>5765</td>\n",
       "      <td>2017-10-01 22:07:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.601478</td>\n",
       "      <td>D</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1073751978</td>\n",
       "      <td>Mùa Thu Trong Mưa</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Trường Sa</td>\n",
       "      <td>100105</td>\n",
       "      <td>2017-10-01 20:58:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.944516</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1073835561</td>\n",
       "      <td>Rồi Ánh Trăng Tan</td>\n",
       "      <td>Lưu Bích</td>\n",
       "      <td>450</td>\n",
       "      <td>Quốc Bảo</td>\n",
       "      <td>4355</td>\n",
       "      <td>2017-11-01 18:16:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.957651</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1073856553</td>\n",
       "      <td>Còn Thương Rau Đắng Mọc Sau Hè</td>\n",
       "      <td>Như Quỳnh</td>\n",
       "      <td>551</td>\n",
       "      <td>Bắc Sơn</td>\n",
       "      <td>7686</td>\n",
       "      <td>2017-11-01 17:36:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>441.272583</td>\n",
       "      <td>0.796499</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1073929630</td>\n",
       "      <td>Người Điên Biết Yêu</td>\n",
       "      <td>Như Loan</td>\n",
       "      <td>513</td>\n",
       "      <td>Lê Minh Kha</td>\n",
       "      <td>100466</td>\n",
       "      <td>2017-11-01 17:49:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.946167</td>\n",
       "      <td>0.860068</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1073929880</td>\n",
       "      <td>Đàn Bà</td>\n",
       "      <td>Don Hồ</td>\n",
       "      <td>6515</td>\n",
       "      <td>Song Ngọc</td>\n",
       "      <td>100288</td>\n",
       "      <td>2017-10-01 21:33:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>436.960693</td>\n",
       "      <td>0.768609</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1073949310</td>\n",
       "      <td>Yêu Một Người Sống Bên Một Người</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Hoài An</td>\n",
       "      <td>100133</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>442.548920</td>\n",
       "      <td>0.701749</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9078</td>\n",
       "      <td>1073994292</td>\n",
       "      <td>Giấc Mơ Mình Em</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Minh Vy</td>\n",
       "      <td>100019</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>...</td>\n",
       "      <td>441.782684</td>\n",
       "      <td>0.659616</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "      <td>Bb</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1073994297</td>\n",
       "      <td>Mất Nhau Trong Đời</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Huỳnh Nhật Tân</td>\n",
       "      <td>100306</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.913990</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "      <td>C</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1073994298</td>\n",
       "      <td>Những Ân Tình Xưa</td>\n",
       "      <td>Minh Tuyết, Bằng Kiều</td>\n",
       "      <td>455.306</td>\n",
       "      <td>Hoài An</td>\n",
       "      <td>100133</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>437.971466</td>\n",
       "      <td>0.726335</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1073994300</td>\n",
       "      <td>Ở Nơi Đó Em Cười</td>\n",
       "      <td>Minh Tuyết</td>\n",
       "      <td>455</td>\n",
       "      <td>Hoài An</td>\n",
       "      <td>100133</td>\n",
       "      <td>2017-11-01 18:27:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>437.718536</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1074011942</td>\n",
       "      <td>Tình Chấp Nhận</td>\n",
       "      <td>Quỳnh Vi</td>\n",
       "      <td>7928</td>\n",
       "      <td>Trần Đức</td>\n",
       "      <td>7898</td>\n",
       "      <td>2017-11-01 18:49:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.763123</td>\n",
       "      <td>0.655899</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1074239565</td>\n",
       "      <td>Rồi Mai Tôi Đưa Em</td>\n",
       "      <td>Trần Thái Hòa</td>\n",
       "      <td>901</td>\n",
       "      <td>Trường Sa</td>\n",
       "      <td>100105</td>\n",
       "      <td>2017-10-01 20:58:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.508636</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "      <td>D</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1074245455</td>\n",
       "      <td>Thiên Đàng Đánh Mất</td>\n",
       "      <td>Dương Triệu Vũ</td>\n",
       "      <td>5072</td>\n",
       "      <td>Nguyễn Hồng Thuận</td>\n",
       "      <td>9140</td>\n",
       "      <td>2017-11-01 18:42:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>443.316437</td>\n",
       "      <td>0.793990</td>\n",
       "      <td>G</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>major</td>\n",
       "      <td>A</td>\n",
       "      <td>major</td>\n",
       "      <td>F</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1074245511</td>\n",
       "      <td>Vai Phụ</td>\n",
       "      <td>Loan Châu</td>\n",
       "      <td>827</td>\n",
       "      <td>Phạm Khải Tuấn</td>\n",
       "      <td>100116</td>\n",
       "      <td>2017-10-01 21:22:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.983561</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>1074247432</td>\n",
       "      <td>Tình Nhỏ Mau Quên</td>\n",
       "      <td>Quang Lê, Hương Thủy</td>\n",
       "      <td>828.87</td>\n",
       "      <td>Hàn Châu</td>\n",
       "      <td>100406</td>\n",
       "      <td>2017-10-01 22:32:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>441.272583</td>\n",
       "      <td>0.738260</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "      <td>Eb</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>9079</td>\n",
       "      <td>1074250503</td>\n",
       "      <td>Rồi 30 Năm Qua</td>\n",
       "      <td>Tâm Đoan</td>\n",
       "      <td>518</td>\n",
       "      <td>Nhật Ngân</td>\n",
       "      <td>100218</td>\n",
       "      <td>2017-10-01 22:07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>...</td>\n",
       "      <td>440.508636</td>\n",
       "      <td>0.702865</td>\n",
       "      <td>F#</td>\n",
       "      <td>minor</td>\n",
       "      <td>B</td>\n",
       "      <td>major</td>\n",
       "      <td>B</td>\n",
       "      <td>major</td>\n",
       "      <td>B</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>1074254689</td>\n",
       "      <td>Chơi Vơi Dòng Đời</td>\n",
       "      <td>Duy Linh</td>\n",
       "      <td>918</td>\n",
       "      <td>Huy Liêu</td>\n",
       "      <td>430572</td>\n",
       "      <td>2017-11-07 23:40:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.961273</td>\n",
       "      <td>D</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>1074254690</td>\n",
       "      <td>Dòng Sông Quê Hương</td>\n",
       "      <td>Lan Phương</td>\n",
       "      <td>6517</td>\n",
       "      <td>Huy Liêu</td>\n",
       "      <td>430572</td>\n",
       "      <td>2017-11-07 23:43:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.676015</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "      <td>C#</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>9080</td>\n",
       "      <td>1074254709</td>\n",
       "      <td>Hòa Bình Hoan Ca</td>\n",
       "      <td>Hùng Phú, Duy Linh</td>\n",
       "      <td>18241.918</td>\n",
       "      <td>Huy Liêu</td>\n",
       "      <td>430572</td>\n",
       "      <td>2017-11-07 23:47:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>...</td>\n",
       "      <td>436.960693</td>\n",
       "      <td>0.790986</td>\n",
       "      <td>A</td>\n",
       "      <td>minor</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          ID                             title  \\\n",
       "0       0  1073748245            Đêm Chôn Dầu Vượt Biển   \n",
       "1       1  1073751978                 Mùa Thu Trong Mưa   \n",
       "2       2  1073835561                 Rồi Ánh Trăng Tan   \n",
       "3       3  1073856553    Còn Thương Rau Đắng Mọc Sau Hè   \n",
       "4       4  1073929630               Người Điên Biết Yêu   \n",
       "5       5  1073929880                            Đàn Bà   \n",
       "6       6  1073949310  Yêu Một Người Sống Bên Một Người   \n",
       "7    9078  1073994292                   Giấc Mơ Mình Em   \n",
       "8       7  1073994297                Mất Nhau Trong Đời   \n",
       "9       8  1073994298                 Những Ân Tình Xưa   \n",
       "10      9  1073994300                  Ở Nơi Đó Em Cười   \n",
       "11     10  1074011942                    Tình Chấp Nhận   \n",
       "12     11  1074239565                Rồi Mai Tôi Đưa Em   \n",
       "13     12  1074245455               Thiên Đàng Đánh Mất   \n",
       "14     13  1074245511                           Vai Phụ   \n",
       "15     14  1074247432                 Tình Nhỏ Mau Quên   \n",
       "16   9079  1074250503                    Rồi 30 Năm Qua   \n",
       "17     15  1074254689                 Chơi Vơi Dòng Đời   \n",
       "18     16  1074254690               Dòng Sông Quê Hương   \n",
       "19   9080  1074254709                  Hòa Bình Hoan Ca   \n",
       "\n",
       "              artist_name  artist_id     composers_name composers_id  \\\n",
       "0               Như Quỳnh        551       Châu Đình An         5765   \n",
       "1              Minh Tuyết        455          Trường Sa       100105   \n",
       "2                Lưu Bích        450           Quốc Bảo         4355   \n",
       "3               Như Quỳnh        551            Bắc Sơn         7686   \n",
       "4                Như Loan        513        Lê Minh Kha       100466   \n",
       "5                  Don Hồ       6515          Song Ngọc       100288   \n",
       "6              Minh Tuyết        455            Hoài An       100133   \n",
       "7              Minh Tuyết        455            Minh Vy       100019   \n",
       "8              Minh Tuyết        455     Huỳnh Nhật Tân       100306   \n",
       "9   Minh Tuyết, Bằng Kiều    455.306            Hoài An       100133   \n",
       "10             Minh Tuyết        455            Hoài An       100133   \n",
       "11               Quỳnh Vi       7928           Trần Đức         7898   \n",
       "12          Trần Thái Hòa        901          Trường Sa       100105   \n",
       "13         Dương Triệu Vũ       5072  Nguyễn Hồng Thuận         9140   \n",
       "14              Loan Châu        827     Phạm Khải Tuấn       100116   \n",
       "15   Quang Lê, Hương Thủy     828.87           Hàn Châu       100406   \n",
       "16               Tâm Đoan        518          Nhật Ngân       100218   \n",
       "17               Duy Linh        918           Huy Liêu       430572   \n",
       "18             Lan Phương       6517           Huy Liêu       430572   \n",
       "19     Hùng Phú, Duy Linh  18241.918           Huy Liêu       430572   \n",
       "\n",
       "           release_time  label dataset  ...  tonal.tuning_frequency  \\\n",
       "0   2017-10-01 22:07:00    7.0   train  ...              440.000000   \n",
       "1   2017-10-01 20:58:00    3.0   train  ...              434.193115   \n",
       "2   2017-11-01 18:16:00    6.0   train  ...              434.193115   \n",
       "3   2017-11-01 17:36:00    2.0   train  ...              441.272583   \n",
       "4   2017-11-01 17:49:00    7.0   train  ...              434.946167   \n",
       "5   2017-10-01 21:33:00    3.0   train  ...              436.960693   \n",
       "6   2017-11-01 18:27:00    4.0   train  ...              442.548920   \n",
       "7   2017-11-01 18:27:00    NaN    test  ...              441.782684   \n",
       "8   2017-11-01 18:27:00    8.0   train  ...              434.193115   \n",
       "9   2017-11-01 18:27:00    4.0   train  ...              437.971466   \n",
       "10  2017-11-01 18:27:00    8.0   train  ...              437.718536   \n",
       "11  2017-11-01 18:49:00    5.0   train  ...              440.763123   \n",
       "12  2017-10-01 20:58:00    3.0   train  ...              440.508636   \n",
       "13  2017-11-01 18:42:00    9.0   train  ...              443.316437   \n",
       "14  2017-10-01 21:22:00    6.0   train  ...              434.193115   \n",
       "15  2017-10-01 22:32:00    1.0   train  ...              441.272583   \n",
       "16  2017-10-01 22:07:00    NaN    test  ...              440.508636   \n",
       "17  2017-11-07 23:40:00    9.0   train  ...              434.193115   \n",
       "18  2017-11-07 23:43:00    8.0   train  ...              440.000000   \n",
       "19  2017-11-07 23:47:00    NaN    test  ...              436.960693   \n",
       "\n",
       "   tonal.tuning_nontempered_energy_ratio tonal.chords_key tonal.chords_scale  \\\n",
       "0                               0.601478                D              major   \n",
       "1                               0.944516                C              minor   \n",
       "2                               0.957651               Bb              major   \n",
       "3                               0.796499                G              minor   \n",
       "4                               0.860068                A              minor   \n",
       "5                               0.768609                A              minor   \n",
       "6                               0.701749                D              minor   \n",
       "7                               0.659616               Bb              major   \n",
       "8                               0.913990                C              minor   \n",
       "9                               0.726335                A              minor   \n",
       "10                              0.732484                A              minor   \n",
       "11                              0.655899                A              minor   \n",
       "12                              0.673602                D              minor   \n",
       "13                              0.793990                G              minor   \n",
       "14                              0.983561                A              minor   \n",
       "15                              0.738260               Eb              minor   \n",
       "16                              0.702865               F#              minor   \n",
       "17                              0.961273                D              major   \n",
       "18                              0.676015               C#              major   \n",
       "19                              0.790986                A              minor   \n",
       "\n",
       "   tonal.key_edma.key tonal.key_edma.scale  tonal.key_krumhansl.key  \\\n",
       "0                   G                major                        G   \n",
       "1                   C                minor                        C   \n",
       "2                   D                minor                        D   \n",
       "3                   G                minor                        G   \n",
       "4                   D                minor                        D   \n",
       "5                   D                minor                        A   \n",
       "6                   A                minor                        A   \n",
       "7                  Bb                major                       Bb   \n",
       "8                   C                minor                        C   \n",
       "9                   A                minor                        A   \n",
       "10                  D                minor                        D   \n",
       "11                  A                minor                        A   \n",
       "12                  D                minor                        D   \n",
       "13                  A                major                        A   \n",
       "14                  A                minor                        A   \n",
       "15                 Eb                minor                       Eb   \n",
       "16                  B                major                        B   \n",
       "17                  G                major                        G   \n",
       "18                 C#                major                       C#   \n",
       "19                  G                major                        G   \n",
       "\n",
       "   tonal.key_krumhansl.scale  tonal.key_temperley.key  \\\n",
       "0                      major                        G   \n",
       "1                      minor                        C   \n",
       "2                      minor                        D   \n",
       "3                      minor                        G   \n",
       "4                      minor                        D   \n",
       "5                      minor                        A   \n",
       "6                      minor                        A   \n",
       "7                      major                       Bb   \n",
       "8                      minor                        C   \n",
       "9                      minor                        A   \n",
       "10                     minor                        A   \n",
       "11                     minor                        A   \n",
       "12                     minor                        D   \n",
       "13                     major                        F   \n",
       "14                     minor                        A   \n",
       "15                     minor                       Eb   \n",
       "16                     major                        B   \n",
       "17                     major                        G   \n",
       "18                     major                       C#   \n",
       "19                     major                        G   \n",
       "\n",
       "    tonal.key_temperley.scale  \n",
       "0                       major  \n",
       "1                       minor  \n",
       "2                       minor  \n",
       "3                       minor  \n",
       "4                       minor  \n",
       "5                       minor  \n",
       "6                       minor  \n",
       "7                       major  \n",
       "8                       minor  \n",
       "9                       minor  \n",
       "10                      minor  \n",
       "11                      minor  \n",
       "12                      minor  \n",
       "13                      major  \n",
       "14                      minor  \n",
       "15                      minor  \n",
       "16                      major  \n",
       "17                      major  \n",
       "18                      major  \n",
       "19                      major  \n",
       "\n",
       "[20 rows x 137 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i = pd.read_csv(TRAININFO, delimiter='\\t',encoding='utf-8')\n",
    "df_r = pd.read_csv(TRAINRANK)\n",
    "df_i_train = df_i.merge(df_r, left_on='ID', right_on='ID')\n",
    "df_i_train[\"dataset\"] = \"train\"\n",
    "\n",
    "df_i_test = pd.read_csv(TESTINFO, delimiter='\\t',encoding='utf-8')\n",
    "df_i_test[\"label\"] = np.nan\n",
    "df_i_test[\"dataset\"] = \"test\"\n",
    "\n",
    "df = pd.concat([df_i_train, df_i_test])\n",
    "print(len(df))\n",
    "df_track_info = pd.read_csv(\"../csv/all_track_info.csv\")\n",
    "df = df.merge(df_track_info, left_on='ID', right_on='ID')\n",
    "df_audio_features = pd.read_csv(\"../csv/all_track_audio_features.csv\")\n",
    "df =df.merge(df_audio_features,left_on=\"ID\",right_on=\"ID\", how=\"left\")\n",
    "\n",
    "df = df.sort_values(by=['ID'])\n",
    "df= df.reset_index()\n",
    "# for i,o in df.iterrows():\n",
    "#     print(o.ID,\"------\",o.album,\"--------\", o.genre,\"------\",o.artist_name,\"---\",o.label)\n",
    "df.columns\n",
    "# print(len(df))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imagehash\n",
    "import os\n",
    "images_train_path = \"/data/zalo/hit-song-prediction/train-image-samples\"\n",
    "images_test_path = \"/data/zalo/hit-song-prediction/test-image-samples\"\n",
    "\n",
    "df[\"album_hash\"] = df[\"album\"] \n",
    "for i, row in df.iterrows():\n",
    "    if row[\"dataset\"]== \"train\":\n",
    "        images_path = images_train_path\n",
    "    else:\n",
    "        images_path = images_test_path\n",
    "    \n",
    "    jpg_path = \"/\".join([images_path, str(row[\"ID\"])+\".mp3.jpg\"])\n",
    "    if os.path.isfile(jpg_path):\n",
    "        df[\"album_hash\"].iloc[i] = str(imagehash.average_hash(Image.open(jpg_path)))\n",
    "        print(i)\n",
    "    else:\n",
    "        df[\"album_hash\"].iloc[i] = np.nan\n",
    "        \n",
    "df[\"album_hash\"] =  df[\"album_hash\"].astype(\"category\").cat.codes\n",
    "df[\"numSongInAlbum\"] = df.groupby(\"album_hash\")[\"album_hash\"].transform(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"numsongInAlbum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df[\"custom_album\"] = df[\"album\"].copy()\n",
    "\n",
    "custom_album_id_count = 1\n",
    "previous_row= None\n",
    "for i,row in df.iterrows():\n",
    "#     print(row[\"custom_album\"] , type(row[\"custom_album\"] ))\n",
    "    if type(row[\"custom_album\"]) != str:\n",
    "        current_artist_ids = re.split(',|\\.', df.iloc[i][\"artist_id\"])\n",
    "        preious_artist_ids = re.split(',|\\.', df.iloc[i-1][\"artist_id\"])\n",
    "        denta= list(set(current_artist_ids) - set(preious_artist_ids))\n",
    "        print(\"current:\", current_artist_ids)\n",
    "        print(\"previous: \", preious_artist_ids)\n",
    "        print(\"denta: \", denta)\n",
    "        print(\"len(denta): \", len(denta),\"\\n\")\n",
    "\n",
    "        if len(denta) != len(current_artist_ids):\n",
    "            df[\"custom_album\"].iloc[i] = custom_album_id_count\n",
    "        else:\n",
    "            custom_album_id_count +=1\n",
    "            df[\"custom_album\"].iloc[i] = custom_album_id_count\n",
    "        \n",
    "df[\"custom_album\"] = df[\"custom_album\"].astype('category').cat.codes    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in df.iterrows():\n",
    "    print(o.ID,\"------\",o.album,\"--------\", o.custom_album, \"---\", o.artist_id, \"---\", o.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(data={\"x\":[1,2,1], \"y\":[4,5,4], \"z\":[4,5,6]})\n",
    "a[\"group\"] = a.groupby([\"x\",\"y\"]).ngroup()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"album\")[\"album\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0.24038838760298156 ratio is nan album\n",
      "There is 0.0017653981953707335 ratio is nan genre\n",
      "There is 0.24038838760298156 ratio is nan album_artist\n",
      "There is 0.0007846214201647705 ratio is nan track\n",
      "There is 0.6722244017261672 ratio is nan lyric\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Fill nan album\n",
    "print(\"There is {} ratio is nan album\".format(len(df[df[\"album\"].isnull()])/len(df)))\n",
    "df[\"album\"]  = df[\"album\"].fillna(\"\")\n",
    "df[\"len_album_name\"] = df[\"album\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemixAlbum\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isOSTAlbum\"] = [ 1 if \"OST\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isSingleAlbum\"] = [ 1 if \"Single\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isBeatAlbum\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isTopHitAlbum\"] = [ 1 if \"Top Hits\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isCoverAlbum\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isEPAlbum\"] = [ 1 if \"EP\" in t else 0 for t in df[\"album\"]]\n",
    "df[\"isLienKhucAlbum\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"album\"]]\n",
    "\n",
    "df[\"album_name_is_title_name\"]= [1 if r.title in r.album  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album\"] = df[\"album\"].astype('category')\n",
    "df[\"album\"] =  df[\"album\"].cat.codes\n",
    "\n",
    "df[\"artist_name_cat\"] = df[\"artist_name\"].astype('category')\n",
    "df[\"artist_name_cat\"] =  df[\"artist_name_cat\"].cat.codes\n",
    "df[\"composers_name_cat\"] = df[\"composers_name\"].astype('category')\n",
    "df[\"composers_name_cat\"] =  df[\"composers_name_cat\"].cat.codes\n",
    "df[\"copyright_cat\"] = df[\"copyright\"].astype('category')\n",
    "df[\"copyright_cat\"] =  df[\"copyright_cat\"].cat.codes\n",
    "\n",
    "import re\n",
    "def get_min_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.min(ps)\n",
    "\n",
    "def get_max_artist_id(s):\n",
    "    ps = re.split(',|\\.',s)\n",
    "    ps = [int(p) for p in ps]\n",
    "    return np.max(ps)\n",
    "\n",
    "df[\"artist_id_min\"]=  df[\"artist_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"artist_id_min_cat\"] = df[\"artist_id_min\"].astype('category')\n",
    "df[\"artist_id_min_cat\"] =  df[\"artist_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_min\"]=  df[\"composers_id\"].apply(lambda x: get_min_artist_id(x))\n",
    "df[\"composers_id_min_cat\"] = df[\"composers_id_min\"].astype('category')\n",
    "df[\"composers_id_min_cat\"] =  df[\"composers_id_min_cat\"].cat.codes\n",
    "\n",
    "df[\"artist_id_max\"]=  df[\"artist_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"artist_id_max_cat\"] = df[\"artist_id_max\"].astype('category')\n",
    "df[\"artist_id_max_cat\"] =  df[\"artist_id_max_cat\"].cat.codes\n",
    "\n",
    "df[\"composers_id_max\"]=  df[\"composers_id\"].apply(lambda x: get_max_artist_id(x))\n",
    "df[\"composers_id_max_cat\"] = df[\"composers_id_max\"].astype('category')\n",
    "df[\"composers_id_max_cat\"] =  df[\"composers_id_max_cat\"].cat.codes\n",
    "\n",
    "#New feature\n",
    "# df[\"group_album_artist_id_min_cat\"] = df.groupby([\"album\",\"artist_id_min_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_min_cat\"] = df[\"group_album_artist_id_min_cat\"].astype(\"category\").cat.codes\n",
    "# df[\"group_album_artist_id_max_cat\"] = df.groupby([\"album\",\"artist_id_max_cat\"]).ngroup()\n",
    "# df[\"group_album_artist_id_max_cat\"] = df[\"group_album_artist_id_max_cat\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Fill genre\n",
    "print(\"There is {} ratio is nan genre\".format(len(df[df[\"genre\"].isnull()])/len(df)))\n",
    "df[\"genre\"]  = df[\"genre\"].fillna(\"No genre\")\n",
    "df[\"genre\"] = df[\"genre\"].astype('category')\n",
    "df[\"genre\"] =  df[\"genre\"].cat.codes\n",
    "\n",
    "# Fill album_artist\n",
    "print(\"There is {} ratio is nan album_artist\".format(len(df[df[\"album_artist\"].isnull()])/len(df)))\n",
    "df[\"album_artist\"]  = df[\"album_artist\"].fillna(\"No album_artist\")\n",
    "df[\"album_artist_contain_artistname\"]= [1 if r.album_artist in r.artist_name  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album_artist\"] = df[\"album_artist\"].astype('category')\n",
    "df[\"album_artist\"] =  df[\"album_artist\"].cat.codes\n",
    "\n",
    "# Fill track\n",
    "print(\"There is {} ratio is nan track\".format(len(df[df[\"track\"].isnull()])/len(df)))\n",
    "df[\"track\"]  = df[\"track\"].fillna(\"(1, 1)\")\n",
    "df[\"istrack11\"] = df[\"track\"] == \"(1, 1)\"\n",
    "def tracknum_to_value(track_num):\n",
    "    try:\n",
    "        \n",
    "        track_num = make_tuple(track_num)\n",
    "        if track_num[0] is not None:\n",
    "            return float(track_num[0]) / float(track_num[1])\n",
    "        else:\n",
    "            return 1.0\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "df[\"track\"] = df[\"track\"].apply(lambda t: tracknum_to_value(t))\n",
    "\n",
    "\n",
    "# Fill lyric\n",
    "print(\"There is {} ratio is nan lyric\".format(len(df[df[\"lyric\"].isnull()])/len(df)))\n",
    "df[\"lyric\"]  = df[\"lyric\"].fillna(\"\")\n",
    "df[\"islyric\"] = df[\"lyric\"].apply(lambda x:  True if len(x)  else False)\n",
    "df[\"num_line_lyric\"] = df[\"lyric\"].apply(lambda x : len(x.split(\"\\r\")))\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "from dateutil import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ast import literal_eval as make_tuple\n",
    "df['no_artist'] = df.artist_name.apply(lambda x: len(x.split(\",\")))\n",
    "df['no_composer'] = df.composers_name.apply(lambda x: len(x.split(\",\")))\n",
    "df[\"freq_artist\"] = df.groupby('artist_id')['artist_id'].transform('count').astype('float')\n",
    "df[\"freq_composer\"] = df.groupby('composers_id')['composers_id'].transform('count').astype('float')\n",
    "df[\"freq_artist_min\"] = df.groupby('artist_id_min_cat')['artist_id_min_cat'].transform('count').astype('float')\n",
    "df[\"freq_composer_min\"] = df.groupby('composers_id_min_cat')['composers_id_min_cat'].transform('count').astype('float')\n",
    "\n",
    "df[\"num_album_per_min_artist\"] = df.groupby(['artist_id_min_cat','album'])['album'].transform('count').astype('float')\n",
    "df[\"num_album_per_min_composer\"] = df.groupby(['composers_id_min','album'])['album'].transform('count').astype('float')\n",
    "# df[\"mean_album_score\"] = df.groupby(['album','dataset'])['label'].transform('mean')\n",
    "# df[\"mean_album_score\"] = df[\"mean_album_score\"].replace(to_replace=-1,value=0)\n",
    "# df[\"mean_artist_score\"] = df.groupby(['composers_id_min_cat','dataset'])['label'].transform('mean')\n",
    "# df[\"mean_artist_score\"] = df[\"mean_artist_score\"].replace(to_replace=-1,value=0)\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df.release_time)\n",
    "df[\"year\"] = df[\"datetime\"].dt.year\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"day\"] = df[\"datetime\"].dt.day\n",
    "df[\"dayofyear\"] = df[\"datetime\"].dt.dayofyear\n",
    "df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "from datetime import date \n",
    "import holidays \n",
    "\n",
    "in_holidays = holidays.HolidayBase() \n",
    "for i in range(26,32):\n",
    "    in_holidays.append(str(i)+'-01-2017')\n",
    "in_holidays.append('01-02-2017')\n",
    "for i in range(14,21):\n",
    "    in_holidays.append(str(i)+'-02-2018')\n",
    "in_holidays.append('30-04-2017')\n",
    "in_holidays.append('30-04-2018')\n",
    "in_holidays.append('01-01-2017')\n",
    "in_holidays.append('01-01-2018')\n",
    "in_holidays.append('14-02-2017')\n",
    "in_holidays.append('14-02-2018')\n",
    "in_holidays.append('08-03-2017')\n",
    "in_holidays.append('08-03-2018')\n",
    "in_holidays.append('01-05-2017')\n",
    "in_holidays.append('01-05-2018')\n",
    "in_holidays.append('06-04-2017')\n",
    "in_holidays.append('25-04-2018')\n",
    "in_holidays.append('01-06-2017')\n",
    "in_holidays.append('01-06-2018')\n",
    "in_holidays.append('04-10-2017')\n",
    "in_holidays.append('24-09-2018')\n",
    "in_holidays.append('20-10-2017')\n",
    "in_holidays.append('20-10-2018')\n",
    "in_holidays.append('20-11-2017')\n",
    "in_holidays.append('20-11-2018')\n",
    "in_holidays.append('24-12-2017')\n",
    "in_holidays.append('24-12-2018')\n",
    "df['isHoliday'] = df.release_time.apply(lambda x: x in in_holidays)\n",
    "\n",
    "\n",
    "\n",
    "df[\"len_of_songname\"] = df[\"title\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"isRemix\"] = [ 1 if \"Remix\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isOST\"] = [ 1 if \"OST\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isBeat\"] = [ 1 if \"Beat\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isVersion\"] = [ 1 if \"Version\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isCover\"] = [ 1 if \"Cover\" in t else 0 for t in df[\"title\"]]\n",
    "df[\"isLienKhuc\"] = [ 1 if \"Liên Khúc\" in t else 0 for t in df[\"title\"]]\n",
    "\n",
    "# features_to_combine = [\"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \n",
    "#                        \"isCover\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\",\n",
    "#                       \"isBeatAlbum\", \"isCoverAlbum\"]\n",
    "\n",
    "# combine_features = []\n",
    "# for i in range(0,len(features_to_combine)-1):\n",
    "#     for j in range(1, len(features_to_combine)):\n",
    "#         combine_features.append(\"{}_{}\".format(features_to_combine[i], features_to_combine[j]))\n",
    "#         df[\"{}_{}\".format(features_to_combine[i], features_to_combine[j])] = df[\"{}\".format(features_to_combine[i])] & \\\n",
    "#                                                                             df[\"{}\".format(features_to_combine[j])]\n",
    "\n",
    "def find_num_song_release_in_final_month(df, day):\n",
    "    month5th = day + relativedelta.relativedelta(months=5)\n",
    "    month6th = day + relativedelta.relativedelta(months=6)  \n",
    "    return len(df.datetime[(df.datetime >= month5th)&(df.datetime<=month6th)])\n",
    "\n",
    "\n",
    "\n",
    "df[\"num_song_release_in_final_month\"] = df.datetime.apply(lambda d:find_num_song_release_in_final_month(df ,d))\n",
    "\n",
    "\n",
    "df[\"album_right\"] = df.groupby(df.release_time).ngroup().astype(\"category\").cat.codes\n",
    "df[\"day_release\"] = df.groupby([\"year\",\"dayofyear\"]).ngroup().astype(\"category\").cat.codes\n",
    "df[\"numsongInAlbum\"] = df.groupby(\"album_right\")[\"album_right\"].transform(\"count\")\n",
    "df[\"isSingleAlbum_onesong\"]= df[\"isSingleAlbum\"] & (df[\"numsongInAlbum\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.violinplot(x=\"istrack11\", y=\"label\", data=df[df.dataset==\"train\"], inner=\"quartile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "chosen_features = [\"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "                   \"isBeat\",  \"num_song_release_in_final_month\",\n",
    "                  \"length\", \"genre\", \"track\",\"album_artist\",\"album\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "                  \"len_album_name\", \"isOSTAlbum\", \"album_name_is_title_name\",\n",
    "                  \"isBeatAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "                  \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "                   \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "                  \"num_album_per_min_artist\", \"num_album_per_min_composer\", \"num_line_lyric\",\n",
    "                    \"custom_album\"]\n",
    "\n",
    "chosen_features = [ \"numSongInAlbum\", \"isBeat\", \"isOSTAlbum\",\"isBeatAlbum\"]\n",
    "\n",
    "chosen_features.append(\"mean_score_by_album_artist_id_min_cat\")\n",
    "df_train = df_train[df_train.numSongInAlbum>3]\n",
    "\n",
    "\n",
    "folds = StratifiedKFold(n_splits=3(len(df[df[\"album_artist\"].isnull()])/len(df)))\n",
    "df[\"album_artist\"]  = df[\"album_artist\"].fillna(\"No album_artist\")\n",
    "df[\"album_artist_contain_artistname\"]= [1 if r.album_artist in r.artist_name  else 0 for i,r in df.iterrows() ]\n",
    "df[\"album_artist\"] = df[\"album_artist\"].astype('category')\n",
    "df[\"album_artist\"] =  df[\"album_artist\"].cat.codes\n",
    ", shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "labels= df_train.label\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "\n",
    "    mlp = MLPRegressor( verbose=False,early_stopping=True)\n",
    "    #Generate mean based on album and artist\n",
    "    df_train[\"mean_score_by_album_artist_id_min_cat\"] = np.nan # Init\n",
    "    df_train[\"mean_score_by_album_artist_id_min_cat\"].iloc[trn_idx] = df_train.iloc[trn_idx].  \\\n",
    "                                    groupby([\"album_hash\"])[\"label\"].transform(\"mean\")\n",
    "    \n",
    "#     haha = df_train.iloc[trn_idx].groupby(\"album\")[\"album\"].transform(\"count\")\n",
    "#     tmp = haha[haha.tmp==1]\n",
    "#     print(tmp.label.mean())\n",
    "    \n",
    "    df_train[\"mean_score_by_album_artist_id_min_cat\"] = df_train.groupby([\"album_hash\"])[\"mean_score_by_album_artist_id_min_cat\"]\\\n",
    "                                            .transform(lambda column: column.fillna(column.mean()))\\\n",
    "                                            .fillna(1)\n",
    "#     print(df_train[chosen_features].values)\n",
    "    mlp.fit(df_train.iloc[trn_idx][chosen_features].values,labels.iloc[trn_idx])\n",
    "    oof[val_idx] = mlp.predict(df_train.iloc[val_idx][chosen_features].values)\n",
    "    \n",
    "    df_test[\"mean_score_by_album_artist_id_min_cat\"] = np.nan \n",
    "    df_all_tmp = pd.concat([df_train,df_test])\n",
    "    df_all_tmp[\"mean_score_by_album_artist_id_min_cat\"] = df_all_tmp.groupby([\"album_hash\"])[\"label\"]\\\n",
    "                                            .transform(lambda column: column.fillna(column.mean()))\\\n",
    "                                            .fillna(1)\n",
    "    \n",
    "    \n",
    "    df_test = df_all_tmp[df_all_tmp.dataset == \"test\"]\n",
    "    predictions += mlp.predict(df_test[chosen_features]) / folds.n_splits\n",
    "    print(\"RMSE: {:<8.5f}\".format(sqrt(mean_squared_error(labels.iloc[val_idx], oof[val_idx]))))\n",
    "    \n",
    "    \n",
    "print(\"Final RMSE: {:<8.5f}\".format(sqrt(mean_squared_error(labels, oof))))\n",
    "sub = pd.DataFrame({\"ID\": df_test.ID.values})\n",
    "sub[\"label\"] = predictions\n",
    "sub.to_csv(\"submission_lightgbm1.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean_score_by_album_artist_id_min_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3957)\n",
    "a = df.groupby([\"album\",\"artist_id_max_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "df.groupby([\"album\",\"artist_id_max_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "b = a[(\"label\", \"std\")].fillna(-2)\n",
    "c = b[b!=-2]\n",
    "print(\"Mean std:\", np.median(c))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gửi ngàn lời yêu karaoke pd.set_option('display.max_rows', 3957)\n",
    "a = df.groupby([\"album\",\"artist_id_max_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "df.groupby([\"album\",\"artist_id_max_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "b = a[(\"label\", \"std\")].fillna(-2)\n",
    "c = b[b!=-2]\n",
    "print(\"Mean std:\", np.median(c))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3957)\n",
    "a = df.groupby([\"album\",\"artist_id_max_cat\"])[\"label\"].transform(\"mean\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3957)\n",
    "a = df.groupby([\"album\",\"composers_id_min_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "df.groupby([\"album\",\"composers_id_min_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "b = a[(\"label\", \"std\")].fillna(-2)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3957)\n",
    "a = df.groupby([\"custom_album\",\"artist_name\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "df.groupby([\"custom_album\",\"artist_name\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "b = a[(\"label\", \"std\")].fillna(-2)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3957)\n",
    "a = df.groupby([\"album\",\"artist_id_min_cat\"])[[\"labe              l\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "df.groupby([\"album\",\"artist_id_min_cat\"])[[\"label\"]].agg([\"mean\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[(\"label\", \"std\")].fillna(-2)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3957)\n",
    "a[\"b\"] = df.groupby([\"album\",\"artist_id_min_cat\"])[\"label\"].transform(\"mean\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gửi ngàn lời yêu karaoke # features_to_combine = [\"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \n",
    "#                        \"isCover\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\",\n",
    "#                       \"isBeatAlbum\", \"isCoverAlbum\"]\n",
    "\n",
    "features_to_combine = [\"isLienKhuc\", \"isLienKhucAlbum\", \"isSingleAlbum\",\"numsongInAlbum\", \"isSingleAlbum_onesong\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "title_features = features_to_combine\n",
    "fig, axes = plt.subplots(23, 2, figsize=(20, 10*23))fig, axes = plt.subplots(23, 2, figsize=(20, 10*23))\n",
    "\n",
    "for feature, ax in zip(title_features, axes.flat):\n",
    "    sns.violinplot(x=feature, y=\"label\", data=df[df.dataset==\"train\"], inner=\"quartile\",ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "title_features = combine_features\n",
    "fig, axes = plt.subplots(23, 2, figsize=(20, 10*23))\n",
    "for feature, ax in zip(title_features, axes.flat):\n",
    "    sns.violinplot(x=feature, y=\"label\", data=df[df.dataset==\"train\"], inner=\"quartile\",ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['album_right', 'istrack11', 'no_artist', 'no_composer', 'freq_artist', 'freq_composer', 'year', 'month', 'hour', 'day', 'len_of_songname', 'isRemix', 'isOST', 'isBeat', 'isVersion', 'isCover', 'num_song_release_in_final_month', 'length', 'genre', 'track', 'album_artist', 'islyric', 'album_artist_contain_artistname', 'len_album_name', 'isRemixAlbum', 'isOSTAlbum', 'isSingleAlbum', 'album_name_is_title_name', 'isBeatAlbum', 'isCoverAlbum', 'artist_name_cat', 'composers_name_cat', 'copyright_cat', 'artist_id_min_cat', 'composers_id_min_cat', 'artist_id_max_cat', 'composers_id_max_cat', 'freq_artist_min', 'freq_composer_min', 'dayofyear', 'weekday', 'isHoliday', 'num_album_per_min_artist', 'num_album_per_min_composer', 'numsongInAlbum', 'isSingleAlbum_onesong']\n",
      "46\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74664\tvalid_1's rmse: 1.9543\n",
      "[10000]\ttraining's rmse: 1.42821\tvalid_1's rmse: 1.78485\n",
      "[15000]\ttraining's rmse: 1.23979\tvalid_1's rmse: 1.7117\n",
      "[20000]\ttraining's rmse: 1.10738\tvalid_1's rmse: 1.67003\n",
      "[25000]\ttraining's rmse: 1.0059\tvalid_1's rmse: 1.6432\n",
      "[30000]\ttraining's rmse: 0.923804\tvalid_1's rmse: 1.62449\n",
      "[35000]\ttraining's rmse: 0.855801\tvalid_1's rmse: 1.61213\n",
      "[40000]\ttraining's rmse: 0.797323\tvalid_1's rmse: 1.60203\n",
      "[45000]\ttraining's rmse: 0.745528\tvalid_1's rmse: 1.59502\n",
      "[50000]\ttraining's rmse: 0.70037\tvalid_1's rmse: 1.58943\n",
      "[55000]\ttraining's rmse: 0.659744\tvalid_1's rmse: 1.58517\n",
      "[60000]\ttraining's rmse: 0.623274\tvalid_1's rmse: 1.58188\n",
      "[65000]\ttraining's rmse: 0.589784\tvalid_1's rmse: 1.57901\n",
      "[70000]\ttraining's rmse: 0.559601\tvalid_1's rmse: 1.57727\n",
      "[75000]\ttraining's rmse: 0.531483\tvalid_1's rmse: 1.57563\n",
      "[80000]\ttraining's rmse: 0.506028\tvalid_1's rmse: 1.57441\n",
      "[85000]\ttraining's rmse: 0.482422\tvalid_1's rmse: 1.57335\n",
      "[90000]\ttraining's rmse: 0.46049\tvalid_1's rmse: 1.57296\n",
      "[95000]\ttraining's rmse: 0.440181\tvalid_1's rmse: 1.57241\n",
      "[100000]\ttraining's rmse: 0.421368\tvalid_1's rmse: 1.57192\n",
      "[105000]\ttraining's rmse: 0.403865\tvalid_1's rmse: 1.57171\n",
      "[110000]\ttraining's rmse: 0.387491\tvalid_1's rmse: 1.57127\n",
      "[115000]\ttraining's rmse: 0.372086\tvalid_1's rmse: 1.57103\n",
      "[120000]\ttraining's rmse: 0.357603\tvalid_1's rmse: 1.57096\n",
      "[125000]\ttraining's rmse: 0.344068\tvalid_1's rmse: 1.57099\n",
      "[130000]\ttraining's rmse: 0.331571\tvalid_1's rmse: 1.5712\n",
      "[135000]\ttraining's rmse: 0.319706\tvalid_1's rmse: 1.57128\n",
      "Early stopping, best iteration is:\n",
      "[119225]\ttraining's rmse: 0.359755\tvalid_1's rmse: 1.5709\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74461\tvalid_1's rmse: 1.94579\n",
      "[10000]\ttraining's rmse: 1.42312\tvalid_1's rmse: 1.79436\n",
      "[15000]\ttraining's rmse: 1.23306\tvalid_1's rmse: 1.7342\n",
      "[20000]\ttraining's rmse: 1.09968\tvalid_1's rmse: 1.70076\n",
      "[25000]\ttraining's rmse: 0.998249\tvalid_1's rmse: 1.68083\n",
      "[30000]\ttraining's rmse: 0.91642\tvalid_1's rmse: 1.66668\n",
      "[35000]\ttraining's rmse: 0.848384\tvalid_1's rmse: 1.65698\n",
      "[40000]\ttraining's rmse: 0.790386\tvalid_1's rmse: 1.65\n",
      "[45000]\ttraining's rmse: 0.739771\tvalid_1's rmse: 1.64441\n",
      "[50000]\ttraining's rmse: 0.69489\tvalid_1's rmse: 1.64025\n",
      "[55000]\ttraining's rmse: 0.654122\tvalid_1's rmse: 1.63687\n",
      "[60000]\ttraining's rmse: 0.618139\tvalid_1's rmse: 1.63413\n",
      "[65000]\ttraining's rmse: 0.58516\tvalid_1's rmse: 1.6317\n",
      "[70000]\ttraining's rmse: 0.555365\tvalid_1's rmse: 1.63005\n",
      "[75000]\ttraining's rmse: 0.527506\tvalid_1's rmse: 1.62852\n",
      "[80000]\ttraining's rmse: 0.502135\tvalid_1's rmse: 1.62735\n",
      "[85000]\ttraining's rmse: 0.478765\tvalid_1's rmse: 1.62624\n",
      "[90000]\ttraining's rmse: 0.456709\tvalid_1's rmse: 1.62554\n",
      "[95000]\ttraining's rmse: 0.436435\tvalid_1's rmse: 1.62492\n",
      "[100000]\ttraining's rmse: 0.417639\tvalid_1's rmse: 1.62449\n",
      "[105000]\ttraining's rmse: 0.400109\tvalid_1's rmse: 1.62418\n",
      "[110000]\ttraining's rmse: 0.383606\tvalid_1's rmse: 1.62401\n",
      "[115000]\ttraining's rmse: 0.368306\tvalid_1's rmse: 1.62372\n",
      "[120000]\ttraining's rmse: 0.353843\tvalid_1's rmse: 1.62346\n",
      "[125000]\ttraining's rmse: 0.340405\tvalid_1's rmse: 1.62342\n",
      "[130000]\ttraining's rmse: 0.327801\tvalid_1's rmse: 1.6234\n",
      "[135000]\ttraining's rmse: 0.315886\tvalid_1's rmse: 1.62337\n",
      "[140000]\ttraining's rmse: 0.30442\tvalid_1's rmse: 1.62331\n",
      "[145000]\ttraining's rmse: 0.293545\tvalid_1's rmse: 1.62333\n",
      "[150000]\ttraining's rmse: 0.283544\tvalid_1's rmse: 1.62355\n",
      "[155000]\ttraining's rmse: 0.273988\tvalid_1's rmse: 1.62371\n",
      "Early stopping, best iteration is:\n",
      "[138696]\ttraining's rmse: 0.307328\tvalid_1's rmse: 1.62326\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.75079\tvalid_1's rmse: 1.93682\n",
      "[10000]\ttraining's rmse: 1.43067\tvalid_1's rmse: 1.7564\n",
      "[15000]\ttraining's rmse: 1.24116\tvalid_1's rmse: 1.67747\n",
      "[20000]\ttraining's rmse: 1.10792\tvalid_1's rmse: 1.63189\n",
      "[25000]\ttraining's rmse: 1.00578\tvalid_1's rmse: 1.60252\n",
      "[30000]\ttraining's rmse: 0.922902\tvalid_1's rmse: 1.58269\n",
      "[35000]\ttraining's rmse: 0.853714\tvalid_1's rmse: 1.56854\n",
      "[40000]\ttraining's rmse: 0.795335\tvalid_1's rmse: 1.55804\n",
      "[45000]\ttraining's rmse: 0.743656\tvalid_1's rmse: 1.55014\n",
      "[50000]\ttraining's rmse: 0.698328\tvalid_1's rmse: 1.54441\n",
      "[55000]\ttraining's rmse: 0.657506\tvalid_1's rmse: 1.53966\n",
      "[60000]\ttraining's rmse: 0.621074\tvalid_1's rmse: 1.53608\n",
      "[65000]\ttraining's rmse: 0.587829\tvalid_1's rmse: 1.53277\n",
      "[70000]\ttraining's rmse: 0.557694\tvalid_1's rmse: 1.53043\n",
      "[75000]\ttraining's rmse: 0.529787\tvalid_1's rmse: 1.52851\n",
      "[80000]\ttraining's rmse: 0.504415\tvalid_1's rmse: 1.52699\n",
      "[85000]\ttraining's rmse: 0.480792\tvalid_1's rmse: 1.52588\n",
      "[90000]\ttraining's rmse: 0.458884\tvalid_1's rmse: 1.52517\n",
      "[95000]\ttraining's rmse: 0.438739\tvalid_1's rmse: 1.52446\n",
      "[100000]\ttraining's rmse: 0.420155\tvalid_1's rmse: 1.52382\n",
      "[105000]\ttraining's rmse: 0.402659\tvalid_1's rmse: 1.52344\n",
      "[110000]\ttraining's rmse: 0.386311\tvalid_1's rmse: 1.52304\n",
      "[115000]\ttraining's rmse: 0.370955\tvalid_1's rmse: 1.52299\n",
      "[120000]\ttraining's rmse: 0.356475\tvalid_1's rmse: 1.52284\n",
      "[125000]\ttraining's rmse: 0.343095\tvalid_1's rmse: 1.52298\n",
      "[130000]\ttraining's rmse: 0.330543\tvalid_1's rmse: 1.52311\n",
      "[135000]\ttraining's rmse: 0.31868\tvalid_1's rmse: 1.52314\n",
      "Early stopping, best iteration is:\n",
      "[118925]\ttraining's rmse: 0.359492\tvalid_1's rmse: 1.52281\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.7452\tvalid_1's rmse: 1.97513\n",
      "[10000]\ttraining's rmse: 1.42776\tvalid_1's rmse: 1.79614\n",
      "[15000]\ttraining's rmse: 1.23871\tvalid_1's rmse: 1.71791\n",
      "[20000]\ttraining's rmse: 1.10599\tvalid_1's rmse: 1.67441\n",
      "[25000]\ttraining's rmse: 1.00397\tvalid_1's rmse: 1.64647\n",
      "[30000]\ttraining's rmse: 0.921242\tvalid_1's rmse: 1.62904\n",
      "[35000]\ttraining's rmse: 0.852967\tvalid_1's rmse: 1.61704\n",
      "[40000]\ttraining's rmse: 0.794752\tvalid_1's rmse: 1.60867\n",
      "[45000]\ttraining's rmse: 0.742893\tvalid_1's rmse: 1.60246\n",
      "[50000]\ttraining's rmse: 0.69764\tvalid_1's rmse: 1.59779\n",
      "[55000]\ttraining's rmse: 0.656944\tvalid_1's rmse: 1.59471\n",
      "[60000]\ttraining's rmse: 0.620405\tvalid_1's rmse: 1.59267\n",
      "[65000]\ttraining's rmse: 0.586948\tvalid_1's rmse: 1.59118\n",
      "[70000]\ttraining's rmse: 0.556835\tvalid_1's rmse: 1.59015\n",
      "[75000]\ttraining's rmse: 0.528776\tvalid_1's rmse: 1.58951\n",
      "[80000]\ttraining's rmse: 0.503396\tvalid_1's rmse: 1.58909\n",
      "[85000]\ttraining's rmse: 0.479698\tvalid_1's rmse: 1.58878\n",
      "[90000]\ttraining's rmse: 0.457476\tvalid_1's rmse: 1.58889\n",
      "[95000]\ttraining's rmse: 0.437146\tvalid_1's rmse: 1.58895\n",
      "[100000]\ttraining's rmse: 0.418342\tvalid_1's rmse: 1.58907\n",
      "[105000]\ttraining's rmse: 0.400724\tvalid_1's rmse: 1.58945\n",
      "Early stopping, best iteration is:\n",
      "[88198]\ttraining's rmse: 0.465317\tvalid_1's rmse: 1.58867\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74255\tvalid_1's rmse: 1.98084\n",
      "[10000]\ttraining's rmse: 1.42385\tvalid_1's rmse: 1.80981\n",
      "[15000]\ttraining's rmse: 1.23505\tvalid_1's rmse: 1.73583\n",
      "[20000]\ttraining's rmse: 1.10251\tvalid_1's rmse: 1.69412\n",
      "[25000]\ttraining's rmse: 1.00008\tvalid_1's rmse: 1.66721\n",
      "[30000]\ttraining's rmse: 0.917604\tvalid_1's rmse: 1.64864\n",
      "[35000]\ttraining's rmse: 0.849234\tvalid_1's rmse: 1.63563\n",
      "[40000]\ttraining's rmse: 0.790701\tvalid_1's rmse: 1.62641\n",
      "[45000]\ttraining's rmse: 0.73934\tvalid_1's rmse: 1.6194\n",
      "[50000]\ttraining's rmse: 0.694289\tvalid_1's rmse: 1.6139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55000]\ttraining's rmse: 0.653848\tvalid_1's rmse: 1.60948\n",
      "[60000]\ttraining's rmse: 0.617702\tvalid_1's rmse: 1.60571\n",
      "[65000]\ttraining's rmse: 0.584548\tvalid_1's rmse: 1.6028\n",
      "[70000]\ttraining's rmse: 0.554517\tvalid_1's rmse: 1.60049\n",
      "[75000]\ttraining's rmse: 0.526545\tvalid_1's rmse: 1.59887\n",
      "[80000]\ttraining's rmse: 0.501322\tvalid_1's rmse: 1.59724\n",
      "[85000]\ttraining's rmse: 0.477699\tvalid_1's rmse: 1.59596\n",
      "[90000]\ttraining's rmse: 0.455853\tvalid_1's rmse: 1.59507\n",
      "[95000]\ttraining's rmse: 0.435553\tvalid_1's rmse: 1.59427\n",
      "[100000]\ttraining's rmse: 0.416722\tvalid_1's rmse: 1.59348\n",
      "[105000]\ttraining's rmse: 0.399245\tvalid_1's rmse: 1.59305\n",
      "[110000]\ttraining's rmse: 0.382821\tvalid_1's rmse: 1.59261\n",
      "[115000]\ttraining's rmse: 0.367455\tvalid_1's rmse: 1.59231\n",
      "[120000]\ttraining's rmse: 0.352939\tvalid_1's rmse: 1.59185\n",
      "[125000]\ttraining's rmse: 0.339547\tvalid_1's rmse: 1.59169\n",
      "[130000]\ttraining's rmse: 0.327029\tvalid_1's rmse: 1.59171\n",
      "[135000]\ttraining's rmse: 0.315176\tvalid_1's rmse: 1.59168\n",
      "[140000]\ttraining's rmse: 0.303755\tvalid_1's rmse: 1.59185\n",
      "[145000]\ttraining's rmse: 0.29282\tvalid_1's rmse: 1.59195\n",
      "[150000]\ttraining's rmse: 0.282767\tvalid_1's rmse: 1.59202\n",
      "Early stopping, best iteration is:\n",
      "[134260]\ttraining's rmse: 0.316894\tvalid_1's rmse: 1.59165\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74295\tvalid_1's rmse: 1.97903\n",
      "[10000]\ttraining's rmse: 1.42437\tvalid_1's rmse: 1.80919\n",
      "[15000]\ttraining's rmse: 1.23633\tvalid_1's rmse: 1.73653\n",
      "[20000]\ttraining's rmse: 1.10393\tvalid_1's rmse: 1.69486\n",
      "[25000]\ttraining's rmse: 1.00163\tvalid_1's rmse: 1.66925\n",
      "[30000]\ttraining's rmse: 0.919538\tvalid_1's rmse: 1.65089\n",
      "[35000]\ttraining's rmse: 0.851392\tvalid_1's rmse: 1.63852\n",
      "[40000]\ttraining's rmse: 0.793197\tvalid_1's rmse: 1.62974\n",
      "[45000]\ttraining's rmse: 0.741904\tvalid_1's rmse: 1.62309\n",
      "[50000]\ttraining's rmse: 0.696715\tvalid_1's rmse: 1.6182\n",
      "[55000]\ttraining's rmse: 0.65589\tvalid_1's rmse: 1.61417\n",
      "[60000]\ttraining's rmse: 0.619409\tvalid_1's rmse: 1.61126\n",
      "[65000]\ttraining's rmse: 0.586152\tvalid_1's rmse: 1.60917\n",
      "[70000]\ttraining's rmse: 0.55571\tvalid_1's rmse: 1.60728\n",
      "[75000]\ttraining's rmse: 0.527578\tvalid_1's rmse: 1.60605\n",
      "[80000]\ttraining's rmse: 0.501903\tvalid_1's rmse: 1.60497\n",
      "[85000]\ttraining's rmse: 0.47842\tvalid_1's rmse: 1.60419\n",
      "[90000]\ttraining's rmse: 0.456386\tvalid_1's rmse: 1.60356\n",
      "[95000]\ttraining's rmse: 0.435863\tvalid_1's rmse: 1.60303\n",
      "[100000]\ttraining's rmse: 0.416948\tvalid_1's rmse: 1.60277\n",
      "[105000]\ttraining's rmse: 0.399241\tvalid_1's rmse: 1.60242\n",
      "[110000]\ttraining's rmse: 0.382615\tvalid_1's rmse: 1.60222\n",
      "[115000]\ttraining's rmse: 0.367094\tvalid_1's rmse: 1.60228\n",
      "[120000]\ttraining's rmse: 0.352491\tvalid_1's rmse: 1.60229\n",
      "[125000]\ttraining's rmse: 0.338787\tvalid_1's rmse: 1.6022\n",
      "[130000]\ttraining's rmse: 0.325923\tvalid_1's rmse: 1.60224\n",
      "[135000]\ttraining's rmse: 0.313836\tvalid_1's rmse: 1.60232\n",
      "[140000]\ttraining's rmse: 0.302254\tvalid_1's rmse: 1.60248\n",
      "Early stopping, best iteration is:\n",
      "[124208]\ttraining's rmse: 0.340898\tvalid_1's rmse: 1.60214\n",
      "Fold 6\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74796\tvalid_1's rmse: 1.96581\n",
      "[10000]\ttraining's rmse: 1.4301\tvalid_1's rmse: 1.79075\n",
      "[15000]\ttraining's rmse: 1.24111\tvalid_1's rmse: 1.71465\n",
      "[20000]\ttraining's rmse: 1.10729\tvalid_1's rmse: 1.67186\n",
      "[25000]\ttraining's rmse: 1.005\tvalid_1's rmse: 1.64497\n",
      "[30000]\ttraining's rmse: 0.921129\tvalid_1's rmse: 1.62692\n",
      "[35000]\ttraining's rmse: 0.851945\tvalid_1's rmse: 1.61396\n",
      "[40000]\ttraining's rmse: 0.792775\tvalid_1's rmse: 1.60436\n",
      "[45000]\ttraining's rmse: 0.740664\tvalid_1's rmse: 1.59673\n",
      "[50000]\ttraining's rmse: 0.694795\tvalid_1's rmse: 1.59072\n",
      "[55000]\ttraining's rmse: 0.653777\tvalid_1's rmse: 1.58609\n",
      "[60000]\ttraining's rmse: 0.617026\tvalid_1's rmse: 1.58247\n",
      "[65000]\ttraining's rmse: 0.583639\tvalid_1's rmse: 1.57953\n",
      "[70000]\ttraining's rmse: 0.553418\tvalid_1's rmse: 1.57754\n",
      "[75000]\ttraining's rmse: 0.525185\tvalid_1's rmse: 1.57555\n",
      "[80000]\ttraining's rmse: 0.499782\tvalid_1's rmse: 1.57431\n",
      "[85000]\ttraining's rmse: 0.476083\tvalid_1's rmse: 1.57329\n",
      "[90000]\ttraining's rmse: 0.45413\tvalid_1's rmse: 1.5727\n",
      "[95000]\ttraining's rmse: 0.433891\tvalid_1's rmse: 1.57195\n",
      "[100000]\ttraining's rmse: 0.415068\tvalid_1's rmse: 1.57139\n",
      "[105000]\ttraining's rmse: 0.397436\tvalid_1's rmse: 1.57096\n",
      "[110000]\ttraining's rmse: 0.381267\tvalid_1's rmse: 1.57082\n",
      "[115000]\ttraining's rmse: 0.365894\tvalid_1's rmse: 1.57071\n",
      "[120000]\ttraining's rmse: 0.351488\tvalid_1's rmse: 1.57077\n",
      "[125000]\ttraining's rmse: 0.338183\tvalid_1's rmse: 1.5706\n",
      "[130000]\ttraining's rmse: 0.325603\tvalid_1's rmse: 1.57062\n",
      "[135000]\ttraining's rmse: 0.313772\tvalid_1's rmse: 1.57076\n",
      "[140000]\ttraining's rmse: 0.302354\tvalid_1's rmse: 1.57092\n",
      "[145000]\ttraining's rmse: 0.291644\tvalid_1's rmse: 1.57102\n",
      "Early stopping, best iteration is:\n",
      "[128388]\ttraining's rmse: 0.329556\tvalid_1's rmse: 1.57051\n",
      "Fold 7\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74174\tvalid_1's rmse: 1.97518\n",
      "[10000]\ttraining's rmse: 1.42266\tvalid_1's rmse: 1.81478\n",
      "[15000]\ttraining's rmse: 1.23542\tvalid_1's rmse: 1.74467\n",
      "[20000]\ttraining's rmse: 1.10309\tvalid_1's rmse: 1.70291\n",
      "[25000]\ttraining's rmse: 1.00168\tvalid_1's rmse: 1.67504\n",
      "[30000]\ttraining's rmse: 0.918921\tvalid_1's rmse: 1.65545\n",
      "[35000]\ttraining's rmse: 0.850077\tvalid_1's rmse: 1.64147\n",
      "[40000]\ttraining's rmse: 0.791441\tvalid_1's rmse: 1.63063\n",
      "[45000]\ttraining's rmse: 0.739418\tvalid_1's rmse: 1.62243\n",
      "[50000]\ttraining's rmse: 0.693906\tvalid_1's rmse: 1.61525\n",
      "[55000]\ttraining's rmse: 0.652678\tvalid_1's rmse: 1.6099\n",
      "[60000]\ttraining's rmse: 0.615718\tvalid_1's rmse: 1.60559\n",
      "[65000]\ttraining's rmse: 0.582246\tvalid_1's rmse: 1.60198\n",
      "[70000]\ttraining's rmse: 0.551569\tvalid_1's rmse: 1.59924\n",
      "[75000]\ttraining's rmse: 0.523108\tvalid_1's rmse: 1.59682\n",
      "[80000]\ttraining's rmse: 0.497367\tvalid_1's rmse: 1.59509\n",
      "[85000]\ttraining's rmse: 0.473575\tvalid_1's rmse: 1.59355\n",
      "[90000]\ttraining's rmse: 0.451353\tvalid_1's rmse: 1.59227\n",
      "[95000]\ttraining's rmse: 0.430824\tvalid_1's rmse: 1.59127\n",
      "[100000]\ttraining's rmse: 0.411861\tvalid_1's rmse: 1.59062\n",
      "[105000]\ttraining's rmse: 0.394248\tvalid_1's rmse: 1.59012\n",
      "[110000]\ttraining's rmse: 0.377728\tvalid_1's rmse: 1.58969\n",
      "[115000]\ttraining's rmse: 0.362055\tvalid_1's rmse: 1.58944\n",
      "[120000]\ttraining's rmse: 0.347513\tvalid_1's rmse: 1.58946\n",
      "[125000]\ttraining's rmse: 0.333886\tvalid_1's rmse: 1.58925\n",
      "[130000]\ttraining's rmse: 0.32104\tvalid_1's rmse: 1.5893\n",
      "[135000]\ttraining's rmse: 0.308898\tvalid_1's rmse: 1.5894\n",
      "[140000]\ttraining's rmse: 0.297178\tvalid_1's rmse: 1.58956\n",
      "Early stopping, best iteration is:\n",
      "[124687]\ttraining's rmse: 0.334762\tvalid_1's rmse: 1.58918\n",
      "Fold 8\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74542\tvalid_1's rmse: 1.97518\n",
      "[10000]\ttraining's rmse: 1.42432\tvalid_1's rmse: 1.81519\n",
      "[15000]\ttraining's rmse: 1.23493\tvalid_1's rmse: 1.74742\n",
      "[20000]\ttraining's rmse: 1.10158\tvalid_1's rmse: 1.709\n",
      "[25000]\ttraining's rmse: 0.999565\tvalid_1's rmse: 1.68465\n",
      "[30000]\ttraining's rmse: 0.91625\tvalid_1's rmse: 1.66915\n",
      "[35000]\ttraining's rmse: 0.847145\tvalid_1's rmse: 1.65879\n",
      "[40000]\ttraining's rmse: 0.788581\tvalid_1's rmse: 1.65078\n",
      "[45000]\ttraining's rmse: 0.736707\tvalid_1's rmse: 1.64479\n",
      "[50000]\ttraining's rmse: 0.691363\tvalid_1's rmse: 1.6396\n",
      "[55000]\ttraining's rmse: 0.650611\tvalid_1's rmse: 1.63582\n",
      "[60000]\ttraining's rmse: 0.613747\tvalid_1's rmse: 1.63283\n",
      "[65000]\ttraining's rmse: 0.580842\tvalid_1's rmse: 1.63074\n",
      "[70000]\ttraining's rmse: 0.550718\tvalid_1's rmse: 1.62921\n",
      "[75000]\ttraining's rmse: 0.522911\tvalid_1's rmse: 1.62807\n",
      "[80000]\ttraining's rmse: 0.497687\tvalid_1's rmse: 1.6268\n",
      "[85000]\ttraining's rmse: 0.474282\tvalid_1's rmse: 1.62613\n",
      "[90000]\ttraining's rmse: 0.452624\tvalid_1's rmse: 1.62544\n",
      "[95000]\ttraining's rmse: 0.432489\tvalid_1's rmse: 1.62514\n",
      "[100000]\ttraining's rmse: 0.41389\tvalid_1's rmse: 1.62509\n",
      "[105000]\ttraining's rmse: 0.396624\tvalid_1's rmse: 1.62489\n",
      "[110000]\ttraining's rmse: 0.380476\tvalid_1's rmse: 1.62492\n",
      "[115000]\ttraining's rmse: 0.365302\tvalid_1's rmse: 1.62507\n",
      "[120000]\ttraining's rmse: 0.350979\tvalid_1's rmse: 1.62514\n",
      "[125000]\ttraining's rmse: 0.337813\tvalid_1's rmse: 1.62524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[107735]\ttraining's rmse: 0.387733\tvalid_1's rmse: 1.62481\n",
      "Fold 9\n",
      "Training until validation scores don't improve for 20000 rounds\n",
      "[5000]\ttraining's rmse: 1.74944\tvalid_1's rmse: 1.96065\n",
      "[10000]\ttraining's rmse: 1.42957\tvalid_1's rmse: 1.77733\n",
      "[15000]\ttraining's rmse: 1.23972\tvalid_1's rmse: 1.69783\n",
      "[20000]\ttraining's rmse: 1.10603\tvalid_1's rmse: 1.6518\n",
      "[25000]\ttraining's rmse: 1.00445\tvalid_1's rmse: 1.62194\n",
      "[30000]\ttraining's rmse: 0.921883\tvalid_1's rmse: 1.60249\n",
      "[35000]\ttraining's rmse: 0.85409\tvalid_1's rmse: 1.58785\n",
      "[40000]\ttraining's rmse: 0.795846\tvalid_1's rmse: 1.57801\n",
      "[45000]\ttraining's rmse: 0.744506\tvalid_1's rmse: 1.56995\n",
      "[50000]\ttraining's rmse: 0.698888\tvalid_1's rmse: 1.56408\n",
      "[55000]\ttraining's rmse: 0.658305\tvalid_1's rmse: 1.55965\n",
      "[60000]\ttraining's rmse: 0.621549\tvalid_1's rmse: 1.55568\n",
      "[65000]\ttraining's rmse: 0.58822\tvalid_1's rmse: 1.55285\n",
      "[70000]\ttraining's rmse: 0.557795\tvalid_1's rmse: 1.55074\n",
      "[75000]\ttraining's rmse: 0.52954\tvalid_1's rmse: 1.54891\n",
      "[80000]\ttraining's rmse: 0.503859\tvalid_1's rmse: 1.54733\n",
      "[85000]\ttraining's rmse: 0.480188\tvalid_1's rmse: 1.54628\n",
      "[90000]\ttraining's rmse: 0.458196\tvalid_1's rmse: 1.54551\n",
      "[95000]\ttraining's rmse: 0.437938\tvalid_1's rmse: 1.54486\n",
      "[100000]\ttraining's rmse: 0.419113\tvalid_1's rmse: 1.54468\n",
      "[105000]\ttraining's rmse: 0.401471\tvalid_1's rmse: 1.54436\n",
      "[110000]\ttraining's rmse: 0.38502\tvalid_1's rmse: 1.54417\n",
      "[115000]\ttraining's rmse: 0.369597\tvalid_1's rmse: 1.5441\n",
      "[120000]\ttraining's rmse: 0.355149\tvalid_1's rmse: 1.54417\n",
      "[125000]\ttraining's rmse: 0.3418\tvalid_1's rmse: 1.54428\n",
      "[130000]\ttraining's rmse: 0.329141\tvalid_1's rmse: 1.54431\n",
      "[135000]\ttraining's rmse: 0.317238\tvalid_1's rmse: 1.5447\n",
      "Early stopping, best iteration is:\n",
      "[116815]\ttraining's rmse: 0.364227\tvalid_1's rmse: 1.54402\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# chosen_features = [\"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\"]\n",
    "chosen_features = [\"album_right\", \"istrack11\", \"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "                   \"isRemix\", \"isOST\", \"isBeat\", \"isVersion\", \"isCover\",  \"num_song_release_in_final_month\",\n",
    "                  \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "                  \"len_album_name\", \"isRemixAlbum\", \"isOSTAlbum\", \"isSingleAlbum\", \"album_name_is_title_name\",\n",
    "                  \"isBeatAlbum\", \"isCoverAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "                  \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "                   \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "                  \"num_album_per_min_artist\", \"num_album_per_min_composer\", \n",
    "                   \"numsongInAlbum\",\"isSingleAlbum_onesong\" ]\n",
    "\n",
    "\n",
    "# chosen_features = [\"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "#                    \"isBeat\",  \"num_song_release_in_final_month\",\n",
    "#                   \"length\", \"genre\", \"track\",\"album_artist\",\"album\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "#                   \"len_album_name\", \"isOSTAlbum\", \"album_name_is_title_name\",\n",
    "#                   \"isBeatAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "#                   \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "#                    \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "#                   \"num_album_per_min_artist\", \"num_album_per_min_composer\", \"num_line_lyric\"]\n",
    "\n",
    "df_train = df[df.dataset==\"train\"]\n",
    "# df_train = df_train[df_train.numSongInAlbum==1]\n",
    "\n",
    "df_test = df[df.dataset==\"test\"]\n",
    "# chosen_features = [\"album_right\", \"istrack11\",\"no_artist\", \"no_composer\",\"freq_artist\", \"freq_composer\",\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "#                    \"isBeat\",  \"num_song_release_in_final_month\",\n",
    "#                   \"length\", \"genre\", \"track\",\"album_artist\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "#                   \"len_album_name\", \"isOSTAlbum\", \"album_name_is_title_name\",\n",
    "#                   \"isBeatAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "#                   \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "#                    \"freq_artist_min\", \"freq_composer_min\",\"dayofyear\",\"weekday\",\"isHoliday\",\n",
    "#                   \"num_album_per_min_artist\", \"num_album_per_min_composer\"\n",
    "                   \n",
    "#                     ]\n",
    "\n",
    "# chosen_features += [\"album_hash\",\"numSongInAlbum\"]\n",
    "\n",
    "\n",
    "# chosen_features = [\"year\", \"month\",\"hour\", \"day\", \"len_of_songname\", \n",
    "#                    \"isBeat\",  \"num_song_release_in_final_month\",\n",
    "#                    \"genre\",\"album_artist\",\"album\", \"islyric\", \"album_artist_contain_artistname\",\n",
    "#                   \"len_album_name\", \"isOSTAlbum\", \"album_name_is_title_name\",\n",
    "#                   \"isBeatAlbum\", \"artist_name_cat\",\"composers_name_cat\",\"copyright_cat\" ,\n",
    "#                   \"artist_id_min_cat\", \"composers_id_min_cat\",  \"artist_id_max_cat\", \"composers_id_max_cat\", \n",
    "#                   \"dayofyear\",\"weekday\",\"isHoliday\", \"group_album_artist_id_min_cat\"]\n",
    "\n",
    "# chosen_combined_features = [\"isRemix_isRemixAlbum\", \"isRemix_isCoverAlbum\", \"isOST_isVersion\", \"isOST_isRemixAlbum\", \n",
    "#  \"isOST_isSingleAlbum\"]\n",
    "# fig, axes = plt.subplots(23, 2, figsize=(20, 10*23))\n",
    "\n",
    "X = df_train[chosen_features]\n",
    "# df_train = df_train[df_train.numSongInAlbum]\n",
    "\n",
    "y = df_train.label\n",
    "# df1 = X[X.isna().any(axis=1)]\n",
    "# print(df1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# chosen_features.append(\"mean_album_score\")\n",
    "# chosen_features.append(\"mean_artist_score\")\n",
    "# chosen_features.append(\"mean_score_by_album_artist_id_min_cat\")\n",
    "\n",
    "# chosen_features += combine_features\n",
    "# chosen_features += chosen_combined_features\n",
    "print(chosen_features)\n",
    "print(len(chosen_features))\n",
    "param = {\n",
    "    'bagging_freq': 20,          \n",
    "    'bagging_fraction': 0.95,   'boost_from_average':'false',   \n",
    "    'boost': 'gbdt',             'feature_fraction': 0.1,     'learning_rate': 0.001,\n",
    "    'max_depth': -1,             'metric':'root_mean_squared_error', 'min_data_in_leaf': 5,   \n",
    "       'num_leaves': 50,            \n",
    "    'num_threads': 8,              'tree_learner': 'serial',   'objective': 'regression',\n",
    "    'reg_alpha': 0.1002650970728192, 'reg_lambda': 0.1003427518866501,'verbosity': 1,\n",
    "    \"seed\": 99999\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "labels= df_train.label\n",
    "# fig, axes = plt.subplots(5, 1, figsize=(10, 10*5))\n",
    "# axes = axes.flat\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    # Generate mean_album_score_features\n",
    "#     df_train[\"mean_album_score\"] = np.nan # Init\n",
    "#     df_train[\"mean_album_score\"].iloc[trn_idx] = df_train.iloc[trn_idx].groupby([\"album\"])[\"label\"].transform(\"mean\")\n",
    "#     df_train[\"mean_album_score\"] = df_train.groupby([\"album\"])[\"mean_album_score\"]\\\n",
    "#                                             .transform(lambda column: column.fillna(column.mean()))\\\n",
    "#                                             .fillna(0)\n",
    "#     print(df_train.iloc[trn_idx][\"mean_album_score\"].unique())\n",
    "#     df_test[\"mean_album_score\"] = np.nan \n",
    "#     df_all_tmp = pd.concat([df_train,df_test])\n",
    "#     df_all_tmp[\"mean_album_score\"] = df_all_tmp.groupby([\"album\"])[\"label\"]\\\n",
    "#                                             .transform(lambda column: column.fillna(column.mean()))\\\n",
    "#                                             .fillna(0)\n",
    "#     df_test = df_all_tmp[df_all_tmp.dataset == \"test\"]\n",
    "    \n",
    "    #Generate mean_artist_score\n",
    "#     df_train[\"mean_artist_score\"] = np.nan # Init\n",
    "#     df_train.iloc[trn_idx][\"mean_artist_score\"] = df_train.iloc[trn_idx].groupby([\"artist_id_min_cat\"])[\"label\"].transform(\"mean\")\n",
    "#     df_train[\"mean_artist_score\"] = df_train.groupby([\"artist_id_min_cat\"])[\"mean_artist_score\"]\\\n",
    "#                                             .transform(lambda column: column.fillna(column.mean()))\\\n",
    "#                                             .fillna(0)\n",
    "    \n",
    "#     df_test[\"mean_artist_score\"] = np.nan \n",
    "#     df_all_tmp = pd.concat([df_train,df_test])\n",
    "#     df_all_tmp[\"mean_artist_score\"] = df_all_tmp.groupby([\"artist_id_min_cat\"])[\"label\"]\\\n",
    "#                                             .transform(lambda column: column.fillna(column.mean()))\\\n",
    "#                                             .fillna(0)\n",
    "#     df_test = df_all_tmp[df_all_tmp.dataset == \"test\"]\n",
    "\n",
    "    #Generate mean based on album and artist\n",
    "#     df_train[\"mean_score_by_album_artist_id_min_cat\"] = np.nan # Init\n",
    "#     df_train[\"mean_score_by_album_artist_id_min_cat\"].iloc[trn_idx] = df_train.iloc[trn_idx].  \\\n",
    "#                                     groupby([\"album_hash\",\"artist_id_min_cat\"])[\"label\"].transform(\"mean\")\n",
    "# #     axes[fold_].scatter(df_train.iloc[trn_idx][\"mean_score_by_album_artist_id_min_cat\"].values,\n",
    "# #                     df_train.iloc[trn_idx][\"label\"].values)\n",
    "    \n",
    "# #     print(\"num distinct values:\",  len(df_train.iloc[trn_idx][\"mean_score_by_album_artist_id_min_cat\"].unique()))\n",
    "#     df_train[\"mean_score_by_album_artist_id_min_cat\"] = df_train.groupby([\"album_hash\",\"artist_id_min_cat\"])[\"mean_score_by_album_artist_id_min_cat\"]\\\n",
    "#                                             .transform(lambda column: column.fillna(column.mean()))\\\n",
    "#                                             .fillna(2)\n",
    "#     df_train[\"mean_score_by_album_artist_id_min_cat\"] = df_train[\"mean_score_by_album_artist_id_min_cat\"].apply(lambda x: np.round(x))\n",
    "    \n",
    "    \n",
    "#     df_test[\"mean_score_by_album_artist_id_min_cat\"] = np.nan \n",
    "#     df_all_tmp = pd.concat([df_train,df_test])\n",
    "#     df_all_tmp[\"mean_score_by_album_artist_id_min_cat\"] = df_all_tmp.groupby([\"album_hash\",\"artist_id_min_cat\"])[\"label\"]\\\n",
    "#                                             .transform(lambda column: column.fillna(column.mean()))\\\n",
    "#                                             .fillna(2)\n",
    "    \n",
    "#     df_all_tmp[\"mean_score_by_album_artist_id_min_cat\"] = df_all_tmp[\"mean_score_by_album_artist_id_min_cat\"].apply(lambda x: np.round(x))\n",
    "    \n",
    "#     df_test = df_all_tmp[df_all_tmp.dataset == \"test\"]\n",
    "        \n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][chosen_features], label=labels.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][chosen_features], label=labels.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 20000)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][chosen_features], num_iteration=clf.best_iteration)\n",
    "    predictions += clf.predict(df_test[chosen_features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "dump(clf,\"model.joblib\")\n",
    "\n",
    "np.save(\"oof.npy\",  oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1.59,1.61,1.63,1.63, 1.65]\n",
    "print(np.mean(a))\n",
    "print(np.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.58309 \n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "print(\"RMSE: {:<8.5f}\".format(sqrt(mean_squared_error(df_train.label, oof))))\n",
    "\n",
    "sub = pd.DataFrame({\"ID\": df_test.ID.values})\n",
    "sub[\"label\"] = predictions\n",
    "sub.to_csv(\"submission_lightgbm.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_result(df_train, oof):\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=99999)\n",
    "    fig,ax = plt.subplots(6,2,figsize=(20,30))\n",
    "    ax =ax.flat\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "        \n",
    "        print(\"RMSE: {:<8.5f} \".format(sqrt(mean_squared_error(df_train.label[val_idx], oof[val_idx]))))\n",
    "        \n",
    "        r = {}\n",
    "        for yt, yp  in zip(df_train.label.values[val_idx], oof[val_idx]):\n",
    "            if yt in sorted(r.keys()):\n",
    "                r[yt].append(yp)\n",
    "            else:\n",
    "                r[yt] = [yp]\n",
    "\n",
    "        x1 = sorted(r.keys())\n",
    "        values = np.array([r[k] for k in x1])\n",
    "        print(x1)\n",
    "        means= [np.mean(value) for value in values]\n",
    "#         print(\"Means: \",means)\n",
    "        mi= [np.min(value) for value in values]\n",
    "        ma= [np.max(value) for value in values]\n",
    "        quan25= [np.quantile(value, 0.1) for value in values]\n",
    "        quan75= [np.quantile(value, 0.9) for value in values]\n",
    "\n",
    "        ax[fold_*2].plot(x1, means, label=\"Mean Rank prediction\")\n",
    "        ax[fold_*2].fill_between(x1, mi,ma,color='b', alpha=0.2 , label=\"Min-Max Rank prediction\")\n",
    "        ax[fold_*2].fill_between(x1, quan25,quan75,color='r', alpha=0.5 ,label=\"10-th to 90-th Rank prediction\")\n",
    "        ax[fold_*2].legend(loc='upper left')\n",
    "        ax[fold_*2].set_xlabel(\"Actual Rank\")\n",
    "        ax[fold_*2].set_ylabel(\"Predicted Rank\")\n",
    "        \n",
    "        ax[fold_*2+1].hist(df_train.label.values[val_idx])\n",
    "        ax[fold_*2+1].set_xlabel(\"ActualRank\")\n",
    "        ax[fold_*2+1].set_ylabel(\"Count\")\n",
    "    \n",
    "    \n",
    "    r = {}\n",
    "    for yt, yp  in zip(df_train.label.values, oof):\n",
    "        if yt in sorted(r.keys()):\n",
    "            r[yt].append(yp)\n",
    "        else:\n",
    "            r[yt] = [yp]\n",
    "\n",
    "    x1 = sorted(r.keys())\n",
    "    values = np.array([r[k] for k in x1])\n",
    "    means= [np.mean(value) for value in values]\n",
    "    mi= [np.min(value) for value in values]\n",
    "    ma= [np.max(value) for value in values]\n",
    "    quan25= [np.quantile(value, 0.1) for value in values]\n",
    "    quan75= [np.quantile(value, 0.9) for value in values]\n",
    "    ax[10].plot(x1, means, label=\"Mean Rank prediction\")\n",
    "    ax[10].fill_between(x1, mi,ma,color='b', alpha=0.2 , label=\"Min-Max Rank prediction\")\n",
    "    ax[10].fill_between(x1, quan25,quan75,color='r', alpha=0.5 ,label=\"10-th to 90-th Rank prediction\")\n",
    "    ax[10].legend(loc='upper left')\n",
    "    ax[10].set_xlabel(\"Actual Rank\")\n",
    "    ax[10].set_ylabel(\"Predicted Rank\")\n",
    "    plt.show()\n",
    "\n",
    "from joblib import dump, load\n",
    "# clf1= load(\"model.joblib\")\n",
    "oof = np.load(\"oof.npy\")\n",
    "plot_result(df_train, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=99999)\n",
    "fig,ax = plt.subplots(5,2,figsize=(20,30))\n",
    "ax =ax.flat\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train.label.values)):\n",
    "    if fold_== 0:\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "        oof[val_idx] = clf.predict(df_train.iloc[val_idx][chosen_features], num_iteration=clf.best_iteration)\n",
    "        for oof_e, (i,o) in zip(oof[val_idx], df_train.iloc[val_idx].iterrows()):\n",
    "            if (o.label ==9):\n",
    "                print(o.album,\"----------\",o.album_artist,\"----\",o.artist_name,\"--\",o.label,\"---\",oof_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "lgb.plot_importance(clf, max_num_features=20,importance_type='gain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "lgb.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
